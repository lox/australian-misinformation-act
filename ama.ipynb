{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in ./myenv/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in ./myenv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in ./myenv/lib/python3.12/site-packages (2.1.3)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./myenv/lib/python3.12/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./myenv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./myenv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 pandas numpy openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download submissions pdfs into submissions directory\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "def download_submissions():\n",
    "    # Create submissions directory if it doesn't exist\n",
    "    if not os.path.exists('submissions'):\n",
    "        os.makedirs('submissions')\n",
    "\n",
    "    base_url = 'https://www.infrastructure.gov.au/have-your-say/new-acma-powers-combat-misinformation-and-disinformation'\n",
    "    page = 0\n",
    "    downloaded_files = set()  # Keep track of files we've already seen\n",
    "    \n",
    "    while True:\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        print(f\"Processing page {page}\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to access page {page}. Status code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        found_files = False\n",
    "        \n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx']):\n",
    "                file_url = urljoin(base_url, href)\n",
    "                filename = os.path.basename(href)\n",
    "                file_path = os.path.join('submissions', filename)\n",
    "                \n",
    "                # Skip if we've already seen this file\n",
    "                if filename in downloaded_files:\n",
    "                    continue\n",
    "                    \n",
    "                found_files = True\n",
    "                downloaded_files.add(filename)\n",
    "                \n",
    "                # Skip if file already exists\n",
    "                if os.path.exists(file_path):\n",
    "                    print(f\"Skipping existing file: {filename}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    print(f\"Downloading: {filename}\")\n",
    "                    file_response = requests.get(file_url)\n",
    "                    \n",
    "                    if file_response.status_code == 200:\n",
    "                        with open(file_path, 'wb') as f:\n",
    "                            f.write(file_response.content)\n",
    "                        print(f\"Successfully downloaded: {filename}\")\n",
    "                        time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to download {filename}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading {filename}: {str(e)}\")\n",
    "        \n",
    "        if not found_files:\n",
    "            print(f\"No new files found on page {page}. Ending search.\")\n",
    "            break\n",
    "            \n",
    "        page += 1\n",
    "        time.sleep(2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: acma2023-tranche-1-15sep23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-1-15sep23.xlsx\n",
      "Downloading: acma2023-tranche-2-22sep23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-2-22sep23.xlsx\n",
      "Downloading: acma2023-tranche-3-28sep23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-3-28sep23.xlsx\n",
      "Downloading: acma2023-tranche-4-5oct23_0.xlsx\n",
      "Successfully downloaded: acma2023-tranche-4-5oct23_0.xlsx\n",
      "Downloading: acma2023-tranche-5-23oct23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-5-23oct23.xlsx\n",
      "Downloading: acma2023-tranche-6-13nov23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-6-13nov23.xlsx\n",
      "Downloading: acma2023-tranche-7-11dec23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-7-11dec23.xlsx\n",
      "Downloading: acma2023-tranche-8-18dec23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-8-18dec23.xlsx\n",
      "Downloading: acma2023-tranche-9-21dec23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-9-21dec23.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Download tranches excel files into tranches/\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "def download_tranches():\n",
    "    # Create tranches directory if it doesn't exist\n",
    "    if not os.path.exists('tranches'):\n",
    "        os.makedirs('tranches')\n",
    "\n",
    "    base_url = 'https://www.infrastructure.gov.au/have-your-say/new-acma-powers-combat-misinformation-and-disinformation'\n",
    "    \n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access page. Status code: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        if '.xlsx' in href.lower():\n",
    "            file_url = urljoin(base_url, href)\n",
    "            filename = os.path.basename(href)\n",
    "            file_path = os.path.join('tranches', filename)\n",
    "            \n",
    "            # Skip if file already exists\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"Skipping existing file: {filename}\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                print(f\"Downloading: {filename}\")\n",
    "                file_response = requests.get(file_url)\n",
    "                \n",
    "                if file_response.status_code == 200:\n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        f.write(file_response.content)\n",
    "                    print(f\"Successfully downloaded: {filename}\")\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    print(f\"Failed to download {filename}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {filename}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_tranches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: acma2023-tranche-8-18dec23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-8-18dec23.json with 338 submissions\n",
      "Processing: acma2023-tranche-7-11dec23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-7-11dec23.json with 384 submissions\n",
      "Processing: acma2023-tranche-6-13nov23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-6-13nov23.json with 150 submissions\n",
      "Processing: acma2023-tranche-9-21dec23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-9-21dec23.json with 130 submissions\n",
      "Processing: acma2023-tranche-2-22sep23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-2-22sep23.json with 178 submissions\n",
      "Processing: acma2023-tranche-5-23oct23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-5-23oct23.json with 297 submissions\n",
      "Processing: acma2023-tranche-4-5oct23_0.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-4-5oct23_0.json with 299 submissions\n",
      "Processing: acma2023-tranche-1-15sep23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-1-15sep23.json with 150 submissions\n",
      "Processing: acma2023-tranche-3-28sep23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-3-28sep23.json with 341 submissions\n"
     ]
    }
   ],
   "source": [
    "# Convert tranches excel files to json\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def parse_tranche_date(filename):\n",
    "    # Extract date pattern like \"15sep23\", \"5oct23\", etc.\n",
    "    pattern = r'-(\\d{1,2}(?:sep|oct|nov|dec)23)'\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        # Convert to standard format (e.g., \"2023-09-15\")\n",
    "        day = re.match(r'(\\d{1,2})', date_str).group(1)\n",
    "        month = re.search(r'(sep|oct|nov|dec)', date_str).group(1)\n",
    "        year = '2023'\n",
    "        \n",
    "        # Pad day with leading zero if needed\n",
    "        day = day.zfill(2)\n",
    "        \n",
    "        # Convert month abbreviation to number\n",
    "        month_map = {'sep': '09', 'oct': '10', 'nov': '11', 'dec': '12'}\n",
    "        month_num = month_map[month]\n",
    "        \n",
    "        return f\"{year}-{month_num}-{day}\"\n",
    "    return None\n",
    "\n",
    "def process_tranches_to_json():\n",
    "    # Create tranches directory if it doesn't exist\n",
    "    tranches_dir = 'tranches'\n",
    "    if not os.path.exists(tranches_dir):\n",
    "        print(\"No tranches directory found!\")\n",
    "        return\n",
    "\n",
    "    # Process each xlsx file\n",
    "    for filename in os.listdir(tranches_dir):\n",
    "        if not filename.lower().endswith('.xlsx'):\n",
    "            continue\n",
    "            \n",
    "        xlsx_path = os.path.join(tranches_dir, filename)\n",
    "        json_path = os.path.join(tranches_dir, f\"{os.path.splitext(filename)[0]}.json\")\n",
    "        \n",
    "        # Skip if JSON already exists\n",
    "        if os.path.exists(json_path):\n",
    "            print(f\"Skipping existing JSON: {json_path}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(f\"Processing: {filename}\")\n",
    "            \n",
    "            # Parse tranche date\n",
    "            tranche_date = parse_tranche_date(filename)\n",
    "            \n",
    "            # Read Excel file without headers\n",
    "            df = pd.read_excel(xlsx_path, header=None)\n",
    "            \n",
    "            # Create submissions map (filename -> submitter)\n",
    "            submissions_map = {}\n",
    "            for _, row in df.iterrows():\n",
    "                submitter = row[0]  # First column\n",
    "                submission_file = row[1]  # Second column\n",
    "                if pd.notna(submitter) and pd.notna(submission_file):\n",
    "                    submissions_map[str(submission_file).strip()] = str(submitter).strip()\n",
    "            \n",
    "            # Create JSON structure\n",
    "            json_data = {\n",
    "                'filename': filename,\n",
    "                'metadata': {\n",
    "                    'processed_timestamp': datetime.now().isoformat(),\n",
    "                    'tranche_date': tranche_date,\n",
    "                    'submission_count': len(submissions_map)\n",
    "                },\n",
    "                'submissions': submissions_map\n",
    "            }\n",
    "            \n",
    "            # Save as JSON\n",
    "            with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "            print(f\"Successfully created: {json_path} with {len(submissions_map)} submissions\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_tranches_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymupdf4llm --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pymupdf4llm\n",
    "\n",
    "def process_pdfs():\n",
    "    # Process each PDF in submissions directory\n",
    "    submissions_dir = 'submissions'\n",
    "    for filename in os.listdir(submissions_dir):\n",
    "        if not filename.lower().endswith('.pdf'):\n",
    "            continue\n",
    "            \n",
    "        pdf_path = os.path.join(submissions_dir, filename)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        md_file = os.path.join(submissions_dir, f\"{base_name}.md\")\n",
    "        \n",
    "        # Skip if markdown file already exists\n",
    "        if os.path.exists(md_file):\n",
    "            print(f\"Skipping already processed file: {filename}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Convert to markdown\n",
    "            md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "            \n",
    "            # Save markdown version next to PDF\n",
    "            Path(md_file).write_bytes(md_text.encode())\n",
    "            print(f\"Successfully processed {filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "def analyze_submission(client, text):\n",
    "    system_prompt = \"\"\"You are analyzing submissions about the Australian Government's proposed legislation for new ACMA powers to combat misinformation and disinformation.\n",
    "    For the given submission text, please analyze:\n",
    "    1. Overall Position: Whether they support or oppose the legislation (or neutral/mixed)\n",
    "    2. Key Arguments: Main points made in the submission\n",
    "    3. Main Themes: Recurring themes or areas of focus\n",
    "    4. Notable Quotes: Any particularly significant quotes that support the analysis\n",
    "    5. Stakeholder Type: What type of stakeholder made this submission (e.g., tech company, civil society, academic, individual)\n",
    "    \n",
    "    Return your analysis in JSON format.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-08-06\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ],\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            temperature=0.1,  # Lower temperature for more consistent analysis\n",
    "            seed=1\n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in OpenAI API call: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_submissions():\n",
    "    client = OpenAI()\n",
    "    \n",
    "    # Process each markdown file in submissions directory\n",
    "    submissions_dir = 'submissions'\n",
    "    for filename in sorted(os.listdir(submissions_dir)):\n",
    "        if not filename.lower().endswith('.md'):\n",
    "            continue\n",
    "        md_path = os.path.join(submissions_dir, filename)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        analysis_file = os.path.join(submissions_dir, f\"{base_name}.json\")\n",
    "        \n",
    "        # Skip if analysis already exists\n",
    "        if os.path.exists(analysis_file):\n",
    "            print(f\"Skipping already analyzed file: {filename} ({analysis_file} exists)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Analyzing: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Read markdown content\n",
    "            with open(md_path, 'r', encoding='utf-8') as f:\n",
    "                md_text = f.read()\n",
    "            \n",
    "            # Analyze with OpenAI\n",
    "            analysis = analyze_submission(client, md_text)\n",
    "            \n",
    "            if analysis:\n",
    "                # Save analysis with timestamp and model info\n",
    "                output = {\n",
    "                    'filename': filename,\n",
    "                    'analysis_metadata': {\n",
    "                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'model': 'gpt-4o-2024-08-06'\n",
    "                    },\n",
    "                    'analysis': analysis\n",
    "                }\n",
    "                \n",
    "                with open(analysis_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "                print(f\"Successfully analyzed {filename}, wrote to {analysis_file}\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Missing analysis file: acma2023-17518-anonymous.json\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-18557-anonymous .json\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-21187-anthony-alaveras .json\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-21878-noah-abbey.json\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-27270-suri-ratnapala-and-adrian-ratnapala.json\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-27957-regional-development-australia-southern-inland.json\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-28523-democratic-labour-party-south-australia.json\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-33079-rob-d'ermilo.json\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-34662-feminist-legal-clinic-inc..json\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-e2410-michael-de stoop.json\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-e2420-murray-may.json\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-e2910-alexander-cornell stewart.json\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-e3675-lynda-crawford.json\n",
      "‚ö†Ô∏è Unknown stakeholder type: multistakeholder organization (global network initiative)\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-33576-carolyn-delzoppo.json\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-34751-anonymous.json\n",
      "‚ö†Ô∏è Missing analysis file: acma2023-e3741-ralph-babet-united-australia-party.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    # Extract tranche number for sorting\n",
    "    match = re.search(r'tranche-(\\d+)', s)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def get_position_emoji(analysis, filename):\n",
    "    if not analysis or 'Overall Position' not in analysis:\n",
    "        print(f\"‚ö†Ô∏è Missing Overall Position in analysis for {filename}\")\n",
    "        return \"‚ùì\"  # Unknown\n",
    "    position = analysis['Overall Position'].lower()\n",
    "    if 'support' in position:\n",
    "        return \"‚úÖ\"  # Support\n",
    "    elif 'oppose' in position:\n",
    "        return \"‚ùå\"  # Oppose\n",
    "    elif 'mixed' in position or 'neutral' in position:\n",
    "        return \"‚ÜîÔ∏è\"  # Mixed/Neutral\n",
    "    print(f\"‚ö†Ô∏è Unknown position value: {position} for {filename}\")\n",
    "    return \"‚ùì\"  # Unknown\n",
    "\n",
    "def get_stakeholder_emoji(analysis):\n",
    "    if not analysis or 'Stakeholder Type' not in analysis:\n",
    "        print(f\"‚ö†Ô∏è Missing Stakeholder Type in analysis\")\n",
    "        return \"‚ùì\"\n",
    "    stakeholder = analysis['Stakeholder Type'].lower()\n",
    "    \n",
    "    # Skip processing if N/A\n",
    "    if stakeholder in ['n/a', 'unknown']:\n",
    "        return \"‚ùì\"\n",
    "        \n",
    "    # Business/Industry/Think Tank\n",
    "    if any(x in stakeholder for x in [\n",
    "        'company', 'business', 'industry association', \n",
    "        'media outlet', 'think tank'\n",
    "    ]):\n",
    "        return \"üè¢\"\n",
    "    \n",
    "    # Academic/Research\n",
    "    elif 'academic' in stakeholder:\n",
    "        return \"üéì\"\n",
    "    \n",
    "    # Individual\n",
    "    elif 'individual' in stakeholder:\n",
    "        return \"üë§\"\n",
    "    \n",
    "    # Civil Society/NGO/International/Religious\n",
    "    elif any(x in stakeholder for x in [\n",
    "        'civil society', 'ngo', 'international organization',\n",
    "        'religious organization', 'political party', 'educational institution', \n",
    "        'media organization', 'industry body', 'multistakeholder organization'\n",
    "    ]):\n",
    "        return \"üåê\"\n",
    "    \n",
    "    # Government/Legal\n",
    "    elif any(x in stakeholder for x in ['government', 'legal', 'legislative']):\n",
    "        return \"üèõÔ∏è\"\n",
    "    \n",
    "    # If we get here, it's an unhandled type\n",
    "    print(f\"‚ö†Ô∏è Unknown stakeholder type: {stakeholder}\")\n",
    "    return \"‚ùì\"\n",
    "\n",
    "def generate_summary_markdown():\n",
    "    tranches_dir = 'tranches'\n",
    "    submissions_dir = 'submissions'\n",
    "    output_file = 'submissions_summary.md'\n",
    "    \n",
    "    # Get all tranche JSON files\n",
    "    tranche_files = [f for f in os.listdir(tranches_dir) if f.endswith('.json')]\n",
    "    tranche_files.sort(key=natural_sort_key)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as out:\n",
    "        out.write(\"# Submissions Summary\\n\\n\")\n",
    "        \n",
    "        for tranche_file in tranche_files:\n",
    "            with open(os.path.join(tranches_dir, tranche_file), 'r', encoding='utf-8') as f:\n",
    "                tranche_data = json.load(f)\n",
    "            \n",
    "            # Write tranche header\n",
    "            tranche_date = tranche_data['metadata']['tranche_date']\n",
    "            out.write(f\"## {os.path.splitext(tranche_file)[0]} ({tranche_date})\\n\\n\")\n",
    "            \n",
    "            # Write table header\n",
    "            out.write(\"| # | Submitter | PDF | Analysis | Position | Type |\\n\")\n",
    "            out.write(\"|---|-----------|-----|----------|----------|------|\\n\")\n",
    "            \n",
    "            # Write table rows\n",
    "            for i, (pdf_file, submitter) in enumerate(tranche_data['submissions'].items(), 1):\n",
    "                # Get corresponding JSON analysis file if it exists\n",
    "                base_name = os.path.splitext(pdf_file)[0]\n",
    "                json_file = f\"{base_name}.json\"  # Remove the _analysis suffix\n",
    "                json_path = os.path.join(submissions_dir, json_file)\n",
    "                \n",
    "                analysis = None\n",
    "                if os.path.exists(json_path):\n",
    "                    try:\n",
    "                        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                            analysis_data = json.load(f)\n",
    "                            analysis = analysis_data.get('analysis', {})\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Error reading analysis file {json_file}: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Missing analysis file: {json_file}\")\n",
    "                    continue\n",
    "                \n",
    "                pdf_link = f\"[PDF](submissions/{pdf_file})\" if os.path.exists(os.path.join(submissions_dir, pdf_file)) else \"‚ùå\"\n",
    "                json_link = f\"[Analysis](submissions/{json_file})\" if os.path.exists(json_path) else \"‚ùå\"\n",
    "                \n",
    "                position_emoji = get_position_emoji(analysis, json_path)\n",
    "                stakeholder_emoji = get_stakeholder_emoji(analysis)\n",
    "                \n",
    "                out.write(f\"| {i} | {submitter} | {pdf_link} | {json_link} | {position_emoji} | {stakeholder_emoji} |\\n\")\n",
    "            \n",
    "            out.write(\"\\n\")  # Add space between tables\n",
    "        \n",
    "        # Write legend\n",
    "        out.write(\"\\n## Legend\\n\\n\")\n",
    "        out.write(\"### Position\\n\")\n",
    "        out.write(\"- ‚úÖ Support\\n\")\n",
    "        out.write(\"- ‚ùå Oppose\\n\")\n",
    "        out.write(\"- ‚ÜîÔ∏è Mixed/Neutral\\n\")\n",
    "        out.write(\"- ‚ùì Unknown\\n\\n\")\n",
    "        out.write(\"### Stakeholder Type\\n\")\n",
    "        out.write(\"- üè¢ Company/Business\\n\")\n",
    "        out.write(\"- üéì Academic\\n\")\n",
    "        out.write(\"- üë§ Individual\\n\")\n",
    "        out.write(\"- üåê Civil Society/NGO\\n\")\n",
    "        out.write(\"- üèõÔ∏è Government\\n\")\n",
    "        out.write(\"- ‚ùì Unknown\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_summary_markdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
