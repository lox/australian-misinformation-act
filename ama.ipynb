{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in ./myenv/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in ./myenv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in ./myenv/lib/python3.12/site-packages (2.1.3)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./myenv/lib/python3.12/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./myenv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./myenv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 pandas numpy openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download submissions pdfs into submissions directory\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "def download_submissions():\n",
    "    # Create submissions directory if it doesn't exist\n",
    "    if not os.path.exists('submissions'):\n",
    "        os.makedirs('submissions')\n",
    "\n",
    "    base_url = 'https://www.infrastructure.gov.au/have-your-say/new-acma-powers-combat-misinformation-and-disinformation'\n",
    "    page = 0\n",
    "    downloaded_files = set()  # Keep track of files we've already seen\n",
    "    \n",
    "    while True:\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        print(f\"Processing page {page}\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to access page {page}. Status code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        found_files = False\n",
    "        \n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx']):\n",
    "                file_url = urljoin(base_url, href)\n",
    "                filename = os.path.basename(href)\n",
    "                file_path = os.path.join('submissions', filename)\n",
    "                \n",
    "                # Skip if we've already seen this file\n",
    "                if filename in downloaded_files:\n",
    "                    continue\n",
    "                    \n",
    "                found_files = True\n",
    "                downloaded_files.add(filename)\n",
    "                \n",
    "                # Skip if file already exists\n",
    "                if os.path.exists(file_path):\n",
    "                    print(f\"Skipping existing file: {filename}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    print(f\"Downloading: {filename}\")\n",
    "                    file_response = requests.get(file_url)\n",
    "                    \n",
    "                    if file_response.status_code == 200:\n",
    "                        with open(file_path, 'wb') as f:\n",
    "                            f.write(file_response.content)\n",
    "                        print(f\"Successfully downloaded: {filename}\")\n",
    "                        time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"Failed to download {filename}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading {filename}: {str(e)}\")\n",
    "        \n",
    "        if not found_files:\n",
    "            print(f\"No new files found on page {page}. Ending search.\")\n",
    "            break\n",
    "            \n",
    "        page += 1\n",
    "        time.sleep(2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: acma2023-tranche-1-15sep23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-1-15sep23.xlsx\n",
      "Downloading: acma2023-tranche-2-22sep23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-2-22sep23.xlsx\n",
      "Downloading: acma2023-tranche-3-28sep23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-3-28sep23.xlsx\n",
      "Downloading: acma2023-tranche-4-5oct23_0.xlsx\n",
      "Successfully downloaded: acma2023-tranche-4-5oct23_0.xlsx\n",
      "Downloading: acma2023-tranche-5-23oct23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-5-23oct23.xlsx\n",
      "Downloading: acma2023-tranche-6-13nov23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-6-13nov23.xlsx\n",
      "Downloading: acma2023-tranche-7-11dec23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-7-11dec23.xlsx\n",
      "Downloading: acma2023-tranche-8-18dec23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-8-18dec23.xlsx\n",
      "Downloading: acma2023-tranche-9-21dec23.xlsx\n",
      "Successfully downloaded: acma2023-tranche-9-21dec23.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Download tranches excel files into tranches/\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "def download_tranches():\n",
    "    # Create tranches directory if it doesn't exist\n",
    "    if not os.path.exists('tranches'):\n",
    "        os.makedirs('tranches')\n",
    "\n",
    "    base_url = 'https://www.infrastructure.gov.au/have-your-say/new-acma-powers-combat-misinformation-and-disinformation'\n",
    "    \n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access page. Status code: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        if '.xlsx' in href.lower():\n",
    "            file_url = urljoin(base_url, href)\n",
    "            filename = os.path.basename(href)\n",
    "            file_path = os.path.join('tranches', filename)\n",
    "            \n",
    "            # Skip if file already exists\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"Skipping existing file: {filename}\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                print(f\"Downloading: {filename}\")\n",
    "                file_response = requests.get(file_url)\n",
    "                \n",
    "                if file_response.status_code == 200:\n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        f.write(file_response.content)\n",
    "                    print(f\"Successfully downloaded: {filename}\")\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    print(f\"Failed to download {filename}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {filename}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_tranches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: acma2023-tranche-8-18dec23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-8-18dec23.json with 338 submissions\n",
      "Processing: acma2023-tranche-7-11dec23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-7-11dec23.json with 384 submissions\n",
      "Processing: acma2023-tranche-6-13nov23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-6-13nov23.json with 150 submissions\n",
      "Processing: acma2023-tranche-9-21dec23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-9-21dec23.json with 130 submissions\n",
      "Processing: acma2023-tranche-2-22sep23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-2-22sep23.json with 178 submissions\n",
      "Processing: acma2023-tranche-5-23oct23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-5-23oct23.json with 297 submissions\n",
      "Processing: acma2023-tranche-4-5oct23_0.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-4-5oct23_0.json with 299 submissions\n",
      "Processing: acma2023-tranche-1-15sep23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-1-15sep23.json with 150 submissions\n",
      "Processing: acma2023-tranche-3-28sep23.xlsx\n",
      "Successfully created: tranches/acma2023-tranche-3-28sep23.json with 341 submissions\n"
     ]
    }
   ],
   "source": [
    "# Convert tranches excel files to json\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def parse_tranche_date(filename):\n",
    "    # Extract date pattern like \"15sep23\", \"5oct23\", etc.\n",
    "    pattern = r'-(\\d{1,2}(?:sep|oct|nov|dec)23)'\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        # Convert to standard format (e.g., \"2023-09-15\")\n",
    "        day = re.match(r'(\\d{1,2})', date_str).group(1)\n",
    "        month = re.search(r'(sep|oct|nov|dec)', date_str).group(1)\n",
    "        year = '2023'\n",
    "        \n",
    "        # Pad day with leading zero if needed\n",
    "        day = day.zfill(2)\n",
    "        \n",
    "        # Convert month abbreviation to number\n",
    "        month_map = {'sep': '09', 'oct': '10', 'nov': '11', 'dec': '12'}\n",
    "        month_num = month_map[month]\n",
    "        \n",
    "        return f\"{year}-{month_num}-{day}\"\n",
    "    return None\n",
    "\n",
    "def process_tranches_to_json():\n",
    "    # Create tranches directory if it doesn't exist\n",
    "    tranches_dir = 'tranches'\n",
    "    if not os.path.exists(tranches_dir):\n",
    "        print(\"No tranches directory found!\")\n",
    "        return\n",
    "\n",
    "    # Process each xlsx file\n",
    "    for filename in os.listdir(tranches_dir):\n",
    "        if not filename.lower().endswith('.xlsx'):\n",
    "            continue\n",
    "            \n",
    "        xlsx_path = os.path.join(tranches_dir, filename)\n",
    "        json_path = os.path.join(tranches_dir, f\"{os.path.splitext(filename)[0]}.json\")\n",
    "        \n",
    "        # Skip if JSON already exists\n",
    "        if os.path.exists(json_path):\n",
    "            print(f\"Skipping existing JSON: {json_path}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(f\"Processing: {filename}\")\n",
    "            \n",
    "            # Parse tranche date\n",
    "            tranche_date = parse_tranche_date(filename)\n",
    "            \n",
    "            # Read Excel file without headers\n",
    "            df = pd.read_excel(xlsx_path, header=None)\n",
    "            \n",
    "            # Create submissions map (filename -> submitter)\n",
    "            submissions_map = {}\n",
    "            for _, row in df.iterrows():\n",
    "                submitter = row[0]  # First column\n",
    "                submission_file = row[1]  # Second column\n",
    "                if pd.notna(submitter) and pd.notna(submission_file):\n",
    "                    submissions_map[str(submission_file).strip()] = str(submitter).strip()\n",
    "            \n",
    "            # Create JSON structure\n",
    "            json_data = {\n",
    "                'filename': filename,\n",
    "                'metadata': {\n",
    "                    'processed_timestamp': datetime.now().isoformat(),\n",
    "                    'tranche_date': tranche_date,\n",
    "                    'submission_count': len(submissions_map)\n",
    "                },\n",
    "                'submissions': submissions_map\n",
    "            }\n",
    "            \n",
    "            # Save as JSON\n",
    "            with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "            print(f\"Successfully created: {json_path} with {len(submissions_map)} submissions\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_tranches_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymupdf4llm --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pymupdf4llm\n",
    "\n",
    "def process_pdfs():\n",
    "    # Process each PDF in submissions directory\n",
    "    submissions_dir = 'submissions'\n",
    "    for filename in os.listdir(submissions_dir):\n",
    "        if not filename.lower().endswith('.pdf'):\n",
    "            continue\n",
    "            \n",
    "        pdf_path = os.path.join(submissions_dir, filename)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        md_file = os.path.join(submissions_dir, f\"{base_name}.md\")\n",
    "        \n",
    "        # Skip if markdown file already exists\n",
    "        if os.path.exists(md_file):\n",
    "            print(f\"Skipping already processed file: {filename}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Convert to markdown\n",
    "            md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "            \n",
    "            # Save markdown version next to PDF\n",
    "            Path(md_file).write_bytes(md_text.encode())\n",
    "            print(f\"Successfully processed {filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "def analyze_submission(client, text):\n",
    "    system_prompt = \"\"\"You are analyzing submissions about the Australian Government's proposed legislation for new ACMA powers to combat misinformation and disinformation.\n",
    "    For the given submission text, please analyze:\n",
    "    1. Overall Position: Whether they support or oppose the legislation (or neutral/mixed)\n",
    "    2. Key Arguments: Main points made in the submission\n",
    "    3. Main Themes: Recurring themes or areas of focus\n",
    "    4. Notable Quotes: Any particularly significant quotes that support the analysis\n",
    "    5. Stakeholder Type: What type of stakeholder made this submission (e.g., tech company, civil society, academic, individual)\n",
    "    \n",
    "    Return your analysis in JSON format.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-08-06\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ],\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            temperature=0.1,  # Lower temperature for more consistent analysis\n",
    "            seed=1\n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in OpenAI API call: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_submissions():\n",
    "    client = OpenAI()\n",
    "    \n",
    "    # Process each markdown file in submissions directory\n",
    "    submissions_dir = 'submissions'\n",
    "    for filename in sorted(os.listdir(submissions_dir)):\n",
    "        if not filename.lower().endswith('.md'):\n",
    "            continue\n",
    "        md_path = os.path.join(submissions_dir, filename)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        analysis_file = os.path.join(submissions_dir, f\"{base_name}.json\")\n",
    "        \n",
    "        # Skip if analysis already exists\n",
    "        if os.path.exists(analysis_file):\n",
    "            print(f\"Skipping already analyzed file: {filename} ({analysis_file} exists)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Analyzing: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Read markdown content\n",
    "            with open(md_path, 'r', encoding='utf-8') as f:\n",
    "                md_text = f.read()\n",
    "            \n",
    "            # Analyze with OpenAI\n",
    "            analysis = analyze_submission(client, md_text)\n",
    "            \n",
    "            if analysis:\n",
    "                # Save analysis with timestamp and model info\n",
    "                output = {\n",
    "                    'filename': filename,\n",
    "                    'analysis_metadata': {\n",
    "                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'model': 'gpt-4o-2024-08-06'\n",
    "                    },\n",
    "                    'analysis': analysis\n",
    "                }\n",
    "                \n",
    "                with open(analysis_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "                print(f\"Successfully analyzed {filename}, wrote to {analysis_file}\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Statistics:\n",
      "Total Submissions: 2267\n",
      "Analyzed: 2251\n",
      "\n",
      "Positions:\n",
      "Support: 32 (1.4%)\n",
      "Oppose: 2067 (91.8%)\n",
      "Mixed: 152 (6.8%)\n",
      "Unknown: 16 (0.7%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_statistics():\n",
    "    tranches_dir = 'tranches'\n",
    "    submissions_dir = 'submissions'\n",
    "    \n",
    "    # Initialize statistics structure\n",
    "    stats = {\n",
    "        \"overall\": {\n",
    "            \"total\": 0,\n",
    "            \"analyzed\": 0,\n",
    "            \"positions\": {\n",
    "                \"support\": 0,\n",
    "                \"oppose\": 0,\n",
    "                \"mixed\": 0,\n",
    "                \"unknown\": 0\n",
    "            },\n",
    "            \"percentages\": {}\n",
    "        },\n",
    "        \"by_tranche\": {},\n",
    "        \"by_stakeholder\": defaultdict(lambda: {\n",
    "            \"support\": 0,\n",
    "            \"oppose\": 0,\n",
    "            \"mixed\": 0,\n",
    "            \"unknown\": 0\n",
    "        }),\n",
    "        \"metadata\": {\n",
    "            \"generated_at\": datetime.now().isoformat(),\n",
    "            \"tranches_analyzed\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Process each tranche\n",
    "    tranche_files = sorted([f for f in os.listdir(tranches_dir) if f.endswith('.json')], \n",
    "                          key=lambda x: int(re.search(r'tranche-(\\d+)', x).group(1)))\n",
    "    \n",
    "    for tranche_file in tranche_files:\n",
    "        with open(os.path.join(tranches_dir, tranche_file), 'r') as f:\n",
    "            tranche_data = json.load(f)\n",
    "            \n",
    "        tranche_name = os.path.splitext(tranche_file)[0]\n",
    "        tranche_date = tranche_data['metadata']['tranche_date']\n",
    "        \n",
    "        # Initialize tranche statistics\n",
    "        stats[\"by_tranche\"][tranche_name] = {\n",
    "            \"date\": tranche_date,\n",
    "            \"total\": 0,\n",
    "            \"analyzed\": 0,\n",
    "            \"positions\": {\n",
    "                \"support\": 0,\n",
    "                \"oppose\": 0,\n",
    "                \"mixed\": 0,\n",
    "                \"unknown\": 0\n",
    "            },\n",
    "            \"percentages\": {}\n",
    "        }\n",
    "        \n",
    "        # Process each submission in tranche\n",
    "        for pdf_file, submitter in tranche_data['submissions'].items():\n",
    "            stats[\"overall\"][\"total\"] += 1\n",
    "            stats[\"by_tranche\"][tranche_name][\"total\"] += 1\n",
    "            \n",
    "            # Check for analysis file\n",
    "            base_name = os.path.splitext(pdf_file)[0]\n",
    "            json_file = f\"{base_name}.json\"\n",
    "            json_path = os.path.join(submissions_dir, json_file)\n",
    "            \n",
    "            if os.path.exists(json_path):\n",
    "                try:\n",
    "                    with open(json_path, 'r') as f:\n",
    "                        analysis_data = json.load(f)\n",
    "                        analysis = analysis_data.get('analysis', {})\n",
    "                        \n",
    "                    stats[\"overall\"][\"analyzed\"] += 1\n",
    "                    stats[\"by_tranche\"][tranche_name][\"analyzed\"] += 1\n",
    "                    \n",
    "                    # Process position\n",
    "                    position = analysis.get('Overall Position', '').lower()\n",
    "                    if 'support' in position:\n",
    "                        position_key = 'support'\n",
    "                    elif 'oppose' in position:\n",
    "                        position_key = 'oppose'\n",
    "                    elif 'mixed' in position or 'neutral' in position:\n",
    "                        position_key = 'mixed'\n",
    "                    else:\n",
    "                        position_key = 'unknown'\n",
    "                        \n",
    "                    stats[\"overall\"][\"positions\"][position_key] += 1\n",
    "                    stats[\"by_tranche\"][tranche_name][\"positions\"][position_key] += 1\n",
    "                    \n",
    "                    # Process stakeholder type\n",
    "                    stakeholder = analysis.get('Stakeholder Type', 'unknown').lower()\n",
    "                    stats[\"by_stakeholder\"][stakeholder][position_key] += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing analysis file {json_file}: {str(e)}\")\n",
    "                    stats[\"overall\"][\"positions\"][\"unknown\"] += 1\n",
    "                    stats[\"by_tranche\"][tranche_name][\"positions\"][\"unknown\"] += 1\n",
    "            else:\n",
    "                stats[\"overall\"][\"positions\"][\"unknown\"] += 1\n",
    "                stats[\"by_tranche\"][tranche_name][\"positions\"][\"unknown\"] += 1\n",
    "    \n",
    "    # Calculate percentages for overall statistics\n",
    "    analyzed_total = stats[\"overall\"][\"analyzed\"]\n",
    "    if analyzed_total > 0:\n",
    "        for position in [\"support\", \"oppose\", \"mixed\", \"unknown\"]:\n",
    "            count = stats[\"overall\"][\"positions\"][position]\n",
    "            stats[\"overall\"][\"percentages\"][position] = round(count / analyzed_total * 100, 1)\n",
    "    \n",
    "    # Calculate percentages for each tranche\n",
    "    for tranche in stats[\"by_tranche\"].values():\n",
    "        if tranche[\"analyzed\"] > 0:\n",
    "            for position in [\"support\", \"oppose\", \"mixed\", \"unknown\"]:\n",
    "                count = tranche[\"positions\"][position]\n",
    "                tranche[\"percentages\"][position] = round(count / tranche[\"analyzed\"] * 100, 1)\n",
    "    \n",
    "    # Update metadata\n",
    "    stats[\"metadata\"][\"tranches_analyzed\"] = len(tranche_files)\n",
    "    \n",
    "    # Save results\n",
    "    with open('results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stats = calculate_statistics()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nOverall Statistics:\")\n",
    "    print(f\"Total Submissions: {stats['overall']['total']}\")\n",
    "    print(f\"Analyzed: {stats['overall']['analyzed']}\")\n",
    "    print(\"\\nPositions:\")\n",
    "    for position, percentage in stats['overall']['percentages'].items():\n",
    "        count = stats['overall']['positions'][position]\n",
    "        print(f\"{position.title()}: {count} ({percentage}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Missing analysis file: acma2023-17518-anonymous.json\n",
      "âš ï¸ Missing analysis file: acma2023-18557-anonymous .json\n",
      "âš ï¸ Missing analysis file: acma2023-21187-anthony-alaveras .json\n",
      "âš ï¸ Missing analysis file: acma2023-21878-noah-abbey.json\n",
      "âš ï¸ Missing analysis file: acma2023-27270-suri-ratnapala-and-adrian-ratnapala.json\n",
      "âš ï¸ Missing analysis file: acma2023-27957-regional-development-australia-southern-inland.json\n",
      "âš ï¸ Missing analysis file: acma2023-28523-democratic-labour-party-south-australia.json\n",
      "âš ï¸ Missing analysis file: acma2023-33079-rob-d'ermilo.json\n",
      "âš ï¸ Missing analysis file: acma2023-34662-feminist-legal-clinic-inc..json\n",
      "âš ï¸ Missing analysis file: acma2023-e2410-michael-de stoop.json\n",
      "âš ï¸ Missing analysis file: acma2023-e2420-murray-may.json\n",
      "âš ï¸ Missing analysis file: acma2023-e2910-alexander-cornell stewart.json\n",
      "âš ï¸ Missing analysis file: acma2023-e3675-lynda-crawford.json\n",
      "âš ï¸ Missing analysis file: acma2023-33576-carolyn-delzoppo.json\n",
      "âš ï¸ Missing analysis file: acma2023-34751-anonymous.json\n",
      "âš ï¸ Missing analysis file: acma2023-e3741-ralph-babet-united-australia-party.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    # Extract tranche number for sorting\n",
    "    match = re.search(r'tranche-(\\d+)', s)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def get_position_emoji(analysis, filename):\n",
    "    if not analysis or 'Overall Position' not in analysis:\n",
    "        print(f\"âš ï¸ Missing Overall Position in analysis for {filename}\")\n",
    "        return \"â“\"  # Unknown\n",
    "    position = analysis['Overall Position'].lower()\n",
    "    if 'support' in position:\n",
    "        return \"âœ…\"  # Support\n",
    "    elif 'oppose' in position:\n",
    "        return \"âŒ\"  # Oppose\n",
    "    elif 'mixed' in position or 'neutral' in position:\n",
    "        return \"â†”ï¸\"  # Mixed/Neutral\n",
    "    print(f\"âš ï¸ Unknown position value: {position} for {filename}\")\n",
    "    return \"â“\"  # Unknown\n",
    "\n",
    "def get_stakeholder_emoji(analysis):\n",
    "    if not analysis or 'Stakeholder Type' not in analysis:\n",
    "        print(f\"âš ï¸ Missing Stakeholder Type in analysis\")\n",
    "        return \"â“\"\n",
    "    stakeholder = analysis['Stakeholder Type'].lower()\n",
    "    \n",
    "    # Skip processing if N/A\n",
    "    if stakeholder in ['n/a', 'unknown']:\n",
    "        return \"â“\"\n",
    "        \n",
    "    # Business/Industry/Think Tank\n",
    "    if any(x in stakeholder for x in [\n",
    "        'company', 'business', 'industry association', \n",
    "        'media outlet', 'think tank'\n",
    "    ]):\n",
    "        return \"ðŸ¢\"\n",
    "    \n",
    "    # Academic/Research\n",
    "    elif 'academic' in stakeholder:\n",
    "        return \"ðŸŽ“\"\n",
    "    \n",
    "    # Individual\n",
    "    elif 'individual' in stakeholder:\n",
    "        return \"ðŸ‘¤\"\n",
    "    \n",
    "    # Civil Society/NGO/International/Religious\n",
    "    elif any(x in stakeholder for x in [\n",
    "        'civil society', 'ngo', 'international organization',\n",
    "        'religious organization', 'political party', 'educational institution', \n",
    "        'media organization', 'industry body', 'multistakeholder organization'\n",
    "    ]):\n",
    "        return \"ðŸŒ\"\n",
    "    \n",
    "    # Government/Legal\n",
    "    elif any(x in stakeholder for x in ['government', 'legal', 'legislative']):\n",
    "        return \"ðŸ›ï¸\"\n",
    "    \n",
    "    # If we get here, it's an unhandled type\n",
    "    print(f\"âš ï¸ Unknown stakeholder type: {stakeholder}\")\n",
    "    return \"â“\"\n",
    "\n",
    "def generate_summary_markdown():\n",
    "    tranches_dir = 'tranches'\n",
    "    submissions_dir = 'submissions'\n",
    "    output_file = 'SUMMARY.md'\n",
    "    \n",
    "    # Get all tranche JSON files\n",
    "    tranche_files = [f for f in os.listdir(tranches_dir) if f.endswith('.json')]\n",
    "    tranche_files.sort(key=natural_sort_key)\n",
    "    \n",
    "    # Load statistics\n",
    "    try:\n",
    "        with open('results.json', 'r', encoding='utf-8') as f:\n",
    "            stats = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error loading results.json: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as out:\n",
    "        out.write(\"# Submissions Summary\\n\\n\")\n",
    "        \n",
    "        # Write overall statistics\n",
    "        out.write(\"## Overall Statistics\\n\\n\")\n",
    "        out.write(f\"Total Submissions: **{stats['overall']['total']}**  \\n\")\n",
    "        out.write(f\"Analyzed: **{stats['overall']['analyzed']}**  \\n\\n\")\n",
    "        \n",
    "        # Write position percentages\n",
    "        out.write(\"### Positions\\n\\n\")\n",
    "        out.write(\"| Position | Count | Percentage |\\n\")\n",
    "        out.write(\"|----------|-------|------------|\\n\")\n",
    "        for position in [\"support\", \"oppose\", \"mixed\", \"unknown\"]:\n",
    "            count = stats['overall']['positions'][position]\n",
    "            percentage = stats['overall']['percentages'][position]\n",
    "            emoji = \"âœ…\" if position == \"support\" else \"âŒ\" if position == \"oppose\" else \"â†”ï¸\" if position == \"mixed\" else \"â“\"\n",
    "            out.write(f\"| {emoji} {position.title()} | {count} | {percentage}% |\\n\")\n",
    "        out.write(\"\\n\")\n",
    "        \n",
    "        # Write stakeholder statistics\n",
    "        out.write(\"### By Stakeholder Type\\n\\n\")\n",
    "        out.write(\"| Type | Support | Oppose | Mixed | Unknown |\\n\")\n",
    "        out.write(\"|------|---------|---------|--------|----------|\\n\")\n",
    "        \n",
    "        stakeholder_emojis = {\n",
    "            \"company\": \"ðŸ¢\", \"business\": \"ðŸ¢\", \"industry association\": \"ðŸ¢\", \n",
    "            \"media outlet\": \"ðŸ¢\", \"think tank\": \"ðŸ¢\",\n",
    "            \"academic\": \"ðŸŽ“\",\n",
    "            \"individual\": \"ðŸ‘¤\",\n",
    "            \"civil society\": \"ðŸŒ\", \"ngo\": \"ðŸŒ\", \"international organization\": \"ðŸŒ\",\n",
    "            \"religious organization\": \"ðŸŒ\", \"political party\": \"ðŸŒ\",\n",
    "            \"government\": \"ðŸ›ï¸\", \"legal\": \"ðŸ›ï¸\", \"legislative\": \"ðŸ›ï¸\"\n",
    "        }\n",
    "        \n",
    "        for stakeholder, positions in stats['by_stakeholder'].items():\n",
    "            if stakeholder == \"unknown\":\n",
    "                continue\n",
    "            emoji = next((v for k, v in stakeholder_emojis.items() if k in stakeholder.lower()), \"â“\")\n",
    "            out.write(f\"| {emoji} {stakeholder.title()} | {positions['support']} | {positions['oppose']} | {positions['mixed']} | {positions['unknown']} |\\n\")\n",
    "        out.write(\"\\n\")\n",
    "        \n",
    "        # Write tranche statistics\n",
    "        out.write(\"### By Tranche\\n\\n\")\n",
    "        out.write(\"| Tranche | Date | Total | Support | Oppose | Mixed |\\n\")\n",
    "        out.write(\"|---------|------|-------|---------|---------|-------|\\n\")\n",
    "        \n",
    "        for tranche_name, tranche_data in stats['by_tranche'].items():\n",
    "            support_pct = tranche_data['percentages'].get('support', 0)\n",
    "            oppose_pct = tranche_data['percentages'].get('oppose', 0)\n",
    "            mixed_pct = tranche_data['percentages'].get('mixed', 0)\n",
    "            \n",
    "            out.write(f\"| {tranche_name} | {tranche_data['date']} | {tranche_data['analyzed']}/{tranche_data['total']} | \")\n",
    "            out.write(f\"{support_pct}% | {oppose_pct}% | {mixed_pct}% |\\n\")\n",
    "        out.write(\"\\n\")\n",
    "        \n",
    "        # Write submission details\n",
    "        out.write(\"## Submissions by Tranche\\n\\n\")\n",
    "        \n",
    "        for tranche_file in tranche_files:\n",
    "            with open(os.path.join(tranches_dir, tranche_file), 'r', encoding='utf-8') as f:\n",
    "                tranche_data = json.load(f)\n",
    "            \n",
    "            # Write tranche header\n",
    "            tranche_date = tranche_data['metadata']['tranche_date']\n",
    "            out.write(f\"### {os.path.splitext(tranche_file)[0]} ({tranche_date})\\n\\n\")\n",
    "            \n",
    "            # Write table header\n",
    "            out.write(\"| # | Submitter | PDF | Analysis | Position | Type |\\n\")\n",
    "            out.write(\"|---|-----------|-----|----------|----------|------|\\n\")\n",
    "            \n",
    "            # Write table rows\n",
    "            for i, (pdf_file, submitter) in enumerate(tranche_data['submissions'].items(), 1):\n",
    "                # Get corresponding JSON analysis file if it exists\n",
    "                base_name = os.path.splitext(pdf_file)[0]\n",
    "                json_file = f\"{base_name}.json\"\n",
    "                json_path = os.path.join(submissions_dir, json_file)\n",
    "                \n",
    "                analysis = None\n",
    "                if os.path.exists(json_path):\n",
    "                    try:\n",
    "                        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                            analysis_data = json.load(f)\n",
    "                            analysis = analysis_data.get('analysis', {})\n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ Error reading analysis file {json_file}: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"âš ï¸ Missing analysis file: {json_file}\")\n",
    "                    continue\n",
    "                \n",
    "                pdf_link = f\"[PDF](submissions/{pdf_file})\" if os.path.exists(os.path.join(submissions_dir, pdf_file)) else \"âŒ\"\n",
    "                json_link = f\"[Analysis](submissions/{json_file})\" if os.path.exists(json_path) else \"âŒ\"\n",
    "                \n",
    "                position_emoji = get_position_emoji(analysis, json_path)\n",
    "                stakeholder_emoji = get_stakeholder_emoji(analysis)\n",
    "                \n",
    "                out.write(f\"| {i} | {submitter} | {pdf_link} | {json_link} | {position_emoji} | {stakeholder_emoji} |\\n\")\n",
    "            \n",
    "            out.write(\"\\n\")  # Add space between tables\n",
    "        \n",
    "        # Write legend\n",
    "        out.write(\"\\n## Legend\\n\\n\")\n",
    "        out.write(\"### Position\\n\")\n",
    "        out.write(\"- âœ… Support\\n\")\n",
    "        out.write(\"- âŒ Oppose\\n\")\n",
    "        out.write(\"- â†”ï¸ Mixed/Neutral\\n\")\n",
    "        out.write(\"- â“ Unknown\\n\\n\")\n",
    "        out.write(\"### Stakeholder Type\\n\")\n",
    "        out.write(\"- ðŸ¢ Company/Business\\n\")\n",
    "        out.write(\"- ðŸŽ“ Academic\\n\")\n",
    "        out.write(\"- ðŸ‘¤ Individual\\n\")\n",
    "        out.write(\"- ðŸŒ Civil Society/NGO\\n\")\n",
    "        out.write(\"- ðŸ›ï¸ Government\\n\")\n",
    "        out.write(\"- â“ Unknown\\n\")\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_summary_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data from analysis files...\n",
      "Summarizing arguments in chunks...\n",
      "Split 174 arguments into 4 chunks\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "summarize_chunk() missing 2 required positional arguments: 'chunk_num' and 'total_chunks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 239\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError generating commentary: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 239\u001b[0m     \u001b[43mgenerate_commentary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 113\u001b[0m, in \u001b[0;36mgenerate_commentary\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m mixed_summaries \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunk_arguments(support_arguments):\n\u001b[0;32m--> 113\u001b[0m     support_summaries\u001b[38;5;241m.\u001b[39mextend(\u001b[43msummarize_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msupporting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunk_arguments(oppose_arguments):\n\u001b[1;32m    116\u001b[0m     oppose_summaries\u001b[38;5;241m.\u001b[39mextend(summarize_chunk(chunk, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopposing\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: summarize_chunk() missing 2 required positional arguments: 'chunk_num' and 'total_chunks'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "from openai import OpenAI\n",
    "\n",
    "def chunk_arguments(arguments: List[Tuple[str, str]], chunk_size: int = 50) -> List[List[Tuple[str, str]]]:\n",
    "    \"\"\"Split arguments into smaller chunks\"\"\"\n",
    "    return [arguments[i:i + chunk_size] for i in range(0, len(arguments), chunk_size)]\n",
    "\n",
    "def summarize_chunk(arguments: List[Tuple[str, str]], position: str) -> str:\n",
    "    \"\"\"Summarize a chunk of arguments\"\"\"\n",
    "    prompt = f\"\"\"Summarize the following {position} arguments about ACMA's proposed misinformation powers into 3-5 key points.\n",
    "\n",
    "    Arguments:\n",
    "    {json.dumps([arg[0] for arg in arguments], indent=2)}\n",
    "\n",
    "    Return a JSON object with a single field 'key_points' containing an array of the summarized points.\n",
    "    Format: {{\"key_points\": [\"point 1\", \"point 2\", ...]}}\n",
    "    \"\"\"\n",
    "    \n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are summarizing consultation submissions. Always return a JSON object with a 'key_points' array.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        temperature=0.1,\n",
    "        seed=1\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return result.get('key_points', [])\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"Error parsing chunk summary response: {str(e)}\")\n",
    "        print(f\"Response content: {response.choices[0].message.content}\")\n",
    "        return []\n",
    "\n",
    "def generate_commentary():\n",
    "    submissions_dir = 'submissions'\n",
    "    output_file = 'COMMENTARY.md'\n",
    "    \n",
    "    # Collect all arguments and themes\n",
    "    support_arguments = []\n",
    "    oppose_arguments = []\n",
    "    mixed_arguments = []\n",
    "    all_themes = set()\n",
    "    notable_quotes = []\n",
    "    \n",
    "    # Process each analysis file\n",
    "    print(\"Collecting data from analysis files...\")\n",
    "    for filename in os.listdir(submissions_dir):\n",
    "        if not filename.endswith('.json'):\n",
    "            continue\n",
    "            \n",
    "        json_path = os.path.join(submissions_dir, filename)\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                analysis = data.get('analysis', {})\n",
    "                \n",
    "                # Skip if no analysis\n",
    "                if not analysis:\n",
    "                    continue\n",
    "                    \n",
    "                position = analysis.get('Overall Position', '').lower()\n",
    "                arguments = analysis.get('Key Arguments', [])\n",
    "                themes = analysis.get('Main Themes', [])\n",
    "                quotes = analysis.get('Notable Quotes', [])\n",
    "                \n",
    "                # Add themes to overall set\n",
    "                all_themes.update(themes)\n",
    "                \n",
    "                # Categorize arguments by position\n",
    "                if 'support' in position:\n",
    "                    support_arguments.extend([(arg, filename) for arg in arguments])\n",
    "                    notable_quotes.extend([(q, filename, \"support\") for q in quotes])\n",
    "                elif 'oppose' in position:\n",
    "                    oppose_arguments.extend([(arg, filename) for arg in arguments])\n",
    "                    notable_quotes.extend([(q, filename, \"oppose\") for q in quotes])\n",
    "                elif 'mixed' in position or 'neutral' in position:\n",
    "                    mixed_arguments.extend([(arg, filename) for arg in arguments])\n",
    "                    notable_quotes.extend([(q, filename, \"mixed\") for q in quotes])\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    # Load overall statistics\n",
    "    try:\n",
    "        with open('results.json', 'r', encoding='utf-8') as f:\n",
    "            stats = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading results.json: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Summarize arguments in chunks\n",
    "    print(\"Summarizing arguments in chunks...\")\n",
    "    support_summaries = []\n",
    "    oppose_summaries = []\n",
    "    mixed_summaries = []\n",
    "    \n",
    "    for chunk in chunk_arguments(support_arguments):\n",
    "        support_summaries.extend(summarize_chunk(chunk, \"supporting\"))\n",
    "    \n",
    "    for chunk in chunk_arguments(oppose_arguments):\n",
    "        oppose_summaries.extend(summarize_chunk(chunk, \"opposing\"))\n",
    "    \n",
    "    for chunk in chunk_arguments(mixed_arguments):\n",
    "        mixed_summaries.extend(summarize_chunk(chunk, \"mixed/neutral\"))\n",
    "    \n",
    "    # Generate final analysis\n",
    "    print(\"Generating final analysis...\")\n",
    "    final_prompt = f\"\"\"Based on these summarized arguments about ACMA's proposed misinformation powers, \n",
    "    please provide a comprehensive analysis.\n",
    "\n",
    "    Supporting Arguments:\n",
    "    {json.dumps(support_summaries, indent=2)}\n",
    "\n",
    "    Opposing Arguments:\n",
    "    {json.dumps(oppose_summaries, indent=2)}\n",
    "\n",
    "    Mixed/Neutral Arguments:\n",
    "    {json.dumps(mixed_summaries, indent=2)}\n",
    "\n",
    "    Main Themes:\n",
    "    {json.dumps(list(all_themes), indent=2)}\n",
    "\n",
    "    Please provide a detailed analysis with the following sections:\n",
    "    1. executive_summary: Brief overview (2-3 paragraphs)\n",
    "    2. key_supporting_arguments: Main arguments in favor (bullet points)\n",
    "    3. key_opposing_arguments: Main arguments against (bullet points)\n",
    "    4. major_themes: Analysis of recurring themes (bullet points)\n",
    "    5. stakeholder_perspectives: Different viewpoints (1-2 paragraphs)\n",
    "    6. notable_considerations: Important nuances (bullet points)\n",
    "    7. conclusion: Overall assessment (1 paragraph)\n",
    "    \n",
    "    Return as a JSON object with these exact field names.\n",
    "    \"\"\"\n",
    "    \n",
    "    client = OpenAI()\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert policy analyst reviewing public consultation submissions.\"},\n",
    "                {\"role\": \"user\", \"content\": final_prompt}\n",
    "            ],\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            temperature=0.1,\n",
    "            seed=1\n",
    "        )\n",
    "        \n",
    "        analysis = json.loads(response.choices[0].message.content)\n",
    "        \n",
    "        # Write the commentary\n",
    "        print(\"Writing commentary...\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# Analysis of ACMA Misinformation Powers Consultation\\n\\n\")\n",
    "            f.write(f\"*Generated on {datetime.now().strftime('%Y-%m-%d')}*\\n\\n\")\n",
    "            \n",
    "            # Add statistics\n",
    "            f.write(\"## Consultation Overview\\n\\n\")\n",
    "            f.write(f\"Total Submissions: **{stats['overall']['total']}**  \\n\")\n",
    "            f.write(f\"Analyzed: **{stats['overall']['analyzed']}**  \\n\\n\")\n",
    "            f.write(\"### Position Breakdown\\n\\n\")\n",
    "            f.write(\"| Position | Count | Percentage |\\n\")\n",
    "            f.write(\"|----------|-------|------------|\\n\")\n",
    "            for position in [\"support\", \"oppose\", \"mixed\", \"unknown\"]:\n",
    "                count = stats['overall']['positions'][position]\n",
    "                percentage = stats['overall']['percentages'][position]\n",
    "                emoji = \"âœ…\" if position == \"support\" else \"âŒ\" if position == \"oppose\" else \"â†”ï¸\" if position == \"mixed\" else \"â“\"\n",
    "                f.write(f\"| {emoji} {position.title()} | {count} | {percentage}% |\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Add AI analysis\n",
    "            f.write(\"## Executive Summary\\n\\n\")\n",
    "            f.write(f\"{analysis['executive_summary']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Key Supporting Arguments\\n\\n\")\n",
    "            for point in analysis['key_supporting_arguments']:\n",
    "                f.write(f\"- {point}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"## Key Opposing Arguments\\n\\n\")\n",
    "            for point in analysis['key_opposing_arguments']:\n",
    "                f.write(f\"- {point}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"## Major Themes\\n\\n\")\n",
    "            for theme in analysis['major_themes']:\n",
    "                f.write(f\"- {theme}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"## Stakeholder Perspectives\\n\\n\")\n",
    "            f.write(f\"{analysis['stakeholder_perspectives']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Notable Considerations\\n\\n\")\n",
    "            for point in analysis['notable_considerations']:\n",
    "                f.write(f\"- {point}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"## Conclusion\\n\\n\")\n",
    "            f.write(f\"{analysis['conclusion']}\\n\\n\")\n",
    "            \n",
    "            # Add notable quotes section\n",
    "            f.write(\"## Notable Quotes\\n\\n\")\n",
    "            f.write(\"### Supporting Positions\\n\\n\")\n",
    "            for quote, filename, position in notable_quotes:\n",
    "                if position == \"support\":\n",
    "                    f.write(f\"> {quote}\\n\")\n",
    "                    f.write(f\"*â€” [{os.path.splitext(filename)[0]}](submissions/{filename})*\\n\\n\")\n",
    "            \n",
    "            f.write(\"### Opposing Positions\\n\\n\")\n",
    "            for quote, filename, position in notable_quotes:\n",
    "                if position == \"oppose\":\n",
    "                    f.write(f\"> {quote}\\n\")\n",
    "                    f.write(f\"*â€” [{os.path.splitext(filename)[0]}](submissions/{filename})*\\n\\n\")\n",
    "            \n",
    "            f.write(\"### Mixed/Neutral Positions\\n\\n\")\n",
    "            for quote, filename, position in notable_quotes:\n",
    "                if position == \"mixed\":\n",
    "                    f.write(f\"> {quote}\\n\")\n",
    "                    f.write(f\"*â€” [{os.path.splitext(filename)[0]}](submissions/{filename})*\\n\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating commentary: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_commentary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
