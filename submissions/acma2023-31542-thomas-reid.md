# Executive Summary

### 1. The draft Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2023 is a dagger pointed at the heart of ordinary Australians’ freedom of speech.  It should be withdrawn in its entirety and not proceeded with.

 • There is no compelling case for government to regulate online content on the basis of supposed truth or falsity.

 • The powers created by the Bill are easily abused, and there is recent evidence showing that they will be abused if the Bill is enacted.

 • The Bill breaches Article 19, paragraph 2 of the International Covenant on Civil and Political Rights (ICCPR), which is in these terms:

 “Everyone shall have the right to freedom of expression; this right shall include freedom to seek, receive and impart information and ideas of all kinds, regardless of frontiers, either orally, in writing or in print, in the form of art, or through any other media of his choice.”

 2. The Bill makes no attempt to confine its impact on free speech to its stated purpose. In particular, it lacks safeguards to prevent arbitrary and unnecessary restriction of free speech.

 • The Bill provides no redress for content creators or consumers against arbitrary or incorrect decisions by digital platforms or ACMA.

 • Its design minimises the chances of decisions, or the misinformation codes or standards under which they are made, being the subject of independent review.

 • The provisions to protect the implied freedom of political communication are likely to be ineffective in practice.

 • The exclusions cover only those content creators whom the government trusts to stay within an acceptable perimeter around the official narrative.

 3. The Bill requires neither digital platform providers nor ACMA to report publicly on the extent or details of their censorship activities, and the effect of these on freedom of speech.

 4. Compliance costs imposed on digital platform providers create incentives to apply broad brush censorship practices.

 5. This submission focuses exclusively on how the Bill affects freedom of speech. However, there appear to be significant privacy issues associated with its potential application to non-public communications. 


-----

# Table of contents
### Executive Summary 1

 Table of contents 2

 There is no compelling case for government to regulate online content on the basis of supposed truth or falsity 4

 The real purpose and effect of the Bill 5

 The powers created by the Bill are easily abused 8

 Ministerial directions 8

 Current censorship arrangements between the Commonwealth government and digital platform providers 8

 The Bill makes no meaningful attempt to minimise its impact on free speech and lacks safeguards 11

 Statements of regulatory policy 11

 Application of the definitions of mis- and disinformation by platform providers 11

 “False, misleading or deceptive” 12

 “Reasonably likely to cause or contribute to serious harm” 12

 Censorship decisions by platform providers will in most cases not be subject to effective independent review 13

 No requirement for misinformation code or standard to provide right of review 13

 Judicial review under the Administrative Decisions (Judicial Review) Act 1977
 13

 ADJR review of censorship decisions under misinformation code 14

 ADJR review of censorship decisions under misinformation standard 14

 Judicial review of limited utility even if available 15

 Judicial review by way of prerogative writ 15

 Conclusion 15

 Role of ACMA as de facto censor of last resort 15

 ACMA will need to monitor online content 16

 ACMA power to censor online content by remedial direction 16

 ACMA power to censor online content indirectly by means of infringement notice 18


-----

### Lack of remedies for content creators against decisions by ACMA 19

 AAT review of ACMA decisions does not help content creators 19

 ADJR review of ACMA remedial directions enforcing codes and standards 19

 The provisions to protect the implied freedom of political communication are likely to be ineffective in practice 20

 What the other exemptions reveal about the intent of the Bill 21

 Duplication of consumer law and defamation remedies without due process safeguards 22

 Lack of transparency and accountability of the censors 22

 Compliance costs create incentive to apply broad brush censorship practices 23


-----

# There is no compelling case for government to regulate online content on the basis of supposed truth or falsity

### In a free society based on the rule of law, censorship is not intrinsic to the legitimate role of government, whether the censorship is described as “controlling the narrative”, “being your single source of truth” or “combating misinformation and disinformation”.

 Freedom of speech and of the press are fundamental rights of individual citizens. If citizens abuse those freedoms so as to infringe the rights of other citizens, remedies already exist under the general criminal law and the law of torts (defamation, deceit), and specific statutory regimes such as the Australian Consumer Law.

 In Nationwide News Pty Ltd v Wills [1992] HCA 46; (1992) 177 CLR 1, High Court Chief Justice Mason wrote (at paragraph 19, emphasis added):

 The fundamental importance of freedom of expression in modern democratic society is recognized in the following statements:

 The Commonwealth of Australia v. John Fairfax and Sons Ltd. [1980] HCA 44; (1980) 147 CLR 39, per Mason J. at p 52: “It is unacceptable in our democratic society that there should be a restraint on the publication of information relating to government when the only vice of that information is that it enables the public to discuss, review and criticize government action.”

 Attorney-General v. Times Newspapers (1974) AC 273, per Lord Simon of Glaisdale at p 315: “The first public interest involved is that of freedom of discussion in democratic society. People cannot adequately influence the decisions which affect their lives unless they can be adequately informed on facts and arguments relevant to the decisions. Much of such fact-finding and argumentation necessarily has to be conducted vicariously, the public press being a principal instrument.”

 Smith v. Daily Mail Publishing Co. (1979) 443 US 97, per Rehnquist J. at p 106: “Historically, we have viewed freedom of speech and of the press as indispensable to a free society and its government.”

 Censorship based on the assertion that the censored material is “misinformation or disinformation” is especially pernicious. Not only is the government suppressing speech, it is arrogating to itself the right to decide whether the speech is true or false. The Bill reflects this arrogance by exempting content that is authorised by the


-----

### Commonwealth, a State, a Territory, or a local government. In other words, 1
 governments at all levels are not held to the same standard as the Bill seeks to apply to content created by private citizens.

 The Bill’s regime for suppressing online content has no counterpart in the body of law governing publication of content in printed form. Neither the Commonwealth Parliament nor a State or Territory legislature would dare to impose a similar regime on printed content, such would be the outcry about infringing freedom of the press.

 Far from striking “an appropriate balance of a range of issues such as freedom of expression”, the Bill is a dagger pointed at the heart of ordinary Australians’ freedom 2
 of speech. It should be withdrawn in its entirety and not proceeded with.

## The real purpose and effect of the Bill

### The main thrust of the Bill is to create various mechanisms that force digital platform providers to “implement measures to prevent or respond to misinformation and disinformation on digital platform services”. These mechanisms involve ACMA deciding whether “adequate protection for the community from misinformation or disinformation on the services” is being provided.3

 The amount of content available online is vast. Much of it is true. Much of it is false. Which of these categories a given piece of content falls into is very often unverifiable and in many cases unknowable.

 Furthermore, much online content, perhaps the greatest part, is neither true nor false, in the sense that it sets out interpretations of facts, expresses opinions about what is true, or makes predictions about future events. Interpretations, opinions and predictions are all debatable. By their very nature they are incapable of being misinformation. If posted with intent to deceive, these kinds of content might be

1 See subparagraph (e)(i) of the definition of excluded content for misinformation purposes in clause 2.

- References in this submission to clauses, and to provisions within clauses, are to clauses and provisions of

the new Schedule 9 that item 2 of Schedule 1 to the Bill proposes to insert into the Broadcasting Services
_Act 1992._

- References to Schedule 9 are references to the Schedule inserted by that item.

- References to the Principal Act are references to that Act.

- References to the Guidance Note are references to the Guidance Note for the Bill.

2 Page 6 of the Guidance Note.

3 Subparagraph 37(1)(e)(iii), subparagraph 40(1)(e)(iii), paragraph 46(1)(c), paragraph 47(1)(e) and
subclause 51(1).


-----

### disinformation, but that will often not be apparent on the face of the content in its 4
 immediate context.[5]

 Given all that, and the fact that no government agency is omniscient, how can ACMA possibly assess the adequacy, in the aggregate, of measures to prevent or respond to false information online?

 The obvious answer is that ACMA cannot do so. Once that is understood, the real purpose of the Bill becomes apparent. It is to provide the means for particular content to be censored.

 On page 7 of the Guidance Note, it is asserted that:

 The Bill does not seek to curtail freedom of speech, nor is it intended that powers will be used to remove individual pieces of content on a platform.

 These claims are completely contradicted by the design and content of the Bill itself. How is it possible to “to prevent … misinformation and disinformation on digital platform services” without censoring, that is rejecting or removing, particular 6
 content? There are a range of ways to “respond to” mis- or disinformation once it is on a platform, but they mostly involve addressing particular content.

 The whole point of the Bill is to ensure that digital platform providers censor particular content. As shown later in this submission, the Bill also empowers ACMA to require, both directly through remedial directions and indirectly through infringement notices and civil penalty proceedings, the censorship of particular content.

4 That being the case, subclause 7(2) may define disinformation too narrowly. I do not however propose that
it be broadened.

5 There appear to be only 2 places where the Bill refers to disinformation without also referring to
misinformation:

- For some reason, subclause 7(2) makes the somewhat circular point that “Disinformation includes

disinformation by or on behalf of a foreign power.”.

- Subclause 35(1) excludes electoral and referendum content from the scope of misinformation codes and

misinformation standards except to prevent or respond to a particular kind of disinformation.

As a matter of drafting, this distinction could have been made without creating the category of
disinformation, which is otherwise redundant, apart from the issue raised in footnote 4 above.
This redundancy is recognised in the Guidance Note by a remarkably naïve statement at the top of
page 11:

The use of [sic] the terms ‘misinformation’ and ‘disinformation’ are used interchangeably in the
remainder of this Guidance Note, and are intended to refer to both misinformation and
disinformation, unless otherwise specified, or clear from its context that the reference is meant to
refer to just one concept or the other.

The references in the Bill to disinformation appear to have the purely rhetorical function of arousing fear and
loathing in the public mind, owing to disinformation being more reprehensible.

6 Clause 32 and numerous other places in Schedule 9.


-----

### Which particular content can be seen by reading the definition of harm in clause 2, which clause 7 relies on in defining mis- and disinformation.

 The definition of harm sets out a checklist of topics about which governments have in recent times sought to set up an official narrative, and about which there has also been enormous controversy in public debate, generating a great deal of online content whose truth or falsity, even on purely factual matters, is disputed and cannot be definitively established.

 Major examples are:

 • race relations, including the upcoming referendum on an Indigenous Voice to the
 Parliament (paragraph (a) of the definition);

 • gender ideology, including treatments for gender dysphoria in children and
 adolescents (ditto);

 • Vaccine safety, vaccine mandates, lockdowns, mask mandates and other aspects
 of government response to the COVID pandemic (paragraph (d));

 • Climate and energy policy, including debates about climate science (paragraph
 (e)).

 This is exactly the kind of content that the right of free speech and freedom of the press exist to protect, yet it is clear that the Bill is targeting it.

 If the government proceeds with this Bill, it will be undermining the foundation of a free and open society. The right of free speech is premised on distrust of governments’ motives for restricting speech. This Bill amply demonstrates why that distrust is well-founded.

 Every orthodoxy regards its opponents’ views as false and harmful. This Bill equips the government with the means to suppress every view that disputes or dissents from government orthodoxy. The fact that the government even wants this regime is already a red flag.

 Contrary to what the Bill asserts about “protecting the community”, censorship 7
 doesn’t protect anyone from misinformation. A government that censors what can be seen and read by adult citizens is like an overprotective parent who never lets their child grow up.

 The proper way to protect against the risks that arise from incorrect information in the public square is by putting out correct information that is equally freely available. The internet provides ample opportunity for the government to do this.

7 See footnote 3 above and subclauses 4(6), 48(6) and 49(6).


-----

### In a free society adults get to make up their own minds about what is true or untrue, harmful or not harmful.

 Yet in this Bill the government seeks to be able to shut down views that are inconsistent with its own. How the Bill achieves this is discussed below.

 The Guidance Note mentions content “being reported and complained about” on 8
 digital platforms. How platform providers and their users deal with user misconduct on platforms is a matter for the users and the providers. It is not a legitimate concern of government except insofar as it involves conduct that is unlawful.

# The powers created by the Bill are easily abused

## Ministerial directions

### Under subsection 14(1) of the Australian Communications and Media Authority Act 2005, the Minister administering that Act “may give written directions to the ACMA in relation to the performance of its functions and the exercise of its powers”.

 Subsection 14(2) limits this power to directions “of a general nature” in the case of ACMA’s new functions under the Bill, because those functions are inserted into 9
 section 10 of the Act, which sets out broadcasting, content and datacasting functions. The same is likely true of ACMA’s new powers under the Bill, because they relate to the new functions.

 Nonetheless, subsection 14(2) would not prevent the Minister from directing ACMA, in performing and exercising those new functions and powers, to pay particular attention to mis- and disinformation of a particular kind, for example, content that challenges government energy policy, the government’s policy responses to climate change or government information about COVID treatments such as Ivermectin.

## Current censorship arrangements between the Commonwealth government and digital platform providers

### Recent history shows that the Commonwealth Government, including officers of the Australian Public Service, does not require statutory authority to achieve de facto censorship of online content.

 This was revealed by answers to questions asked by Senator Antic in the Senate Legal and Constitutional Affairs Committee on 22 May 2023 about the Department of

8 On page 15 “Scenario 1: Record keeping rules”.

9 See item 2 of Schedule 2 to the Bill.


-----

### Home Affairs’ “Online Content Incident Arrangement Procedural Guideline”, which was also the subject of an FOI request by Senator Antic .10

 Attached to this submission is the Hansard transcript of the hearing in which those questions were asked and answered. I have highlighted the relevant passages from pages 49 to 51. In summary, what they describe is a process under which, as described by Mr Pezzullo, the Secretary of the Department of Home Affairs (emphasis added):

 Following a decision of the Morrison government in the early part of the
 [COVID] pandemic [...] Mr Hunt [then Minister for Health] announced some work that Health, in conjunction with other partners and departments, would undertake from about March or April of 2020. [...] The Department of Home Affairs, because we’re scanning social media for other purposes—terrorist incidents being live streamed and the like—if we came across material that, in the department of health’s view, offended, or potentially offended, the disinformation and misinformation standards that the technology companies themselves have in place around COVID, as an assistant or an adjunct to the department of health—typically; I don’t know if they’ve since built a 24/7 referral capability—we would take that on as, if you like, a utility player able to assist the department of health.

 Mr Pezzullo made it clear that this action did not purport to be based on the statutory powers of any agency. It follows that, if the action was lawful at all, it was an exercise of the executive power of the Commonwealth .11

 An official of the Department also makes the point that any citizen can make a 12
 referral to a platform provider about content on the platform that breaches the provider’s terms of service.

 The disclosures made at the Senate Committee hearing highlight these important points that are relevant to how the Bill will operate in practice:

 1. Even under the current law, online content is being censored on the basis of platform providers’ terms of service, in some cases at the behest of a government agency acting under direction from a Minister.

 2. The Bill tasks ACMA with ensuring that, one way or another, platform providers will be obliged to “prevent or respond to” content that is alleged to be mis- or

10 See https://www.alexantic.com.au/home_affairs_freedom_of_information_request

11 See section 61 of the Constitution.

12 See bottom of page 49 of the transcript.


-----

### disinformation. To comply, platform providers will need to ensure that their terms of service give them the legal right to remove content without consulting the person posting the content.

 3. As a result, the regime established under the Bill greatly increases the scope for government agencies, acting without explicit statutory authority, to influence providers to remove content: just as Home Affairs colluded with platform providers to remove content that in the Department of Health’s view, “offended, or potentially offended, the disinformation and misinformation standards” (transcript page 50) of the providers themselves.

 These points are further confirmed by the Guidance Note.

 Scenario 2 on page 16 describes a situation where ACMA would use its powers under the Bill at the instigation of federal government officials wishing to suppress debate about an issue such as the risks of 5G telecommunications infrastructure.

 Scenario 3 describes a situation where ACMA chooses to put pressure on a platform provider because of information provided to ACMA by other providers about content relating to an issue such as the war in Ukraine.

 The scenarios are predicated on the impugned content being “demonstrably false”, “refuted by the Chief Scientist” and “objectively false”. This betrays the censorship mindset that animates the Bill: first, assuming the right of the state to determine what is “demonstrably” or “objectively” false in contexts where there is genuine debate; and second, seeking to suppress speech rather than counter it with evidence and argument.

 Scenario 2 is particularly instructive in this regard. It asserts a connection between “false and misleading claims” and an increase in vandalism of infrastructure. Such a connection might be demonstrable if the online content specifically advocated vandalism. In that case, criminal sanctions should be pursued against the persons posting the content. In the absence of evidence of incitement, ACMA would be seeking to suppress free speech on the basis of mere supposition.


-----

# The Bill makes no meaningful attempt to minimise its impact on free speech and lacks safeguards

## Statements of regulatory policy

### Item 7 of Schedule 2 to the Bill inserts a new subsection 4(3AC) into the Principal Act. It states:

 The Parliament also intends that digital platform services be regulated, in order to prevent and respond to misinformation and disinformation on the services, in a manner that:

 (a) has regard to freedom of expression; and

 …

 (c) protects the community and safeguards end-users against harm
 caused, or contributed to, by misinformation and disinformation on digital platform services; and

 …

 Paragraph (a) is no more than window dressing. The phrase “having regard to freedom of expression” is quite weak, and for the reasons set out earlier in this submission, paragraphs (a) and (c) are fundamentally incompatible.

 As appears from the analysis below, the Bill would enact little or nothing to support the pious hope embodied in paragraph (a).

 Note, in particular, that clause 32 contains a more specific statement of what Parliament intends for the censorship regime in Schedule 9. It makes no mention of freedom of speech.

## Application of the definitions of mis- and disinformation by platform providers

### Clause 7 defines misinformation and disinformation. The 2 most important elements of the definition of misinformation are:

 • First, that the disseminated content “contains information that is false, misleading
 or deceptive”; and

 • Secondly, “that the provision of the content on the digital service is reasonably
 likely to cause or contribute to serious harm”.


-----

### “False, misleading or deceptive”

 The expressions “false or misleading”, “misleading or deceptive” and “false, misleading or deceptive” are found in a large number of provisions in existing Commonwealth Acts. These provisions mainly create offences, although quite a few are civil penalty provisions and a number create private rights of action, for example under consumer protection provisions.

 The common feature of all the provisions just described is that the element must be established in judicial proceedings before any legal consequence, such as a sanction, can apply. Few, if any, current provisions in Commonwealth Acts involve a government official, much less a private sector entity such as a digital platform provider, applying any of these tests in a way that changes a person’s rights or obligations.

 The position under the Bill is fundamentally different. Whether acting under a misinformation code or a misinformation standard, it will be the digital platform provider who refuses to accept a customer’s content, or takes it down, on the basis of the provider’s view that it is “false, misleading or deceptive”. Providers will of course alter their terms of service to allow them to do so without being exposed to legal action by the customer, even if the content is not in fact false, misleading or deceptive.

 “Reasonably likely to cause or contribute to serious harm”

 Some of the same observations can be made about the second main element of the definition: “the provision of the content on the digital service is reasonably likely to cause or contribute to serious harm”. This test does not have much legislative precedent behind it. In practice it too will be applied by platform provider staff members and not by a court. In many cases, the staff applying the test will have nothing on which to base a conclusion except the content itself and their own suppositions and prejudices.

 It is worth noting here that paragraph 7(3)(g) adds significant confusion to how the serious harm test is to be applied. It reads:

 (g) whether the information has been attributed to a source and, if so, the
 authority of the source and whether the attribution is correct;

 Consider a case where the attribution of information is correct and the source is authoritative. On what basis will the information then be determined to be “false, misleading or deceptive”? The obvious inference is that if information contradicts an official narrative, it will be presumed to be false, misleading or deceptive even if it is


-----

### accurately sourced to a reliable authority. That would be a telling indication of the mindset of the framers of the Bill.

## Censorship decisions by platform providers will in most cases not be subject to effective independent review

### No requirement for misinformation code or standard to provide right of review

 The Bill’s requirements for misinformation codes and misinformation standards do not include providing a right of review or appeal to a customer who disputes a censorship decision of the digital platform provider.

 The Bill does not give such a customer the right to apply to the Administrative Appeals Tribunal for review of such a decision. For decisions made under a misinformation code, it also seems unlikely that giving the customer that right would be consistent with subsection 25(1) of the Administrative Appeals Tribunal Act 1975 (AAT Act).[13]

 Judicial review under the Administrative Decisions (Judicial Review) Act 1977

 It is unclear whether a customer could dispute a decision by making an application under the Administrative Decisions (Judicial Review) Act 1977 (ADJR Act). The better view seems to be that they could not if the decision is made under a misinformation code, but that they could if it is made under a misinformation standard.

 An application under section 5 of the Act for review of a decision to which the Act applies can be made by a “person who is aggrieved” by the decision. It seems fairly clear that a person whose content has been censored by a platform provider (by refusal to post it or by removing it) is “a person whose interests are adversely affected by” the censorship decision. So the person would be a “person who is aggrieved” by the decision within the meaning given by subparagraph 3(4)(a)(i) of the Act. This is the case whether the decision is made under a code or a standard.

 However, the decision must be one to which the Act applies. For that to be the case, it must be “of an administrative character” and made “under” certain kinds of “enactment”.

13 This is because the definition of enactment in subsection 3(1) of the AAT Act is similar to the
corresponding definition in the ADJR Act, which is discussed below.


-----

### In general, digital platform providers will not be government agencies. If that means their censorship decisions cannot be “of an administrative character”, then the decisions will not be reviewable under the Act. However, the ordinary meaning of “administrative” may be broad enough to cover censorship decisions made by a non- government entity.

 ADJR review of censorship decisions under misinformation code

 However, the requirement for a decision to be made under an enactment is where the position likely diverges according to whether the censorship decision is made under a misinformation code or a misinformation standard.

 A misinformation code is “developed” by a body or association that “represents a particular section of the digital platform industry” and is then registered by 14
 ACMA . It is arguable that a registered code is made “under” the Principal Act, but 15
 the argument seems fairly weak.

 The Act contemplates the making of the code in the manner just mentioned. Indeed clause 33 goes so far as to say “The Parliament intends that one or more bodies or associations that the ACMA is satisfied represent sections of the digital platform industry should develop” misinformation codes. And this is backed up by ACMA’s powers to make a misinformation standard if a suitable misinformation code is not forthcoming.

 However, a code is not made “under” the Act in the more usual sense of being authorised or legally required by the Act. A code is thus unlikely to be an “enactment” covered by (c) of the definition of enactment in subsection 3(1) of the ADJR Act.

 There is also an argument that censorship decisions made under a misinformation code are (indirectly) made under the Principal Act itself. The considerations just outline make this a weak argument also.

 ADJR review of censorship decisions under misinformation standard

 It is much clearer that a misinformation standard is an an “enactment” covered by paragraph (c) of the definition of enactment in subsection 3(1) of the ADJR Act. Standards are determined by legislative instruments governed by the Legislation Act 2003.

14 Subclause 37(1)

15 Subclause 37(3)


-----

### It should be noted that paragraph (a) of the definition of decision to which this Act applies in subsection 3(1) of the ADJR Act does require that the decision be made by a Commonwealth agency. The fact that most digital platform providers are non-16
 government entities is thus no impediment to judicial review of censorship decisions made under misinformation standards (as opposed to misinformation codes).

 It is anomalous that the availability of judicial review depends on whether the censorship decision is made under a misinformation code or under a misinformation standard. This seems arbitrary and is further evidence of the Bill’s disregard for the free speech rights of ordinary citizens.

 Judicial review of limited utility even if available

 Even if a censorship decision is made under a misinformation standard and thus eligible to be challenged under the ADJR Act, the grounds of challenge are restricted, since review under that Act is about the lawfulness of the decision, not its substantive merits. An application would usually not provide redress where a consumer wishes to challenge a decision on the basis of matters of fact, for example, by showing that their content is factually correct.

 In addition, even though applications under the ADJR Act can be made to the Federal Circuit and Family Court of Australia (Division 2), as well as to the Federal Court, the cost of making an application would be beyond ordinary citizens whose content has been censored.

 Judicial review by way of prerogative writ

 I have not researched the relevant case law, but from first principles it seems unlikely that a censorship decision by a digital platform provider that is a non-government entity could be challenged by seeking a prerogative writ.

 Conclusion

 This analysis demonstrates that the Bill subjects the customer’s free speech rights to incorrect and even arbitrary censorship decisions made by staff of the platform provider.

## Role of ACMA as de facto censor of last resort

### Although the Bill is designed so that the day to day censorship of online content will be done by the digital platform provider acting in compliance with either a

16 Contrast paragraph (b) of the definition in this respect.


-----

### misinformation code or a misinformation standard, ACMA has extensive powers under the Bill to ensure that the providers perform their role as censors. By their nature, these powers mean that ACMA will also perform the function of censor, for 2 reasons.

 ACMA will need to monitor online content

 First, ACMA must review specific material posted on platforms in order to form a view about the following matters that enliven its powers:
 • whether a digital platform provider has failed to comply with an applicable
 misinformation code (subclause 43(4): power to issue a formal warning) or applicable misinformation standard (subclause 53(4): power to issue a formal warning);

 • whether a digital platform provider has contravened, or is contravening, a
 misinformation code (clause 44: power to give remedial direction) or misinformation standard (clause 54: power to give remedial direction);

 • whether a code is operating to provide adequate protection for the community from
 misinformation or disinformation (clause 48: power to determine misinformation standard because of “total failure“ of misinformation code);

 • whether a code is operating to provide adequate protection for the community from
 misinformation or disinformation in relation to a particular matter (clause 49: power to determine misinformation standard because of “partial failure“ of misinformation code);

 • whether there are exceptional and urgent circumstances justifying the determination
 of a misinformation standard (clause 50).

 ACMA’s review of specific material is attended by all the same issues that affect decision-making by digital platform providers, as discussed above.

 The Guidance Note confirms that the Bill will operate as just described. 17

 ACMA power to censor online content by remedial direction

 Secondly, the exercise of some of ACMA’s powers will be directed to removing specific content from a platform. The result of the analysis that follows is that the Bill, on its true construction, gives ACMA the power to censor content on online platforms.

 Clause 44 empowers ACMA to give remedial directions based on a past or present contravention of a misinformation code. Clause 54 makes corresponding provision in relation to a misinformation standard.

17 See Scenarios 2 and 3 on page 16.


-----

### The Guidance Note asserts that it is not “intended that powers will be used to 18
 remove individual pieces of content on a platform”. Clauses 44 and 54 may have been drafted with that intention, or to create that impression.

 However, the drafting of clause 44 is undeniably broad enough to authorise a direction to remove specific content that ACMA considers to be mis- or disinformation (whether or not it actually is).

 Clause 44 applies if “ACMA is satisfied that the provider has contravened, or is contravening” a misinformation code. The purpose of a code is to require providers to “implement measures to prevent or respond to misinformation and disinformation on digital platform services” (emphasis added). It is clear from the Guidance Note that 19
 the Bill uses “respond to” in a broad sense, “including takedowns, content demotion” .20

 If ACMA is satisfied that particular content is mis- or disinformation as defined by clause 7, and that the only effective way to “respond” to the content is to take it down or limit its reach via “content demotion”, it follows that ACMA must be satisfied that the provider’s ongoing failure to take that action is a continuing contravention of the code by the platform provider.

 A remedial direction under clause 44 can require “the provider to take specified action directed towards ensuring that the provider does not contravene the code”. In the case just described, clause 44 authorises a direction to remedy the continuing contravention by taking the content down or limiting its reach.

 A similar analysis applies to the drafting of clause 54. The continuing presence of offending content on a platform will, in cases like the one just discussed, represent a continuing contravention by the platform provider of a misinformation standard, since the determination of the standard was predicated on ACMA being satisfied that the standard was necessary “to provide adequate protection for the community from misinformation or disinformation” .21

18 Page 7 of the Guidance Note. This claim has been repeated in public statements by the Minister.

19 Clause 32.

20 Page 16, Scenario 2, final paragraph. There are a variety of ways to “respond to” mis- and disinformation.
Most compatible with freedom of speech is an online response that sets out information and argument to
rebut the false information. However,

21 Paragraph 46(1)(c), 47(1)(e), 48(1)(a), 49(1)(c) or 50(1)(a).


-----

### ACMA power to censor online content indirectly by means of infringement notice

 The provisions requiring a digital platform provider to comply with a misinformation code and a misinformation standard are “designated infringement notice provisions”. This means that contraventions of those provisions attract the operation 22
 of Part 14E of the Principal Act, which deals with infringement notices.

 Under section 205Y of the Principal Act, an infringement notice can be given if ACMA “has reasonable grounds to believe that a person has contravened a designated infringement notice provision”.

 Allowing specific content to be posted on a platform, or failing to “respond to” it, can constitute a contravention of a misinformation code or misinformation standard and hence of a designated infringement notice provision. An infringement notice for such a contravention will specify a penalty of 60 penalty units (currently $18,780) if the 23
 provider is a body corporate and otherwise 10 penalty units (currently $3,130) .24 25

 If the penalty is paid, proceedings cannot be brought against the person for the alleged contravention.[26]

 This infringement notice regime could easily be exploited to enforce censorship of specific content on digital platforms. The “reasonable grounds” threshold is low, and the penalty is substantial enough to incentivise a digital platform provider to avoid it by not posting similar content in future. But the penalty is not high enough to make it worthwhile for the provider to refuse to pay it and defend proceedings for the alleged contravention.

22 Subclauses 43(3) and 53(3).

23 New paragraph 205ZA(1)(ab) inserted by item 23 of Schedule 2 to the Bill.

24 Paragraph 205ZA(1)(b) of the Principal Act.

25 For the current dollar amount of a Commonwealth penalty unit, see https://asic.gov.au/about-asic/asicinvestigations-and-enforcement/fines-and-penalties/

26 Section 205ZC of the Principal Act.


-----

## Lack of remedies for content creators against decisions by ACMA

### AAT review of ACMA decisions does not help content creators

 The Bill includes amendments of the Principal Act that make decisions by ACMA to give remedial directions reviewable by the Administrative Appeals Tribunal (AAT).[27]

 However, only the digital platform provider to whom a remedial direction is given can apply for review. The person whose content is affected by a direction has no standing to apply for review if the platform provider chooses not to.

 It can be expected that in general digital platform providers will find it more convenient to comply with a remedial direction than to challenge it, and that the providers’ terms of service will ensure that compliance with an invalid direction does not amount to a breach of their contracts with users of the platforms.

 It is clear from how the Guidance Note discusses “enforcement” that the interests of 28
 users of platforms, and their right to freedom of expression, are of no interest to the makers of the policy implemented by the Bill. This passage is a good example:

 It [the approach to compliance and enforcement] also recognises the role of co- regulation set out in the legislation it administers and of engaging with the regulated community to promote compliance. 29

 Although users of platforms are directly affected by the censorship regime, they are plainly not regarded as part of the “regulated community”.

 ADJR review of ACMA remedial directions enforcing codes and standards

 It has been shown above that ACMA’s power to give remedial directions is broad enough to include directions to takedown or demote specific content in at least some cases.

 A decision to give such a direction would be subject to review under the ADJR Act on application by the person posting the content, who is a “a person whose interests are adversely affected by” the decision.

 However, as noted earlier, judicial review is of limited utility in this kind of case. A further obstacle is that the person may not even be aware that the direction has been

27 See items 12 to 14 of Schedule 2 to the Bill.

28 Pages 23 and following.

29 Page 23, section 5.1, first paragraph.


-----

### given, and may have no means of finding out, depending on whether and when information about the giving of ACMA remedial directions is made public.[30]

## The provisions to protect the implied freedom of political communication are likely to be ineffective in practice

### The Bill references the implied freedom of political communication at a number of points.

 Subaragraph 37(1)(d)(i) requires ACMA, when deciding whether to register a misinformation code, to consider whether the proposed code burdens the implied freedom. Subparagraph 40(1)(d)(i) has a corresponding requirement for when ACMA decides whether to approve a draft variation of a misinformation code.

 Clause 45 requires ACMA, before determining a misinformation standard, to consider whether the standard burdens the implied freedom and if so, whether “the burden would be reasonable and not excessive”.

 The above provisions are essentially window-dressing, since ACMA is an administrative agency and cannot make binding determinations on legal questions, let alone constitutional matters. The provisions do nothing to help ACMA if a court holds that a provision in a code or standard does infringe the doctrine.

 However, clause 60 provides that Schedule 9, and instruments made or registered under it, have no effect “to the extent (if any) that their operation would infringe any constitutional doctrine of implied freedom of political communication”. This clause ensures that Schedule 9 once enacted will withstand constitutional challenge on the basis of the doctrine.

 Nonetheless, for a range of reasons, neither the doctrine nor clause 60 will have much effect in practice as a safeguard of freedom of speech.

 For one thing, although the doctrine has been much litigated since first promulgated by the High Court in the early 1990s, very few clearcut propositions have emerged from the case law that can be used to guide decision-making in practice about what it does or doesn’t protect.

 Two crucial points are clear, however, from this passage of the joint judgment of 3 Justices in Clubb v Edwards; Preston v Avery [2019] HCA 11 (10 April 2019):

 It is well settled that the implied freedom is a limitation upon the power of government to regulate communication relating to matters of government and

30 See discussion below about the Bill’s lack of provision for transparency and accountability.


-----

### politics. It does not confer a right to communicate a particular message in a particular way.[31]

 First, the doctrine is not co-extensive with an explicit constitutional guarantee of the right of free speech, such as exists in the First Amendment to the United States Constitution. As a result, much of the online content that the Bill seeks to have policed for mis- or disinformation will not be protected by the High Court doctrine.

 Second, because the limitation is on the power of government to regulate communication on certain matters, it does not directly affect actions of private entities such as digital platform providers. This means that, in applying misinformation codes or standards, providers will have no incentive to concern themselves with whether removal of particular content would, if mandated by law, infringe the implied freedom. This will be so despite clause 60, and despite anything included in a code or standard because of clause 37, 40 or 45.

 In principle, a misinformation code or standard could expressly prohibit rejection or removal of online content that is protected by the doctrine (as opposed to providing that removal is not required). However, there is nothing in the Bill that requires this, or indicates that it would happen in practice.

 Even if there were such a prohibition, the lack of appeal or review rights would make it of little avail to someone whose content was wrongly refused or taken down.

## What the other exemptions reveal about the intent of the Bill

### The Guidance Note states:

 The Bill excludes certain content from the definition of misinformation to strike a balance between the public interest in combatting misinformation, with the right to freedom of expression.[32]

 An analysis of the list in the definition of excluded content for misinformation purposes in clause 2 indicates rather that most of the exclusions are for sources that can be relied on to keep their content within, or fairly close to, the boundaries of the government-approved narrative on any topic.

 The Bill is clearly targeted at ordinary citizens posting content online, including dissident journalists and scholars.

31 per KIEFEL CJ, BELL AND KEANE JJ.

32 Page 12, Section 2.1.4


-----

### Printed material is not subject to similar censorship outside of these curated categories. Online material should not be subject to any greater restriction than print media.

## Duplication of consumer law and defamation remedies without due process safeguards

### As alluded to earlier, the first main element of the definition of misinformation is very similar to section 18 of the Australian Consumer Law, which reads:

 A person must not, in trade or commerce, engage in conduct that is misleading or deceptive or is likely to mislead or deceive.

 A person who posts information that is false, misleading or deceptive will contravene section 18 if the person does so “in trade or commerce”. To that extent the Bill duplicates existing law, but without the judicial due process requirements that apply to seeking remedies under the Australian Consumer Law.

 The same can be said of the law of defamation, under which a defendant has the opportunity to prove to a court the truth of an allegedly defamatory statement, or facts that otherwise establish a defence. A statement is not subject to suppression under defamation law on the mere say so of a private party or government official.

 These due process safeguards form part of the justification for existing legal remedies that impinge on freedom of speech. They are completely absent from the draft Bill.

# Lack of transparency and accountability of the censors

### The Bill appears to contain no provision requiring public reporting by digital platform providers about the nature and extent of censorship activity under misinformation codes and misinformation standards.

 Part 2 of Schedule 9 includes extensive provisions about ACMA collecting information from digital platform providers about their censorship systems and activities. Clause 14 empowers ACMA to make “digital platform rules” requiring providers to keep records of these matters and to prepare reports derived from those records. There are elaborate provisions for enforcing these rules.

 Clause 25 authorises ACMA to publish on its website information about mis- and disinformation on digital platform services, and about providers’ censorship activities. However, there is no requirement to publish any of this material.

 Clause 25 may be broad enough to authorise publication of information about ACMA’s performance and exercise of its functions and powers under Schedule 9,


-----

### such as giving remedial directions, issuing infringement notices, and conducting proceedings under civil penalty provisions. However, again there is no requirement for transparency about these matters.

 Discussion on page 13 of the Guidance Note is very focussed on the fact that:

 These new powers will provide greater transparency and insights on the effectiveness of platform measures to combat misinformation on their services. Information gathered by these powers may be used to inform investigations into potential breaches of misinformation codes and/or standards.

 There is no mention of transparency about the exercise of the powers and their impact on freedom of speech.

 By contrast, subsection 183(2) of the Online Safety Act 2021 includes quite detailed requirements about including in the Online Safety Commissioner’s Annual Report statistics about the respective numbers of notices given under specified provisions of that Act, although there seems to be no requirement for transparency about the content of the notices and their outcome.

 There do not appear to be any specific FOI exemptions relating to ACMA and its role under Schedule 9. However, the general exemptions relating to commercial confidentiality would apply. Also, having to apply under FOI laws is a poor substitute for having this kind of information published in the interests of transparency. Finally, FOI laws do not apply to the vast majority of digital platform providers that are non- government entities.

# Compliance costs create incentive to apply broad brush censorship practices

### The powers conferred on ACMA by Part 2 of Schedule 9 will create red tape and compliance costs for digital platform providers. The providers will want to minimise these. It is much easier to censor content that contradicts the official narrative and that the government claims is “misinformation” than to go to the trouble and expense of verifying the content independently.

 Another way to cut costs is to use software to screen content on a platform. This is a crude technique that produces arbitrary results and suppresses content without any substantive evidence that it is false or misleading.

 Platform providers should be free to respond to their users’ concerns by facilitating the posting of content to contradict disputed material posted on the platform. The Community Notes feature on X (formerly Twitter) is a good example of how this can


-----

### be done. This is cheaper, more effective, and more compatible with freedom of speech, than the measures imposed by the Bill.

 Thomas Reid  Wednesday, 16 August 2023


-----

