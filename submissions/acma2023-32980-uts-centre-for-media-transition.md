#### CENTRE FOR MEDIA TRANSITION

# Exposure Draft: Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2023 

## Consultation on new ACMA powers, June 2023

#### Submission to Department of Infrastructure, Transport, Regional Development, Communications and the Arts

##### Date: 19 August 2023


# Exposure Draft: Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2023 

## Consultation on new ACMA powers, June 2023

#### Submission to Department of Infrastructure, Transport, Regional Development, Communications and the Arts


-----

### About the Centre for Media Transition 

The Centre (CMT) was established in 2017 as an applied research unit based at the
University of Technology Sydney (UTS). It is an interdisciplinary initiative of the Faculty of
Arts and Social Sciences and the Faculty of Law, sitting at the intersection of media,
journalism, technology, ethics, regulation and business.

Working with industry, academia, government and others, the CMT aims to understand
media transition and digital disruption, with a view to recommending legal reform and other
measures that promote the public interest. In addition, the CMT aims to assist news media
to adapt for a digital environment, including by identifying potentially sustainable business
models, develop suitable ethical and regulatory frameworks for a fast-changing digital
ecosystem, foster quality journalism, and develop a diverse media environment that
embraces local/regional, international and transnational issues and debate.

This submission was prepared by:

  - Professor Monica Attard, Co-Director, Centre for Media Transition

  - Dr Michael Davis, Research Fellow

  - Professor Derek Wilding, Co-Director, Centre for Media Transition

**CONTACT**

Centre for Media Transition
Faculty of Law, University of Technology Sydney
Building 2, Level 15
UTS City Campus, Broadway
PO Box 123, Broadway NSW 2007

cmt@uts.edu.au
+61 2 9514 9669

[cmt.uts.edu.au](mailto:CMT@uts.edu.au)


-----

### Executive Summary

To provide further safeguards on freedom of expression, Part 1 of this submission focuses
on revisions to the draft Bill that could further distance the Australian Communications and
Media Authority from direct decision on content. These include:

- Removing from the record-keeping rules the power for ACMA to request that platforms
keep records on ‘the prevalence of content containing false, misleading or deceptive
information’, thereby limiting the records to mis- and disinformation as defined in the
draft Bill.

- Limiting ACMA’s information gathering powers to ‘measures implemented to prevent or
respond’ to mis- and disinformation (including the effectiveness of the measures),
thereby removing any possibility that ACMA could indirectly influence decisions on
which forms of content comprise mis- and disinformation.

- Adding to the statement of regulatory policy a further protection by indicating that
Parliament expects platforms themselves to take freedom of expression into account
when making decisions in relation to online content.

- Inserting a provision that restricts ACMA, in deciding whether to use its enforcement
powers, to decisions that comprise an assessment of the efforts of platforms to
implement effective measures (ie, not whether those practices are effective/applied in
particular cases).

- Restricting ACMA, in making a standard, to obligations concerning measures
implemented. A further step could be to make the decision on whether or not the
standard making power is enlivened to be subject to a disallowable instrument,
meaning Parliament could be engaged in this decision on the advice of the expert
regulator, the ACMA.

Comments on the development of a more comprehensive framework for oversight of
platform moderation practices are presented in Part 2. These include:

- To limit ACMA’s powers and protect free speech, the Bill also limits industry
accountability by placing a range of current platform responses outside the regulatory
scope.

- It is important that free speech is not threatened by zealous or capricious platform
moderation. But this is achieved not by excluding certain content but by ensuring that
platform actions are publicly accountable. Other means can be used to limit ACMA
powers.

- We cannot hope to address misinformation by seeing it solely as a content problem. A
broader approach to platform accountability that includes a responsibility to protect
information integrity and improve the online information environment would be more
effective. This includes being accountable for the effects of system design and platform
policy.

- The boundary between what counts as misinformation and what does not is subjective,
vague and fluid. It is critical for freedom of speech and a functioning public sphere that
this boundary remains open to contestation.

- Promoting the development of principled frameworks for platform decision-making,
preferably through collaborative or participatory governance processes, would
distribute accountability and oversight beyond government and encourage industry to
work creatively to seize responsibility for achieving outcomes.


-----

### Introduction

Thank you for the opportunity to contribute to this consultation. Although we have views on
several aspects of the draft Bill, in this submission we have chosen to concentrate on one
aspect: the ways in which the draft Bill could be adapted to provide additional safeguards
for freedom of expression.

We have taken this approach because we think it is important to continue to improve
oversight and accountability of digital platforms in respect of the actions they take on misand disinformation. The existing Australian Code of Practice on Disinformation and
Misinformation makes a reasonable attempt to tackle this form of online content as it affects
Australians. There are, however, some shortcomings with this self-regulatory initiative. A
recent report by the Australian Communications and Media Authority (ACMA) noted some
improvements to the Code that were made in 2022, but also notes some ongoing problems
associated with the quality of reporting and with complaint and code administration
arrangements, as well as an urgent need to improve the overall level of transparency of
actions taken to address mis- and disinformation.[1]

This is not to say that platforms have abrogated their responsibility for addressing the
issue, but there is good reason to build on the efforts that have been made by industry so
far. For that reason, we support the overall objective and approach adopted in the draft Bill.
These allow, firstly, for an enhancement of the existing industry-based approach with new
powers for the ACMA to require platforms to keep records and provide it with information;
secondly, they allow for the introduction of registered codes of practice and ACMA
standards if self-regulatory codes do not provide adequate community safeguards. This is a
well-established regulatory model that exists in other parts of the communications sector.

While some of the commentary around the impact of the draft Bill on freedom of speech is
overblown, we appreciate there are some concerns over the anticipated role of a
government regulator – in this case, the ACMA – assuming a more interventionist role in
relation to this form of online content. We think the design of the draft Bill largely avoids this
problem as it is mostly directed at giving the ACMA a role in assessing ‘measures’ that
platforms have implemented in order to address mis- and disinformation. The Guidance
Note and some sections of the draft Bill make it clear that ACMA is not expected to make
decisions on specific instances of mis- and dis-information. This approach is different from
both the ACMA’s role in relation to broadcasting content investigations and the eSafety
Commissioner’s role in relation to the matters covered by the Online Safety Act 2021.

Nevertheless, we acknowledge that some additional steps could be taken to further
distance the ACMA from decisions on specific instances of online content. Accordingly, in
Part 1 of this submission we document those sections of the draft Bill where we think an
amendment could be made. In order to restrict our comments to suggestions for adapting
the current draft Bill, we have chosen not to develop the option of an independent, nongovernment intermediary that could review the decisions of digital platforms (leaving the
ACMA to address any systemic issues identified by the intermediary). Should the
government itself decide to develop a proposal for a Digital Platform Ombud Scheme (as
suggested by the ACCC, currently under review by Treasury), we think this would be worth
considering.[2]

In Part Two, we provide some more general comments on a broader framework that might
be adopted at a later stage to place mis- and disinformation within a more comprehensive
approach to regulating content moderation by digital platforms.

1 Australian Communications and Media Authority, Digital Platforms’ Efforts under the Australian Code of Practice on
_[Disinformation and Misinformation: Second Report to Government, July 2023. https://www.acma.gov.au/report-digital-platforms-](https://www.acma.gov.au/report-digital-platforms-efforts-under-australian-code-practice-disinformation-and-misinformation)_
[efforts-under-australian-code-practice-disinformation-and-misinformation.](https://www.acma.gov.au/report-digital-platforms-efforts-under-australian-code-practice-disinformation-and-misinformation)
[2 See Digital Platforms – Consultation on Regulatory Reform | Treasury.gov.au.](https://treasury.gov.au/consultation/c2022-341745)


-----

### Part 1: Proposals for further distancing the ACMA from decisions on online content

Our comments on the draft Bill and how it could be adapted to further distance the
government regulator from direct decisions on content are set out in the third column of the
table below.

|Current provision|What the provision does|Comments and possible revisions|
|---|---|---|
|14 ACMA may make digital platform rules in relation to records|14(1) allows for Digital Platform Rules (‘DP Rules’) that require record keeping on misinformation and disinformation on the service (‘m&d’) and on ‘the prevalence of content containing false, misleading or deceptive information’ (‘fmd’) as well DP Rules on ‘measures implemented … to prevent or respond’ ‘(measures implemented’). 14(2) requires the ACMA, before making a DP Rule, to consider the privacy of end-users and whether the rule is required for the performance of its powers. 14(3) says ‘Digital platform rules made for the purposes of this clause must not require digital platform providers to make or retain records of the content of private messages.’ 14(5) to (10) allow ACMA to require the DP to provide reports on the information in the records.|Here, it is the DP, not ACMA, that makes the judgement on what constitutes m&d and fmd, within the framework provided by the definitions. However, the scope of fmd itself is very broad as it covers any content that could be false, misleading or deceptive (whether harmful or not) and it is not clear how a platform could or should assess the ‘prevalence’ of this. We suggest (e) could be removed from the Bill, with (c) and (d) retained. We also note that the expression ‘on the service’ might need to be aligned with the expression ‘provided on the service’ used in cl 9 in order to capture the ways in which some platforms operate (eg, search engines).|
|18 ACMA may obtain information and documents from digital platform providers 19 ACMA may obtain information and documents from other persons|These sections are similar to 14, allowing ACMA to require the DP provider to give it information/appear before it etc.|A new provision could limit ACMA’s power in respect of obtaining information on m&d content so that it can only be about the operation of ‘measures implemented to prevent or respond’. In this situation, the DP will itself have made a decision on the material that is m&d in accordance with the definitions in a code.|
|25 Publication on website|ACMA is allowed to publish information in its website about m&d, fmd and measures implemented.|The section could limit ACMA, when it publishes information on m&d, to only include (in those categories) content that the DP itself has classified as m&d.|
|32 Statement of regulatory policy|‘The Parliament intends that one or more bodies or associations that the ACMA is satisfied represent sections of the digital platform industry should develop one or more codes (misinformation codes) that require participants in those sections of the digital platform industry to implement measures to prevent or respond to misinformation and disinformation on digital platform services.’|This section limits the scope of the codes to the obligation to ‘implement measures’ so it already provides an appropriate framework for restricting ACMA’s action in relation to specific content. In our view it could provide a further protection by indicating that Parliament expects platforms themselves to take freedom of expression into account when making decisions in relation to online content.|


-----

|Col1|Col2|This could help to foster an environment where platforms take a proportional approach to content moderation.|
|---|---|---|
|33 Examples of matters that may be dealt with by misinformation codes and misinformation standards|33(3) gives 10 examples of matters that may be dealt with. These include: ‘(a) preventing or responding to misinformation or disinformation on digital platform services;’ ‘(b) using technology to prevent or respond to misinformation or disinformation on digital platform services;’|This section is a threshold power that, if expressed too broadly, could lead to ACMA (later in time) making decisions on specific content. As expressed, it does not appear to enable that kind of action but for reassurance, the enforcement power in cl 43/44 and ACMA’s own standards making powers could include a provision that prevents ACMA from taking action based on individual content assessment. By changing those later sections rather than the examples of matters to be included in a code, this approach would avoid unnecessary limitations on the actions that DPs themselves will take. The following could also be added: - Governance arrangements, such as requiring independent assessment of platform reports and the effectiveness of their measures (similar to the TCP code) - Developing publicly responsive systems for assessing content- moderation decisions, including providing an opportunity for users to seek review by platforms of complaints and content moderation decisions.|
|37 Registration of misinformation codes 40 Variation of misinformation codes|The key part of the registration criteria is found in 37(1)(e) (and the similar provision in s 40): ‘(e) the ACMA is satisfied that: (i) the code (or part of the code) requires participants in that section of the digital platform industry to implement measures to prevent or respond to misinformation or disinformation on the services; and (ii) the code (or part of the code) enables assessment of compliance with the measures; and (iii) the code (or part of the code) provides adequate protection for the community from misinformation or disinformation on the services; and’ In addition to this and the usual consultation requirements, s 37(1)(d) requires ACMA to consider any|This test is an expanded form of the tests used in Part 9 of the Broadcasting Services Act and Part 6 of the Telecommunications Act. In order to encourage the development of an effective code, this provision itself should not be limited; instead, a provision limiting ACMA’s powers when assessing compliance could be inserted into Subdivision C (‘Compliance with registered information codes’) of Division 4 (see below). There is some question over the scope of 37(1) since the later provision in cl 60 invalidates anything that would infringe ‘any constitutional doctrine of implied freedom of political communication’, whereas cl 37 only refers to ‘freedom of political communication’. There is perhaps an argument for making cl 37 consistent with the later reference to the freedom implied in the Constitution; however, there may also be an argument for not limiting cl 37(1) to the (notoriously narrow) implied|


-----

|Col1|burden on freedom of political communication: 37 Registration of codes (1) This clause applies if: […] (d) the ACMA considers: (i) whether the code burdens freedom of political communication; and (ii) if so, whether the burden is reasonable and not excessive, having regard to any circumstances the ACMA considers relevant; and|freedom in the Constitution, in which case 37(1)(d)(i) could be broadened to include whether the code burdens freedom of expression, including freedom of political expression. Cl 37(1)(d)(ii) would be retained in its existing form. Using the term ‘freedom of expression’ would align this part with new clause 4(3AC) of the BSA (see cl 7 of Schedule 2 to the Exposure Draft).|
|---|---|---|
|Subdivision C (‘Compliance with registered information codes’) 43 Compliance with registered misinformation code 44 Remedial directions— contravention of misinformation code|These sections require DP providers to comply with a code and give ACMA powers to issue infringement notices, formal warnings and remedial directions and to seek a civil penalty order.|The operative decisions are ACMA being satisfied that a DP provider has contravened the code ((43(4), 44(1)(c)). A new section inserted at the start or end of this subdivision could restrict ACMA to decisions that comprise an assessment of the efforts of platforms to implement effective measures (ie, not whether those practices are effective/applied in particular cases).|
|Div 5 Misinformation standards 46 ACMA may determine standards—request for a code is not complied with 47 ACMA may determine standards—no industry body or association formed 48 ACMA may determine standards—total failure of misinformation code 49 ACMA may determine standards—partial failure of misinformation code 50 ACMA may determine standards—emerging circumstances 51 Variation of misinformation standards|Each of these sections enables ACMA to make a standard that would replace a code in whole or in part.|A new section applying to the whole division could be inserted, similar to the codes limitation mentioned above, ie it could restrict ACMA, in making a standard, to obligations concerning measures implemented, or it could prevent ACMA from making a standard that gives ACMA the power to make an assessment of whether content is m&d. A further step in providing oversight by Parliament, if this is considered necessary, would be for the reserve standard making power in Division 5 to be subject to commencement of a direction or regulation (which should be a disallowable instrument) where the ACMA has provided a report to the Minister that it is satisfied that the conditions for the operation of any of clause 46, clause 47, clause 48 clause 49 or clause 50 apply. This would mean that the decision on whether or not the standard making power is enlivened would engage the Parliament through a regulation, which is disallowable, acting on the advice of the expert regulator, the ACMA.|
|53 Compliance with misinformation standard 54 Remedial directions— contravention of misinformation standard|These sections are similar to the code compliance provisions (43, 44) mentioned above in that they require DP providers to comply with a standard and give ACMA powers to issue infringement notices, formal warnings and remedial directions and to seek a civil penalty order.|As with cl 43 and 44, a new section inserted at the start or end of this subdivision could restrict ACMA to decisions concerning measures implemented or to prevent ACMA from itself making an assessment of whether content is m&d.|


-----

|64 Digital platform rules|This section allows ACMA to make DP Rules required or permitted under the Act (ie the record keeping rules in s 14) or (64(1)(b)) ‘necessary or convenient to be prescribed for carrying out or giving effect to this Schedule’.|There could be an additional provision that restricts ACMA, in making DP Rules, to obligations concerning measures implemented, or it could prevent ACMA from making a DP Rule that gives ACMA the power to make an assessment of whether content is m&d.|
|---|---|---|
|Schedule 2, clause 2 re s 10(1) of the ACMA Act|The change would expand the list of ACMA’s broadcasting, content and datacasting functions to encompass the functions covered by the Bill|If it was considered necessary, Division 4 of the ACMA Act (Requirements relating to these functions and powers) could be amended to introduce a limitation on ACMA’s powers regarding m&d.|
|Schedule 2, clause 7 re s 4(3) of the BSA|A new aspect of regulatory policy is to be inserted to accommodate the m&d scheme that would comprise new Schedule 9. This includes that DP services be regulated in a manner that ‘has regard to freedom of expression’ (3AC)(a)).|This explicit reference to freedom of expression should remain.|


-----

### Part 2: Additional comments on longer-term reform

In the previous part we set out some recommendations that may help to ensure ACMA’s
powers are further distanced from industry content-moderation decisions and thereby
provide better protection for freedom of expression from potential or perceived government
overreach. In this part we look at the other side of the equation, setting out our thinking on
how regulation may better promote industry accountability. The factsheet states that the
proposed powers are ‘designed to encourage digital platform services to be accountable for
improving and implementing measures to counter the spread of misinformation and
disinformation online,’ and it is this objective that we have in mind here.

On the whole, we consider that a broader and more-comprehensive approach to platform
accountability that includes a responsibility to protect information integrity and to improve
the online information environment would be more effective in addressing misinformation
while still protecting freedom of speech. Some aspects of this broader approach could be
fairly easily incorporated into the bill by expanding the range of example matters to be
covered by the code in clause 33 (see our comments on cl. 33 in the table in Part 1), while
others may require some rethinking either now or in the future on the broader regulatory
approach.

###### 2.1 The bill limits industry accountability by placing a range of current platform responses outside the scope of misinformation and disinformation

A fundamental difficulty in the approach taken by the bill is that the limitations on ACMA
power also effectively limit industry accountability. That is because both ACMA’s powers
and industry accountability are limited by the scope set by the definitions of misinformation
and disinformation and the exclusion of particular types of content.

Industry accountability is maximised when all platform policies, measures and decisionmaking relating to misinformation are transparent, subject to review and responsive to
feedback. But by setting a scope that is narrower than this, the bill either (a) renders a
range of relevant platform responses unaccountable, or (b) delegitimises a range of
platform responses that over the last several years have largely been recognised as
appropriate.

Many of the example measures set out in section 5.9 of the current code of practice (and
encompassed by subclauses 33(a) and (b) of the bill) commonly operate well below the
threshold of serious harm that sets the scope of what is to be counted as misinformation or
disinformation under the proposed legislation. For example, platform recommendation
systems routinely demote content that is close to but does not violate their terms of service
(eg, what Facebook calls ‘borderline content’); platforms remove content propagated via
inauthentic behaviour that is not likely to lead to serious harm; and platforms often apply
fact-checking labels to content that is false or misleading but not likely to lead to serious
harm. Under the bill, these actions are outside the scope of what counts as misinformation
and disinformation and are therefore not subject to the accountability provided by the
proposed regulatory powers.

This problem is brought into relief if we think about platform decisions to remove
misinformation – an action that should be on the upper end of possible responses in a riskbased framework that promotes proportionality in decision-making. If we accept that
platforms should only remove misinformation that is reasonably likely to cause serious
harm (and even this may be too generous in scope), then the area in which scrutiny of
platform decision-making is most critical is the boundary zone between what counts as
misinformation and what does not.[3]

3 In this part of the submission, unless otherwise indicated, we will use the term ‘misinformation’ collectively to refer to both
misinformation and disinformation.


-----

But the boundary zone between what counts as misinformation and what does not is
subjective, vague and fluid. What is considered misleading or harmful by some, may not be
by others. It is also context-dependent. What may be a reasonable response in the midst of
a pandemic may not be a reasonable response in normal times. And it is critical for
freedom of speech and a functioning public sphere that this boundary remains open to
contestation.

This problem is exacerbated by the at-least-partly automated approach required to perform
content moderation at scale on digital platforms. Whatever threshold they operate at,
automated content-moderation systems will naturally flag or remove some content that is
below the threshold. Content moderation at scale is thus always a trade-off between
capturing harmful material and capturing non-harmful material — the more of the former
you try to get, the more of the latter you will also get.[4] That is, they necessarily have some
rate of error that can be lowered only by increasing error elsewhere. This is true even for
content that is relatively well defined, such as child sexual exploitation material (CSAM).
The problem is worse for poorly defined and poorly identifiable material such as
misinformation. The need to protect freedom of expression worsens the problem further, as
the trade-off is not merely between harmful and non-harmful content but between harmful
content and content that should be protected under freedom of expression.[5]

Finally, as we explain further below, the dynamic and complex nature of misinformation and
of platform responses to it means that platforms must have room to experiment with
different approaches, including approaches which shift the boundary zone between
material that is addressed and material that isn’t. They should also be accountable for the
approaches they take; which again is to say that the boundary must be contestable. But for
platform approaches to be accountable, they must be within the scope of the regulatory
framework. Here the example of YouTube removing Sky News videos is pertinent.

We suggest that a more effective approach to promoting industry accountability would be to
expand the range of industry actions subject to oversight under the regulatory framework
and use other means to limit ACMA powers, such as those we set out in Part 1. A deeper
reorientation away from content towards systems and processes would also be beneficial.
In addition, promoting or requiring the development of principled frameworks for platform
decision-making (eg, as part of cl 33), preferably through collaborative or participatory
governance processes, would distribute accountability and oversight beyond government
and encourage industry to work creatively to seize responsibility for achieving outcomes
rather than defensively in order to achieve compliance.[6]

Such frameworks would promote principled platform action on content that is excluded
under the proposed legislation, such as professional news, for example by requiring
platforms to refer complainants to existing regulatory schemes.

International policy developments, as well as recent research on platform governance and
online content regulation, also take a broader approach and emphasise the importance of
collaborative and participatory governance structures in addressing a very complex and
multi-sided problem.[7]

###### 2.2 We cannot hope to address misinformation by seeing it solely as a content problem

Content moderation is an important tool in combatting misinformation. But it is plagued with
difficulties that render it unsuitable for a rules-based regulatory approach based on a tightly

4 G Sartor, The Impact of Algorithms for Online Content Filtering or Moderation - Upload Filters, European Parliament,
September 2020.
5 E Douek, ‘Governing Online Speech: From “Posts-As-Trumps” to Proportionality and Probability’, in Columbia Law Review,
vol. 121, 2021, 759–833.
6 J Braithwaite, ‘Accountability and Governance under the New Regulatory State’, in Australian Journal of Public Administration,
vol. 58, 1999, 90–94.
[7 For example, the strengthened EU Code of Practice on Disinformation, the recent UN policy brief Information Integrity on](https://digital-strategy.ec.europa.eu/en/library/2022-strengthened-code-practice-disinformation)
[Digital Platforms and the recently formed International Panel on the Information Environment.](https://www.un.org/sexualviolenceinconflict/wp-content/uploads/2023/06/our-common-agenda-policy-brief-information-integrity-en.pdf)


-----

defined scope. Aside from the problem of scalability discussed above, defining
misinformation to the satisfaction of all parties (industry, government, stakeholders, the
public) is likely impossible. As has recently been noted by Clare Wardle, from whom the
common distinction between misinformation and disinformation based on intention comes,
these terms represent an overly simple, tidy framework that is no longer useful.[8]

First, Wardle and others (including ACMA in its position paper) have long pointed out that it
is very difficult to assess truth and intention, let alone in real time and at scale. Secondly,
the truth-value of much, if not most, problematic misinformation is contentious, and expert
judgements about what is true can differ and change over time. Determining truth is a
public process – and a fallible, ongoing and iterative one. It is important that the space for
contested speech (the ‘public sphere’) is preserved and not threatened by zealous or
capricious platform moderation. But this is achieved not by removing that space from
accountability but instead ensuring that platform actions within that space are publicly
accountable and thus part of the public process of negotiating truth.

###### 2.3 Instead we need to address the systemic issues that allow misinformation to spread and cause social harm

Misinformation is not an isolated phenomenon but related to a range of other problems with
the online information environment such as hate speech, online abuse, the proliferation of
low-quality information and counterfeit or ‘fake’ news, and abhorrent violent content. Some
of these problems, such as the last, are more pressing than others and there is therefore
good reason they have been treated separately. There is also a perfectly reasonable
perceived need to treat misinformation separately due to free-speech concerns. What can
be obscured by this separate treatment, however, is the common, systemic causes that
contribute to several or all of these online problems.[9] There may be value in addressing
these causes within a common framework focused on promoting platform accountability,
particularly for the effects of system design and platform policy on the broader information
environment.

For example, content-recommender systems or algorithms that promote engagement are
acknowledged as one of the principal contributory factors behind online hate speech or
abuse as well as the proliferation of misinformation and low-quality information online.
Grounding the scope of regulation in types of content rather than types of platform activities
or systems, as the current bill does, is not only difficult to operationalise, but in effect
neglects to address these contributory factors. As a recent paper from Harvard’s Berkman
Klein Center puts it, ‘Content moderation, while very important, operates at the scale of
harm posed by discrete user-generated posts rather than the platform-wide interaction
effects imposed by designers.’[10] Thus, while the revised (2022) Australian code includes a
provision addressing user transparency and choice in recommender systems, what we are
suggesting here is a reorientation of the regulatory approach away from addressing a
certain class of content (due to the difficulties outlined in 2.1) towards improving the
information environment through system and interface design as well as platform policy.

This was formerly a stronger element in regulatory discussions. For example, ACMA’s
position paper on code development clearly set out these problems of the broader
information environment, following the (former) Government’s response to the ACCC’s
discussion of the effects of digital platforms on the quality of online news and information.[11]
However, the need to address the broader issues with the information environment seems

8 C Wardle, ‘Misunderstanding Misinformation’, in Issues in Science and Technology, vol. 39, 2023,
[https://issues.org/misunderstanding-misinformation-wardle/.](https://issues.org/misunderstanding-misinformation-wardle/)
9 M Montgomery, Disinformation as a Wicked Problem: Why We Need Co-regulatory Frameworks, Brookings Institution, 20
[August 2020, https://www.brookings.edu/research/disinformation-as-a-wicked-problem-why-we-need-co-regulatory-frameworks/.](https://www.brookings.edu/research/disinformation-as-a-wicked-problem-why-we-need-co-regulatory-frameworks/)
10 N Lubin & TK Gilbert, ‘Accountability Infrastructure: How to Implement Limits on Platform Optimization to Protect Population
[Health’, arXiv, 2023, http://arxiv.org/abs/2306.07443.](http://arxiv.org/abs/2306.07443)
11 ACMA, Misinformation and News Quality on Digital Platforms in Australia: A Position Paper to Guide Code Development,
[June 2020. https://www.acma.gov.au/online-misinformation-and-news-quality-australia-position-paper-guide-code-development.](https://www.acma.gov.au/online-misinformation-and-news-quality-australia-position-paper-guide-code-development)


-----

to have fallen victim to the need to limit ACMA’s powers. This, as discussed in 2.1, results
from tying the regulatory scope to a particular class of content.

###### 2.4 Policy should focus on making industry responsible for improving the online information environment and accountable for the full range of actions they take in doing so

The primary target for the regulation of platforms’ efforts to address misinformation is to
make them accountable for actions they largely already take. As content-moderation expert
Evelyn Douek puts it, ‘The history of online speech governance is a history of platforms
exercising essentially unconstrained discretion, creating ad hoc rules in response to
particular crises.’[12] But, as noted earlier, the dynamic and complex nature of misinformation
and of platform responses to it means that platforms must have room to experiment with
different approaches. This experimentation must be transparent and accountable.

Another way of seeing the regulatory target is to encourage platforms to take greater
responsibility for matters over which they have control. Their focus must also be broader –
rather than addressing misinformation purely as a content-moderation problem, with all its
inherent difficulties, platforms should be encouraged to focus efforts on improving the
online information environment more broadly so that it is able to function as a space for
civic discourse. This includes addressing structural elements of the online environment
through algorithm and interface design.

The broader the scope set by the legislation, the more accountable platform action will be.

###### 2.5 Collaborative, participatory governance

One aspect of the problem that would benefit from the room to experiment is in the
regulatory and platform governance arrangements themselves. Because content
moderation is ‘inevitably political’[13] and constrains individual speech, encouraging
participatory governance of platform decision-making, in which the public provides input
into setting platform rules or policies would help promote industry accountability for
misinformation – not merely to the government, but to the public. As we have argued
above, it is not just individual content-moderation decisions which must be contestable –
the boundaries between what is considered to be harmful and harmless content must also
be contestable. As Appelman and others put it, ‘Truth-seeking has to happen within society
as a form of deliberation through public discourse’.[14] Both platform and government action
within the public sphere must therefore be accountable, contestable and responsive to the
public.

12 Douek, ‘Governing Online Speech', p. 819.
13 NP Suzor, Lawless: The Secret Rules That Govern our Digital Lives, Cambridge, Cambridge University Press, 2019, p. 27.
14 N Appelman et al, ‘Truth, Intention and Harm: Conceptual Challenges for Disinformation-targeted Governance’, in Internet
_[Policy Review, 2022, https://policyreview.info/articles/news/truth-intention-and-harm-conceptual-challenges-disinformation-](https://policyreview.info/articles/news/truth-intention-and-harm-conceptual-challenges-disinformation-targeted-governance/1668)_
[targeted-governance/1668.](https://policyreview.info/articles/news/truth-intention-and-harm-conceptual-challenges-disinformation-targeted-governance/1668)


-----

