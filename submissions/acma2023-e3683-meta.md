# Submission on draft Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2023

## August 2023


-----

## Executive summary

Meta welcomes the opportunity to provide a submission on the draft Communications
Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2023.

We invest significantly in technology, people and systems to combat coordinated
inauthentic behaviour that can lead to misinformation and disinformation on our services.

Meta has also made constructive contributions to the regulatory framework in Australia
governing misinformation. We were a founding member to the voluntary industry code on
misinformation and disinformation, which was established by the industry association
DIGI in 2021. Since then, we have opted into every commitment under the code, and have
provided three annual transparency reports[1]. We also supported the review and public
consultation of the code in 2022, and we sit on the administration committee convened
by DIGI to ensure the code works effectively.

The Australian industry code on misinformation and disinformation has been a
world-leading initiative that has brought welcome additional transparency to the work of
digital platforms.

We recognise that policymakers want to ensure that industry codes can be properly
enforced. It is understandable for the Australian Government to want to put a legislative
framework around voluntary industry efforts that can be administered by a regulator like
the Australian Communications and Media Authority (ACMA). An industry association
cannot compel non-signatories to adopt the code, and it can be difficult for an industry
association to enforce penalties for non-compliance. For these reasons, Meta supports
the principle of the draft legislation.

There are many elements of the legislation that we welcome such as it’s systems-based
approach and protection for private content.

Specifically, the legislation only provides the ACMA with powers to investigate and assess
the overall systems companies adopt to combat misinformation and disinformation,
rather than specific content. This avoids the situation where the ACMA is making
decisions about whether a single piece of online content constitutes misinformation,

1 See e.g., Meta response to the Australian disinformation and misinformation industry code,
Reporting period January - December 2022 (May 2023):
[https://digi.org.au/wp-content/uploads/2023/05/Meta_2023-AU-Misinformation-Transparency-r](https://digi.org.au/wp-content/uploads/2023/05/Meta_2023-AU-Misinformation-Transparency-report_v1.pdf)
[eport_v1.pdf](https://digi.org.au/wp-content/uploads/2023/05/Meta_2023-AU-Misinformation-Transparency-report_v1.pdf)


-----

given the challenges in arbitrating the truth of online content and the risks of government
stifling free expression.

We also welcome the protections that the legislation cannot be used to undermine
encryption, or otherwise compel access to private content.

However, we encourage any policymakers preparing misinformation or
disinformation-related legislation to carefully consider the risk of unreasonably limiting
free expression. Reasonable people may disagree on what constitutes misinformation.
Our societal understanding of what is true or false may evolve as more evidence comes to
light. And even the terms ‘misinformation’ and ‘disinformation’ can be misused in
contemporary political debate to discredit legitimate political opinions.

For this reason, we believe any misinformation regulation should be carefully designed,
tightly scoped, and be as precise as possible around the definition of misinformation and
disinformation. There are two aspects of the current draft legislation that we believe
could risk overreach.

Firstly, the proposed legislation applies to misleading, deceptive or false content which
could cause or contribute to serious "harm". These are incredibly broad and ambiguous
concepts, especially in their application to individual pieces of content. It means content,
including views or ideas, could constitute misinformation if they are false and have broad
impacts including “disruption of Australian society”, democracy, the environment, and
economic or financial harm. It is not possible for digital platforms to foresee how
individual content could result in a “disruption of society” or to the economy or
environment. This definition risks being so broad as to grant a regulator discretionary
standard-making power over a very wide scope of content, some of which may be at the
borderlines of genuine and legitimate expression of views or ideas.

In order to reduce the risk of overreach, we recommend adjusting the definition of harm
to ideally be limited to reasonably foreseeable serious imminent, physical harm
consistent with the definition outlined in the Australian Disinformation and
Misinformation Industry Code. This change would help reduce the risk that regulatory
instruments are developed that could cover benign political expression.

While we welcome a graduated and proportionate approach to enforcement, it is
important to set the above concerns against the very large penalties that can apply - the
greater of AUD7,825,000 or up to 5% of annual global turnover. The significance of these


-----

penalties will likely incentivize an over-enforcement and removal of content, with
possible unintended adverse impacts to free expression and open communication.

Secondly, there should be greater Parliamentary oversight before the ACMA is able to
develop and register a mandatory standard. While the standard-making power as drafted
is similar to other regulatory standard-making powers in relation to issues like online
safety and telecommunications, misinformation is a special issue where the power to
influence national political debate is especially potent. While the ACMA is a very
respected and even-handed regulator, the legislation should be drafted in a way that has
greater checks and balances against the risk of a future version of the ACMA misusing
these very significant powers for political ends.

Given misinformation is inextricably linked to national political debate, and to safeguard
the risk of these powers being used for partisan political ends, we recommend some
measure of Parliamentary oversight before a misinformation standard is developed and
registered. This could take a variety of forms, and our submission makes some
suggestions.

We welcome the opportunity to engage in this process and to work constructively with
policymakers as they consider this issue.


-----

## Table of contents

**Executive summary** **2**

**Table of contents** **5**

**Meta’s work to combat misinformation and disinformation** **6**

Policies and enforcement 6

Misinformation 6

Coordinated inauthentic behaviour 9

Fake accounts 11

Tools to inform and programs to promote media literacy 11

Supporting transparency efforts and research 14

**Comments on draft legislation** **17**

General comments on regulation covering misinformation 17

Specific comments on the draft legislation 18


-----

## Meta’s work to combat misinformation and disinformation

Facebook is committed to create a place for expression and give people a voice, and to
the integrity of our platforms. We invest significantly in safety and security, and to
combat coordinated inauthentic behaviour that can lead to misinformation and
disinformation on our services

We do so via a combination of: (1) policies and enforcement to remove and reduce misand disinformation and inauthentic activity on our services; (2) products and programs to
give people greater context about what they are seeing on our services and to promote
greater media literacy; (3) transparency tools and research.

### Policies and enforcement

 Misinformation

Meta takes a global approach to combatting misinformation. We take a significant
number of steps to remove or reduce misinformation.

**Remove**
Meta removes misinformation that violates our Community Standards and can directly
contribute to imminent, physical harm. Misinformation can cut across different types of
abuse areas - for example, a racial slur could be coupled with a false claim about a group
of people which we would remove for violating our hate speech policy - so it is important
to have a comprehensive set of policies that covers a range of abuses.

For example, during the course of the COVID pandemic, we worked with experts around
the world, especially the World Health Organization, to identify COVID-related claims
that could have directly contributed to imminent, physical harm.

In addition, Meta has established the Oversight Board - comprised of 22 experts in human
rights and technology - including the Australian academic Professor Nic Suzor from
Queensland University of Technology. The Oversight Board is an independent, external
body set up to assess appeals on Facebook’s content decisions, and was established to
give policymakers and the community confidence about Facebook’s content governance.

In July 2022, we asked the independent Oversight Board to advise on whether the
extraordinary COVID measures should remain in place.[2] They issued advice in April 2023

2 Oversight Board, ‘Oversight Board publishes policy advisory opinion on the removal of COVID-19 misinformation’,
Oversight Board,


-----

and we responded in June 2023. Consistent with the Oversight Board’s guidance, we now
take a more tailored approach to our rules around COVID misinformation.[3]

There are additional areas where we consider, based on the advice of experts, that
misinformation could directly contribute to imminent harm to people. One is around voter
interference. Meta removes election-related misinformation that may constitute voter
fraud and/or interference. Under our policies, we prohibit a range of claims that
misrepresent electoral processes in a way that could deter electoral participation. Voting
is essential to democracy, which is why we take a stricter policy on misrepresentations
and misinformation that could result in voter fraud or interference. We also have policies
on manipulated media that are created in order to mislead people. We remove videos that
are created using artificial intelligence or machine learning that would likely mislead
someone into thinking that a subject of the video said words they never actually say.[4]

To provide transparency around our work to combat mis- and disinformation, we provide
Australia-specific data on our enforcement efforts under our regular annual reports under
the voluntary industry code on misinformation and disinformation. In the last year, we
took action on over 91,000 pieces of content across Facebook & Instagram in Australia
for violating our Harmful Health Misinformation policies. We also removed over 100
accounts, pages, and groups in Australia for repeatedly violating our Harmful Health
Misinformation policies.[5]

**Reduce**
For content that does not violate our Community Standards but is rated as false or
altered by Meta’s independent third-party fact-checking partners, we significantly reduce
the number of people who see it through a number of measures. We believe that public
debate and democracy are best served by allowing people to debate different ideas, even
if they are controversial or wrong - but we take steps to limit the distribution of
misinformation that has been found to be false by independent, expert fact checkers.

Meta partners with third-party fact-checking organisations, globally and in Australia, to
assess the accuracy of content on our services. We have commercial arrangements with
independent third-party fact-checking organisations for them to review and rate the

[https://oversightboard.com/news/739141534555182-oversight-board-publishes-policy-advisory-opinion-on-the-removal](https://oversightboard.com/news/739141534555182-oversight-board-publishes-policy-advisory-opinion-on-the-removal-of-covid-19-misinformation/)
[-of-covid-19-misinformation/.](https://oversightboard.com/news/739141534555182-oversight-board-publishes-policy-advisory-opinion-on-the-removal-of-covid-19-misinformation/)

3 N Clegg, ‘Meta Asks Oversight Board to Advise on COVID-19 Misinformation Policies’, Meta Newsroom, 26 July 2022 and
[updated 16 June 2023, https://about.fb.com/news/2022/07/oversight-board-advise-covid-19-misinformation-measures/.](https://about.fb.com/news/2022/07/oversight-board-advise-covid-19-misinformation-measures/)

4 M Bickert, ‘Enforcing Against Manipulated Media’ Meta Newsroom, 6 January 2020

[https://about.fb.com/news/2020/01/enforcing-against-manipulated-media/](https://about.fb.com/news/2020/01/enforcing-against-manipulated-media/)

5 J Machin, ‘Meta’s third annual transparency report under Australia’s misinformation and disinformation code’, Meta
Australia Policy Blog, 7 June 2023,
[https://medium.com/meta-australia-policy-blog/metas-third-annual-transparency-report-on-australia-s-disinformation-a](https://medium.com/meta-australia-policy-blog/metas-third-annual-transparency-report-on-australia-s-disinformation-and-misinformation-industry-c4afe7953af2)
[nd-misinformation-industry-c4afe7953af2.](https://medium.com/meta-australia-policy-blog/metas-third-annual-transparency-report-on-australia-s-disinformation-and-misinformation-industry-c4afe7953af2)


-----

accuracy of posts on Facebook and Instagram. In Australia, we partner with Australian
Associated Press, Agence France Presse and RMIT FactLab, all of whom are certified by
the non-partisan International Fact-Checking Network, as part of a network of over 90
fact-checking partners around the world covering more than 70 languages. All
fact-checks by these partners are publicly available on their websites.[6]

Once a third-party fact-checking partner rates a post as ‘false’, we apply a warning label
that indicates it is false and shows a debunking article from the fact checker. It is not
possible to see the content without clicking past the warning label.

We also notify people before they try to share this content or if they shared it in the past.

Once found to be false or altered, Meta reduces the distribution of that content so it
appears lower in Feed, which slows its distribution significantly. And on Instagram, we
remove it from recommendable surfaces like Explore and hashtag pages, and downrank
content in Feed and Stories.

We provide Australia-specific data on our enforcement efforts under these policies as
part of our regular annual reports under the voluntary industry code on misinformation
and disinformation. In the last year, we displayed warnings on over 9 million distinct

[6 Agence France Presse Australia, Fact Check, https://factcheck.afp.com/afp-australia; Australian Associated Presse, AAP](https://factcheck.afp.com/afp-australia)
[Fact Check, https://www.aap.com.au/category/factcheck/](https://www.aap.com.au/category/factcheck/)


-----

pieces of content on Facebook in Australia (including reshares) based on articles written
by our third party fact checking partners.[7]

### Coordinated inauthentic behaviour

In the social media landscape and beyond, foreign interference relies on inauthenticity where users misrepresent themselves, through fake profiles or non-transparent
behaviours - and coordination.

We consider authentic communications as a central part of people’s experience on Meta’s
services. People find value in connecting with their friends and family, and they also find
value in receiving updates from the Pages and organisations that they choose to follow.
For this reason, authenticity has long been a requirement of our Community Standards.

Our policies in this space have been through a number of iterations over recent years, to
reflect our deepening understanding of the phenomenon of inauthentic behaviour.[8] We
have an Inauthentic Behaviour policy, which has a number of components:

_●_ Coordinated Inauthentic Behaviour (CIB). We define this as groups of accounts
and Pages that work together to mislead people about who they are and what
they’re doing. When we find domestic, non-government campaigns in which the
use of fake accounts is central to the operation, we will remove all inauthentic and
authentic accounts, Pages and groups directly involved in this activity.

_●_ Foreign or Government Interference. These are either (1) foreign-led efforts to
manipulate public debate in another country in a way that is inauthentic; or (2)
inauthentic behaviour operations run by a government to target its own citizens. If
we see any of these instances, we will apply the broadest enforcement measures,
including the removal of every on-platform property connected to the operation
itself and the people and organisations behind it.

_●_ Other inauthentic behaviour, including financially-motivated activity like spam or
fake engagement tactics that rely on inauthentic amplification or evading use of
enforcement (separate to use of fake accounts). The full list of tactics that we do

7 J Machin, ‘Meta’s third annual transparency report under Australia’s misinformation and disinformation code’, Meta
Australia Policy Blog, 7 June 2023,
[https://medium.com/meta-australia-policy-blog/metas-third-annual-transparency-report-on-australia-s-disinformation-a](https://medium.com/meta-australia-policy-blog/metas-third-annual-transparency-report-on-australia-s-disinformation-and-misinformation-industry-c4afe7953af2)
[nd-misinformation-industry-c4afe7953af2.](https://medium.com/meta-australia-policy-blog/metas-third-annual-transparency-report-on-australia-s-disinformation-and-misinformation-industry-c4afe7953af2)

8 N Gleicher, ‘How we respond to Inauthentic Behaviour - policy update’, Facebook Newsroom, 21 October
[2019, https://about.fb.com/news/2019/10/inauthentic-behavior-policy-update/](https://about.fb.com/news/2019/10/inauthentic-behavior-policy-update/)


-----

not allow is available as part of our Community Standards.[9] We enforce against
other inauthentic behaviour based on specific protocols that may involve
temporary restrictions, warnings, down-ranking in Facebook News Feed, or
removal.

We have invested significantly in a team that is able to detect the various forms of
Inauthentic Behaviour on our services.

In December 2022, we reported that since 2017, Meta has removed over 200 operations
for violating our policy against CIB.[10] These networks came from 68 countries and
operated in at least 42 languages, with most targeting audiences in their home countries
and only around one-third aimed solely at audiences abroad.

The United States was the most targeted country by global CIB operations we’ve
disrupted over the years, followed by Ukraine and the United Kingdom.

**Global Coordinated Inauthentic Behaviour Disruptions, 2017 – 2022**

9 Facebook, Community Standards - Inauthentic Behaviour,
[https://www.facebook.com/communitystandards/inauthentic_behavior](https://www.facebook.com/communitystandards/inauthentic_behavior)

10 B Nimmo, D Agranovich, ‘Recapping our 2022 coordinated inauthentic behaviour enforcements’, Meta
Newsroom, 15 December 2022,
[https://about.fb.com/news/2022/12/metas-2022-coordinated-inauthentic-behavior-enforcements/ and](https://about.fb.com/news/2022/12/metas-2022-coordinated-inauthentic-behavior-enforcements/)
Meta, ‘December 2021 Coordinated Inauthentic Behaviour Report’, December 2021,
[https://about.fb.com/wp-content/uploads/2022/01/December-2021-Coordinated-Inauthentic-Behavior-Re](https://about.fb.com/wp-content/uploads/2022/01/December-2021-Coordinated-Inauthentic-Behavior-Report-2.pdf)
[port-2.pdf](https://about.fb.com/wp-content/uploads/2022/01/December-2021-Coordinated-Inauthentic-Behavior-Report-2.pdf)


-----

**Target of Coordinated Inauthentic Behaviour Disruptions, 2017 - 2022[11]**

We know that CIB threats are rarely confined to one platform. We share our findings and
threat indicators with industry peers so they too can detect and stop threat activity, and
we can build our collective response to CIB.

### Fake accounts

We do not allow fake accounts on Facebook, as they can be vehicles for a range of
harmful content and behaviour, including spreading misinformation. In the first quarter of
2023, we detected and removed 426 million fake accounts, 98.7 per cent of which we
detected proactively.[12] The majority are caught within minutes of registration.

### Tools to inform and programs to promote media literacy

We provide a number of programs to inform users about possible misinformation on our
services.

11 We define targets as:

    - Domestic: IO that targets public debate in the same country from which it operates.

    - Foreign: IO that targets the public debate in a different country from which it operates.

    - Mixed: We also see IO campaigns and threat actors that run campaigns that target both
domestic and foreign audiences

12 Meta, Community Standards Enforcement Report Q1 2023,
[https://transparency.fb.com/data/community-standards-enforcement/fake-accounts/facebook/](https://transparency.fb.com/data/community-standards-enforcement/fake-accounts/facebook/)


-----

We earlier outlined how our partnerships with third-party fact checkers helps us label
false content and direct people to authoritative sources. We have also developed a
number of other labels and signals for users relating to the trustworthiness of posts they
see on Facebook. These include:

  - the context button, which provides information about the sources of articles in
Feed[13]

  - labels for content from state media organisations.

These are all just-in-time notices that provide labelling and information related to specific
pieces of content that individuals have seen.

We also provide information about Pages more generally, for those users who would like
more information around the author behind the content.[14] Every Page contains a Page
Transparency tool, which includes information such as previous names for the Page, the
number of admins, the country location of admins, and ads being run by the Page.

On some issues, Meta also provides information hubs with curated and authoritative
expert information. For example, we launched a Coronavirus Information Centre on
Facebook in Australia that provides a centralised hub of the most up-to-date information
on COVID-19. According to our latest transparency report under the misinformation and
disinformation code, there were over 1.8 billion visits to the COVID-19 Information Centre.
Over 7.5 million of these visits were from users in Australia.[15]

And, finally, we also run prompts around local elections to encourage people to vote, and
direct users to the Australian Electoral Commission (AEC) website: The ‘enrol to vote’
prompt was seen by 23.3 million Australians, with 54,000 people clicking through to the
AEC website, and 900,000 people sharing it across their Feeds. The ‘election day
reminder’ was seen by almost 11 million Australians, clicked through 175,000 times, and
shared by 60,000 people on their Feed.[16]

In addition to our in-product work, we know that combatting misinformation requires
cross-sector collaboration. We continue to partner with industry, government, academics

13 J Smith, A Leavitt & G Jackson, ‘Designing New Ways to Give Context to Stories’, Facebook Newsroom,
[https://about.fb.com/news/2018/04/inside-feed-article-context/, 8 April 2018](https://about.fb.com/news/2018/04/inside-feed-article-context/)

14 R Leathern and E Rogers, ‘A New Level of Transparency for Ads and Pages’, Facebook Newsroom, 28 June 2018,
[https://about.fb.com/news/2018/06/transparency-for-ads-and-pages/.](https://about.fb.com/news/2018/06/transparency-for-ads-and-pages/)

15 J Machin, ‘Meta’s third annual transparency report under Australia’s misinformation and disinformation code’, Meta
Australia Policy Blog, 7 June 2023,
[https://medium.com/meta-australia-policy-blog/metas-third-annual-transparency-report-on-australia-s-disinformation-a](https://medium.com/meta-australia-policy-blog/metas-third-annual-transparency-report-on-australia-s-disinformation-and-misinformation-industry-c4afe7953af2)
[nd-misinformation-industry-c4afe7953af2.](https://medium.com/meta-australia-policy-blog/metas-third-annual-transparency-report-on-australia-s-disinformation-and-misinformation-industry-c4afe7953af2)

16 Ibid.


-----

and civil society organisations to ensure the measures we take to address misinformation
are based on expert information, and have the most effective impact.

As well as partnerships with third-party fact-checkers, some of our partnerships include:

  - In September 2022, we launched a “Don’t Be a Misinfluencer” campaign for public
figures and creators. The campaign aims to prevent the amplification of
misinformation and includes a toolkit with information on how to identify and
combat misinformation.[17]

  - In October 2022, we launched a media literacy ‘Check the Facts’ initiative for
Australians with the Australian Associated Press. The campaign uses videos and
social tiles to teach Australians about the importance of fact-checking, and how to
recognise and avoid the spread of misinformation.[18]

  - Supporting First Draft (now RMIT FactLab) to conduct training with journalists on
how to identify and prevent amplifying misinformation and disinformation.

  - Sponsoring events in relation to misinformation and disinformation, including the
Australian Media Literacy Alliance’s inaugural Media Literacy Summit, and the
2023 Sydney Dialogue.

  - We sponsored the Business Council of Australia in their ‘One Shot Closer
Campaign’.[19] The campaign brings together employers across Australia to speak in
a united voice to boost vaccination rates across Australia.

  - We joined UNICEF Australia’s Vaccine Alliance that encourages cross-sector
collaboration and aligns with governments and communities in Australia and
around the world to provide equitable access to the vaccine.[20] We contributed free
advertising to the alliance to promote and encourage people to get vaccinated.

  - To support the rollout of the COVID-19 vaccine in Australia, we gave millions of
dollars worth of advertising credits to the Australian Federal and State
Governments. This helps to ensure the promotion of authoritative vaccine

17 First Draft, ‘Protect your voice: a toolkit for Australian influencers and celebrities’, First Draft website,
[https://firstdraftnews.org/tackling/protect-your-voice-a-toolkit-for-australian-influencers-and-celebrities/](https://firstdraftnews.org/tackling/protect-your-voice-a-toolkit-for-australian-influencers-and-celebrities/)

18 Australian Associated Press, ‘Australians urged to Check the Facts’, AAP website,
[https://www.aap.com.au/news/australians-urged-to-check-the-facts/, 25 October 2021.](https://www.aap.com.au/news/australians-urged-to-check-the-facts/)

[19 Business Council Australia, ‘One shot closer’ Campaign, https://www.oneshotcloser.com.au/](https://www.oneshotcloser.com.au/)

20 UNICEF Australia, ‘COVID vaccination alliance’,
[https://www.unicef.org.au/about-us/partnerships/covid-vaccination-alliance](https://www.unicef.org.au/about-us/partnerships/covid-vaccination-alliance)


-----

information from governments around the country.

And, finally, we recognise that it is critical to develop strong partnerships with local
regulators and law enforcement to deliver our efforts. We worked closely with the
Australian Electoral Commission (AEC), the Government’s Election Integrity Assurance
Taskforce (EIAT), and a range of government and law enforcement agencies in the lead up
to the election.

This involved working closely with the AEC to respond to all content where they had
concerns about compliance with Australian electoral law. We also worked with the
Australian Government’s EIAT to undertake scenario planning for different online issues
that may arise during the course of an election campaign.

### Supporting transparency efforts and research

We also recognise the importance of supporting transparency efforts to encourage
scrutiny of online misinformation trends.

We support transparency efforts via: our own products; and by supporting research by
academics and experts.

We have built industry-leading products like the Meta Ad Library, which is a searchable
archive of all social issues and political ads on our services in Australia. We have
progressively added functionality and real-time data on these ads.

We have also expanded the Facebook Open Research and Transparency (FORT) initiative
so that it is available to Australian researchers. We also expanded the data that is
available within the FORT platform, to now provide access to up to date data sets on
targeting information for ads about social issues, elections, and politics.

Some research we have supported in recent years includes:

  - New research on misinformation from the Queensland University of Technology by
Michelle Riedlinger and Silvia Montaña-Niño, and Marina Joubert (Stellenbosch
University), Víctor García-Perdomo (Universidad de La Sabana) on ‘Countering
misinformation in the Southern Hemisphere: A comparative study’.


-----

  - New research on media literacy of First Nations peoples by Professor Tristan
Kennedy from Monash University.

  - In 2021, we commissioned Dr Jake Wallis and the Australian Strategic Policy
Institute to undertake a review of disinformation-for-hire, specifically targeting
Australia and the Asia-Pacific region. This research was launched in August 2021.[21]

  - Commissioning independent research by respected Australian academic Dr
Andrea Carson to map government approaches to combating misinformation
around the world, focussing on the Asia-Pacific region[22]. The report ‘Tackling Fake
News’ was launched in January 2021.

  - Investing over US$4 million in a global round of funding for academic research on
misinformation and polarisation across 2021 and 2022, with four winners from
Australian universities. Proposals included ‘Testing fact and logic-based responses
to polarising climate misinformation’ (John Cook and Sojung Kim, Monash
University), ‘How fact checkers compare: News trust and COVID-19 information
quality’ (Andrea Carson, James Meese, Justin B. Phillips, Leah Ruppanner, La
Trobe University),[23] ‘Indigenous women and LBGTQI+ people and violence on
Facebook’ (Bronwyn Carlson, Macquarie University), and ‘Unpacking trust and bias
in social media news in developing countries’ (Denis Stukal, University of
Sydney).[24]

  - Supporting the Australian Media Literacy Alliance to conduct the first Australian
national media literacy survey. The results of the survey were released in October
2021 alongside recommendations for governments, companies and communities
to improve media literacy.[25]

[21 Dr J Wallis, ‘Influence for hire: the Asia-Pacific’s online shadow economy’, https://www.aspi.org.au/report/influence-hire,](https://www.aspi.org.au/report/influence-hire)
Australian Strategic Policy Institute, 10 August 2021.

22 A Carson, ‘Fighting Fake News: A Study of Online Misinformation Regulation in the Asia-Pacific’,
[https://www.latrobe.edu.au/__data/assets/pdf_file/0019/1203553/carson-fake-news.pdf](https://www.latrobe.edu.au/__data/assets/pdf_file/0019/1203553/carson-fake-news.pdf)

23 A Leavitt, K Grant, ‘Announcing the winners of Facebook’s request for proposals on misinformation and polarization’,
Facebook Research,
[https://research.fb.com/blog/2020/08/announcing-the-winners-of-facebooks-request-for-proposals-on-misinformation-](https://research.fb.com/blog/2020/08/announcing-the-winners-of-facebooks-request-for-proposals-on-misinformation-and-polarization/)
[and-polarization/, 7 August 2020.](https://research.fb.com/blog/2020/08/announcing-the-winners-of-facebooks-request-for-proposals-on-misinformation-and-polarization/)

24 Facebook Research, ‘Announcing the 2021 recipients of research awards in misinformation and polarisation’, Facebook
Research,
[https://research.fb.com/blog/2020/08/announcing-the-winners-of-facebooks-request-for-proposals-on-misinformation-](https://research.fb.com/blog/2020/08/announcing-the-winners-of-facebooks-request-for-proposals-on-misinformation-and-polarization/)
[and-polarization/, 14 September 2021.](https://research.fb.com/blog/2020/08/announcing-the-winners-of-facebooks-request-for-proposals-on-misinformation-and-polarization/)

25 Australian Media Literacy Alliance, ‘Towards a national strategy for media literacy’,
[https://medialiteracy.org.au/wp-content/uploads/2021/10/AMLA-Consultation-Workshop-Report_UPDATE-25-10-2021.p](https://medialiteracy.org.au/wp-content/uploads/2021/10/AMLA-Consultation-Workshop-Report_UPDATE-25-10-2021.pdf)
[df, October 2021.](https://medialiteracy.org.au/wp-content/uploads/2021/10/AMLA-Consultation-Workshop-Report_UPDATE-25-10-2021.pdf)


-----

  - Supporting an analytical paper by First Draft on disinformation and
misinformation amongst diaspora groups with a focus on Chinese language.[26] The
paper aims to inform policymakers on how to reduce misinformation within
Chinese diaspora communities ahead of the next federal election.

26E Chan, S Zhang, ‘Disinformation, stigma and chinese diaspora: policy guidance for Australia’, First Draft website,
[https://firstdraftnews.org/long-form-article/disinformation-stigma-and-chinese-diaspora-policy-guidance-for-australia/,](https://firstdraftnews.org/long-form-article/disinformation-stigma-and-chinese-diaspora-policy-guidance-for-australia/)
31 August 2021.


-----

## Comments on draft legislation

### General comments on regulation covering misinformation

We are committed to working with policymakers and partners around the world to meet
the challenges posed by misinformation and disinformation. This is a continuous
challenge for governments, industry, media, civil society and academia, and cross-sector
cooperation is essential.

Meta continues to contribute to the debate about effective regulation in this space. For
example, Meta has worked constructively with Government and industry in Australia to
increase accountability and transparency around our misinformation efforts. In 2020,
Meta (then Facebook) became a founding member and signatory to the Australian
Disinformation and Misinformation Industry Code.[27]

The Code is a credible, world-leading first step in the collaboration between the
technology industry and governments to combat misinformation. Two years on, the Code
has been an effective framework to increase transparency around companies’ efforts to
combat misinformation and disinformation, and raise industry standards, so much so,
that other countries around the world are looking to emulate this approach.

We have made one point consistently about regulation or legislation covering
misinformation and disinformation. The key challenge is around definitions and risk of
overreach. There can sometimes be conflation and confusion between concepts such as
foreign interference, foreign influence, disinformation and misinformation.

Even with a definition that tries to anchor to concepts like “harm” and “false”, there is a
level of subjectivity in determining whether particular claims constitute misinformation.
Reasonable people may disagree on what constitutes misinformation. Additionally, our
societal understanding of what is true or false may evolve as more evidence comes to
light: for example, at the beginning of the COVID-19 pandemic, Meta did not allow claims
that COVID vaccines caused blood clots, and this policy needed to evolve as the medical
advice improved its understanding of vaccine side effects.

Even the terms ‘misinformation’ and ‘disinformation’ can be misused in contemporary
political debate to discredit legitimate political opinions. We often see participants in

27 J Machin, ‘Facebook’s response to Australia’s disinformation and misinformation industry code’, Meta
Australia Blog, 21 May 2021,
[https://australia.fb.com/post/facebooks-response-to-australias-disinformation-and-misinformation-industr](https://australia.fb.com/post/facebooks-response-to-australias-disinformation-and-misinformation-industry-code/)
[y-code/](https://australia.fb.com/post/facebooks-response-to-australias-disinformation-and-misinformation-industry-code/)


-----

public debate describing the perspectives of political opponents as ‘misinformation’.
Research that DIGI commissioned in 2021 found that this is the case for the Australian
population, who would take significantly varying views about whether claims were
misinformation and this was often influenced by their own political beliefs.[28]

### Specific comments on the draft legislation

There are two aspects of the current draft legislation that we believe could risk overreach.

Firstly, the legislation captures misinformation that is false, misleading or deceptive and
could cause or contribute to serious “harm”, but the definition of harm is too ambiguous
and broad and determining whether information is false and misleading is often not clear.
It suggests claims could constitute misinformation if they are false and have broad
impacts including “disruption of society”, democracy, the environment, or economic or
financial harm.

It is not possible for digital platforms to foresee (because policies need to be forward
looking) how individual content could adversely affect aspects of the Australian society,
economy or environment in such a way to meet the proposed definitions . It could also be
easy to mount an argument that opposing political opinions could have some possible
long-run harm, and therefore that platforms have obligations to police or limit those
opinions (even if the actual long-run impact is not clear).

This definition risks being so broad as to grant a regulator discretionary standard-making
power over a very wide scope of content. By virtue of requiring platforms to have some
measures in place for a certain category of content (whether those are policies for
removal, fact-checking programs, or commitments to limit the distribution of that
category), a code or standard is removing the possibility for a platform to take a view that
the content is legitimate political speech that should be left untouched.

Moreover, it is important to consider the scope of the definition within the context of the
very large penalties that can apply - the greater of AUD7,825,000 or up to 5% of annual
global turnover. The significance of these penalties will likely incentivize an
over-enforcement and removal of content, with possible unintended adverse impacts to
free expression and open communication.

28 See DIGI, Australian Code of Practice on Disinformation and Misinformation Annual Report
[https://digi.org.au/disinformation-code/code-review/](https://digi.org.au/disinformation-code/code-review/)


-----

In order to reduce the risk of overreach, we recommend adjusting the definition of harm
to ideally be limited to imminent, physical harm that can occur to individuals, or harm to

people's ability to participate in democratic processes such as an election or census. Broader
references to categories such as “environment” or “disruption of society” risk serious
areas of overreach and unintended adverse impacts to legitimate speech. Aligning the
definition to be consistent with the definition outlined in the Australian Disinformation
and Misinformation Industry Code could achieve this.

Secondly, there should be greater Parliamentary oversight before the ACMA is able to
develop and register a mandatory standard. While the standard-making power as drafted
is similar to other regulatory standard-making powers in relation to issues like online
safety and telecommunications, misinformation is a special issue where the power to
influence national political debate is especially potent. While the ACMA has a reputation
as a very respected and even-handed regulator, the legislation should be drafted in a way
that has greater checks and balances against the risk of a future version of the ACMA
misusing these very significant powers for political ends, or to consider any unintended
adverse consequences. This is especially the case for the proposed emergency
standard-making powers where such powers should be subject to greater scrutiny.

Given misinformation is inextricably linked to national political debate, and to safeguard
the risk of these powers being used for partisan political ends, we recommend some
greater measure of oversight before a misinformation standard is developed and
registered. We do not believe the standard powers for the Senate to disallow regulations
is a sufficiently meaningful oversight mechanism in this instance, given the low likelihood
that it will be used.

While Meta does not have a fixed view on what specific oversight would be most
effective, some possible suggestions could include:

  - The establishment of a Parliamentary committee that is required to approve all
misinformation standards prior to their commencement, similar to the
consideration of international treaties.

  - Compulsory requirements that any code or standards must first be developed by
industry with the regulator providing oversight to ensure the standards are able to
meet community expectations.

  - Stronger protections in the legislation (not the explanatory memorandum) that
provide platforms a defence for, and protections of, legitimate political speech.

  - Explicit standing for third parties to seek merits review of any standard by courts.


-----

