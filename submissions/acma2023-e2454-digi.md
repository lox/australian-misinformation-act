# DiGi

To: Department of Infrastructure, Transport, Regional Development, Communications and the Arts
Attention:Pauline Sullivan,
First Assistant Secretary          - Online Safety, Media and Platforms Division

By [emaikinformation.integrity@infrastructure.gov.au.](mailto:emaikinformation.integrity@infrastructure.gov.au)

18 August, 2023

Dear Ms Sullivan,

The Digital Industry Group Inc. (DIGI) thanks you for the opportunity to provide our views on the exposure
draft of Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill
2023 (the Bill).

DIGI is a non-profit industry association that advocates for the interests of the digital industry in Australia.

DIGI's founding members are Apple, eBay, Google, Linktree, Meta, TikTok, X (f.k.a Twitter), Spotify, Snap
and Yahoo. DIGI's vision is a thriving Australian digitally-enabled economy that fosters innovation, a
growing selection of digital products and services, and where online safety and privacy are protected.

At the outset, DIGI wishes to underscore that our members share and support the Government's
commitment to combating the risks posed by disinformation and misinformation while ensuring that
freedom of speech and privacy is protected. DIGI supports legislation that strengthens the ACMA's
oversight of the Australian Code of Practice on Disinformation and Misinformation (ACPDM), developed
and administered by DIGI.

DIGI worked with eight major digital service providers to develop the ACPDM. The ACPDM was launched

in February 2021 in response to government policy as set out in _Regulating_ _in_ _the_ _Digital_ _Age:_ _Government_
_Response_ _and_ _Implementation_ _Roadmap_ _for_ _the_ _Digital_ _Platforms_ _Inquiry.1_ The ACPDM is a dynamic
self-regulatory tool that DIGI has continued to update and strengthen through periodic review and other
enhancements since it was first introduced.

The ACPDM adopts an outcomes based approach that aims to incentivise signatories to be more
transparent and accountable for their response to harms caused by disinformation and misinformation.
To date, the code has been adopted by Apple, Adobe, Google, Meta, Microsoft, Redbubble, TikTok and X
(f.k.aTwitter). These companies have all committed to implement safeguards to protect Australians
against online disinformation and misinformation. Mandatory code commitments include: Publishing &
implementing policies on misinformation and disinformation, providing users with a way to report content

1 The Treasury (12/12/19), _Regulating_ _in_ _the_ _digital_ _age:_ _Government_ _Response_ _and_ _Implementation_ _Roadmap_ _for_ _the_
_Digital_ _Platforms_ _Inquiry,_ [https://treasury.qov.au/sites/default/files/2019-12/Government-Response-p2019-41708.pdf](https://treasury.qov.au/sites/default/files/2019-12/Government-Response-p2019-41708.pdf)


-----

# DiGi

against those policies and implementing a range of scalable measures that reduce its spread & visibility
(Mandatory commitment #1). Every signatory has agreed to prepare annual transparency reports about
those efforts to improve understanding of both the management and scale of mis- and disinformation in
Australia (Mandatory commitment #7).

Additionally, there are a series of opt-in commitments that platforms adopt if relevant to their business
model: (Commitment #2) Addressing disinformation in paid content; (#3) addressing fake bots and
accounts; (#4) transparency about source of content in news and factual information (e.g. promotion of
media literacy, partnerships with fact-checkers) and (#5) political advertising; and (#6) partnering with
universities/researchers to improve understanding of mis and disinformation.

We also acknowledge that drafting legislation that regulates misinformation and disinformation is
inherently challenging, because it must strike a balance between regulating lawful but potentially harmful
online communications while upholding Australians' freedom of expression, particularly concerning
political matters. Some of the key recommendations DIGI advances in this submission include:

A. The Bill should be focused on providing the ACMA with clearly scoped oversight and information
gathering powers.

B. In this regard, the Bill should give priority to tackling the most harmful forms of disinformation
which can artificially manipulate online discourse, and seriously threaten democratic processes
such as elections and social welfare.

C. The Bill should be amended to include stronger safeguards for free speech and greater
transparency and parliamentary accountability concerning the exercise of ACMA's powers.

D. In particular, the powers to request codes and make standards should not be capable of being
exercised as the Bill provides where the ACMA simply deems it 'convenient' but only where it has
determined that the code is needed to address systemic failures of the industry to implement
adequate measures to protect the community in relation to _disinformation._

Our submission contains specific suggestions and recommendations for changes to the Bill in these
areas and others.

We thank you for your consideration of the matters raised in this submission. DIGI considers itself a key
partner in the Government's efforts to address mis- and disinformation online, and we look forward to our
continued dialogue on our shared goals in this area. Should you have any questions about this
submission, please do not hesitate to contact me.

Yours sincerely,

Dr Jennifer Duxbury
Director Policy, Regulatory Affairs and Research


-----

# DiGi

Digital Industry Group Inc. (DIGI)

**Section** **1:** **Summary** **of** **key** **recommendations** **of** **this** **submission.** **3**

###### Recommendation concerning process of development for the Bill 4
 Recommendations concerning regulatory priority of the Bill 4
 Recommendations concerning scope of content subject to the Bill 4
 Recommendations concerning scope of services subject to the Bill 6
 Recommendations concerning the exercise of the ACMA’s regulatory powers 7
 Recommendations about the powers of the ACMA to obtain information, request codes and make standards 7
 Recommendations concerning the processes for development of codes and standards 8 Recommendations about transparency and accountability requirements 9
 Recommendations concerning coordination between government agencies on industry codes 9

Recommendations concerning penalties 9

**Section** **2:** **Discussion** **of** **DIGI's** **key** **issues** **with** **Bill** **10**

1. Process of development for the Bill. 10

2. Regulatory priority of the Bill 11

3. Scope of content subject to the Bill 13

4. Scope of services subject to the Bill 21

5. The exercise of the ACMA’s regulatory powers 26

6. Powers of the ACMA to obtain information, request codes and make standards 30

7. The process for making codes and standards 33

8. Transparency and accountability requirements 35

9. Coordination between government agencies on industry codes 36

10. Penalties 37

### Section 1: Summary of key recommendations of this submission.

The overarching theme of this submission is that the scope of the (Combatting Misinformation and
Disinformation) Bill (the Bill) requires considerable revisions, including amendments limiting the range of
content and services subject to regulation and stronger safeguards concerning the bases upon which the
ACMA can exercise the powers conferred by the Bill. The scope of services covered by the Bill should also
be much more specifically defined to address those services that carry particular risk of being the target
of disinformation campaigns.


-----

# DiGi

DIGI makes **47** **specific** **recommendation:** in this submission, drawing on our experience developing and
administering the _ACPDM2,_ and research in the field, including research commissioned by DIGI into
Australians' perceptions of misinformation3. This section of the submission contains a summary of DIGI's
specific suggestions regarding the form of these safeguards.

#### Recommendation concerning process of development for the Bill

A. While DIGI considers that mis- and disinformation online require concerted efforts from industry,
which has informed our development of the ACPDM, we recommend that the Government
reconsider the very broad scope of the Bill in the light of the range of existing legislative
frameworks concerning harmful and false and misleading content. Where areas of overlap are
identified, we recommend that the scope of the Bill be reduced to avoid regulatory duplication.

#### Recommendations concerning regulatory priority of the Bill

B. Tackling disinformation rather than misinformation should be the Government's regulatory
priority. As a general principle, the Bill should primarily focus on providing the ACMA with powers
to ensure platforms implement adequate measures (systems and processes) to protect the
community from disinformation, and grant the ACMA a more limited set of oversight powers with
respect to platforms1 responses to misinformation.

C. We _recommend_ _that_ _the_ _codes_ _and_ _standards-making_ _powers_ _of_ _the_ _ACMA_ _in_ _the_ _Bill,_ _should_ _be_
_limited_ _to_ _disinformation,_ _not_ _misinformation._

D. We support the ACMA being granted power under the Bill to require platforms to keep and provide
records and information concerning misinformation and disinformation on their services,
although we recommend that these powers be better scoped.

#### Recommendations concerning scope of content subject to the Bill

E. The Bill should be amended to be consistent with the Government's intent to make it exceedingly
clear that the ACMA’s powers do not extend to regulating individual instances of misinformation
or disinformation, for example, by being able to take-down any content, or obtain access to
non-public content, consistent with the statement of intention in the _Guidance_ _Note._

F. In order to ensure that the ACMA and digital platform services in scope of the Bill have a common
understanding about the content that is to be the subject of regulation, the Government needs to
reconsider the proposed scope of 'disinformation' and 'misinformation' as defined under the Bill.
To address this need we have proposed specific amendments in G to 0 of this submission.

G. The definitions of misinformation and disinformation in clause 7(1) and (2) of the Bill should be
amended to ensure that they apply to content that digital platforms can 'reasonably verify1 to be
false, misleading or deceptive.

_2_ _Australian_ _Code_ _of_ _Practice_ _on_ _Disinformation_ _and_ _Misinformation,_ updated 22 December 2022 (ACPDM) available at
[https://digi.org.au/disinformation-code/](https://digi.org.au/disinformation-code/) .

3 Resolve Strategic 'Research on Australians' perceptions of misinformation' in DIGI, _AustraHan_ _Code_ _of_ _Practice_ _on_
_Disinformation_ _and_ _Misinformation_ _Annual_ _Report_ [https://digi.org.au/disinformation-code/code-review/.](https://digi.org.au/disinformation-code/code-review/)


-----

# DiGi

H. _The_ _definition_ _of_ _disinformation_ _should_ _be_ _subject_ _to_ _an_ _objective_ _test,_ _which_ _enables_ _platforms_ _to_
_more_ _readily_ _determine_ _whether_ _content_ _is_ _disinformation_ _in_ _practice._ Specifically, sub-clause 7(2)
of the Bill should be redefined to remove the requirement in sub-clause 7(2)(e) that the person
disseminating or causing the dissemination of the content must 'intend to deceive another
person’. This should be replaced with a requirement that the content be disseminated by
'deceptive, manipulative or bulk, aggressive behaviours (which may be perpetrated via automated
systems) and includes behaviours which are intended to artificially influence users’ online
conversations and/or to encourage users of digital platforms to propagate content'.

1. The ‘contributes to' serious harm element of the definitions of misinformation and disinformation
in the Bill in sub-clauses 7(1)(d) and (2)(d) of the Bill should be removed as this is a highly
subjective test and impractical for platforms to apply.

J. _The_ _term_ _'serious_ _harm’_ _should_ _be_ _defined_ _as_ _a_ _harm_ _that_ _causes_ _'severe_ _and_ _wide-reaching_ _impacts_
_on_ _the_ _community_ _which_ _are_ _reasonably_ _foreseeable._ _We_ support the requirement to have a
'serious harm' threshold for content in scope of the Bill. Clause 7(3) of the Bill lists the factors that
are relevant to the assessment of serious harm, however, these factors should not individually, or
in combination, be the sole determinants of whether content is in scope.

K. _The_ _types_ _of_ _harms_ _attributed_ _to_ _misinformation_ _and_ _disinformation_ _in_ _clause_ _2_ _of_ _the_ _Bill_ _should_ _be_
_limited_ _to_ _harms_ _that_ _can_ _be_ _objectively_ _assessed_ _by_ _providers_ _of_ _digital_ _platform_ _services_ _based_ _on_
_evidence_ _of_ _actual_ _harms_ _caused_ _by_ _the_ _dissemination_ _of_ _false,_ _misleading_ _or_ _deceptive_ _content_
_online,_ _rather_ _than_ _by_ _reference_ _to_ _the_ _hypothetical_ _examples_ _in_ _the_ _Guidance_ _Note._ For examples,
this list might be restricted to:

a. undermining the integrity of Australian democratic processes (such as elections and
referenda).

b. endangering the health, safety, or security of Australians.

To further ensure the ACMA and regulated platforms have a common understanding of the
harmful content in scope of the Bill, the criteria for the types of content that falls in scope of the
definitions of misinformation and disinformation should be further explained in regulatory
guidelines.

L. Harm to the’ ‘integrity of Commonwealth, State, Territory or local government institutions should
be excluded from the lists of harms in scope of the Bill (clause 2), as it implies that platforms
should take measures to protect public institutions (including, for example, the executive
government and ministerial offices) from public criticism, when this is vital to robust political
debate.

M. Harm to the economy related to mis- and disinformation online being otherwise addressed by the
Government through competition and consumer laws, and initiatives concerning scams, and
should be out of scope of the Bill.

N. Harm to the environment does not need to be explicitly within the scope of the Bill, as the most
serious harms to the community that concern false, misleading or deceptive information that
could harm the environment are those which can endanger the health, safety, or security of
Australians and are covered by recommendation I.

O. DIGI supports strengthened regulation of existing State, Territory and Commonwealth laws
concerning hate speech via legislation of general application. Hate speech should not be


-----

# DiGi

regulated as a type of misinformation or disinformation (as provided for in clause 2 (a) of the Bill)
because the harm it causes is not related to matters of truth or falsity; such an approach would
not be comprehensive in addressing harms related to hate speech.

#### Recommendations concerning scope of services subject to the Bill

P. The "tiered approach" to defining the services in scope of this Bill has the potential to extend this
regulatory scheme to an extremely broad range of services and leaves considerable discretion to
the ACMA to determine which services/service categories will eventually be subject to
codes/standards. As a matter _of_ _principle,_ _we_ _think_ _that_ _the_ _scope_ _of_ _services_ _subject_ _to_ _the_ _Bill_
_should_ _be_ _clearly_ _dealt_ _with_ _by_ _parliament_ _via_ _the_ _provisions_ _of_ _the_ _Bill._ _To_ _the_ _extent_ _decisions_
_about_ _the_ _services_ _subject_ _to_ _regulation_ _are_ _delegated_ _to_ _the_ _ACMA_ _the_ _Bill_ _should_ _set_ _out_ _the_
_relevant_ _criteria_ _that_ _will_ _be_ _used_ _to_ _make_ _such_ _decisions._

Q. To ensure low risk services are excluded from the regulatory scheme, the categories of services
currently in scope of the Bill should be limited to those types of services that:

a. host content at the user's request and disseminate such content to the public i.e making
the content available to a potentially unlimited number of persons at the user's request;

b. enable widespread ease of distribution (virality) of content via public means.

R. The Bill be amended to create more specific proposed categories of services than the broad
categories listed in sub-clause 30(2) so that codes/standards can be more readily developed for
’like’ services.

S. The definition of 'content aggregation services' in scope of the Bill (sub-clause 4 (2)) should be
amended so that it relates to services which have ‘the primary function of creating a collection of
content (excluding exempted content such as professional news) that the provider has sourced
from multiple locations on the world wide web to enable users of the service to view the
collection of content on the service in an aggregated format’.

T. If it is intended that search engines are in scope of the Bill, search engines should be treated as a
separate service type as they have different primary function to content aggregation services. We
recommend that an internet search engine service is defined as an electronic service that
satisfies all the following conditions:

a. the service is designed to collect, organise (index) and/or rank material on the world wide
web;

b. the service has the primary function of enabling end-users to search the service’s index of
material on the world wide web or relevant results in response to the end-user’s queries;
and

c. the service returns search results in response to end-user queries.

U. The definition of 'connective media services' in scope of the Bill (sub-clause 4(3)) should be
amended so that it only captures services where the primary function of the service is to enable
online interaction between end-users via an interactive feature.

V. The definition of 'media sharing services' in scope of the Bill (sub-clause 4 (4)) should be
amended so that only services that have an interactive feature fall within this category.


-----

# DiGi

W. The Government should consider if the current self regulatory and co-regulatory codes that apply
to providers of professional news online should be updated to address misinformation and
disinformation.

X. The concept of 'excluded content' in clause 2 of the Bill as compared to 'excluded services' and
the interaction of the two definitions should also be clarified. If a service is limited to providing
excluded content the service should be treated as an excluded service and be out of scope of the
Bill.

Y. The scope of the exemptions for private communications services should be clarified so that the
Bill excludes all types of services that enable communication between a finite group of end users
similar to the approach in the EU under the Digital Services Act. For example, communication
services that are analogous to SMS and MMS services should be excluded from the scope of the
Bill.

#### Recommendations concerning the exercise of the ACMA’s regulatory powers

Z. The statement of parliamentary intent in clause 32 should be amended to make clear whether
parliament intends that the signatories to the ACPDM should request that it be registered or that
it should be replaced by a new code.

AA. The powers to make standards in section 50 of the Bill for 'exceptional and urgent circumstances'
should be removed, or at least 'exceptional and urgent' circumstances should be defined clearly
and any standards made in these circumstances subject to a sunset clause so that the standards
do not remain in place longer than necessary .

BB. Consistent with the _Guidance_ _Note,_ the ACMA should only be empowered to request an industry
code where it has determined that the code is needed to address systemic failures of a section of
the industry to implement adequate measures to protect the community in relation to
_disinformation_ (consistent with our recommendation in C that the ACMA’s code and standards
making powers should be limited to disinformation).

CC. Platforms within a section of the industry subject to a registered industry code should not be
subject to a standard unless the ACMA has determined that a standard is needed to address
systemic breaches by those platforms concerning the implementation of measures under a code.

DD. DIGI requests further clarity about how the ACMA’s powers to make standards for services in
particular sections of the industry will apply where a service in that section is an ACPDM
signatory and is complying with its commitments.

#### Recommendations about the powers of the ACMA to obtain information, request codes and make standards

EE. The power of the ACMA to make rules that require platforms to keep/provide records and the
power to obtain additional information should be limited to the purpose of:


-----

# DiGi

a. determining if providers of platforms have implemented measures (systems and
processes) that are adequate to protect the community from disinformation and
misinformation on the service;

b. enabling the ACMA to publish information about the extent platforms have implemented
measures that are adequate to protect the community from disinformation and
misinformation; and

c. in the case of disinformation, enabling the ACMA to determine if there is a need for
industry codes or standards.

FF. The scope of the ACMA’s powers in relation to requiring and obtaining records and information
should be further limited to specific types of records and information, proportionate to the
different level of harm posed by misinformation and disinformation.

GG. The cadence for demands by the ACMA on digital platforms for records, the time period for
platforms to respond, and the period for which records must be retained should be set out in the
legislation.

HH.The references to the prevalence of false, misleading or deceptive content in sub-clause 14(1)(e)
and sub-clause 18(2)(c) should be replaced with the 'trends relating to misinformation and
disinformation1. The scope of these clauses, as currently drafted, is highly problematic as in effect
gives ACMA power to require platforms to monitor the truthfulness of all the content that is
provided on a platform. In our view, this would be impossible for platforms to implement..

II. The open-ended powers of the ACMA to obtain additional information about misinformation and
disinformation from individuals in clause 19 of the Bill are excessive and should be removed.

JJ. The content of codes and standards should be clearly limited to the purpose of ensuring services
that are at risk of disseminating disinformation (in accordance with criteria set out in the
legislation) have measures in place that provide appropriate protections for the community
against disinformation on their services. As recommended in Q, the risk of virality/widespread
ease of distribution of disinformation via public means should be a key marker of a service in
scope of the Bill.

KK. The Government should give further consideration to specifying the criteria for 'systemic1 failures,
breaches or issues (as the case may be) that will trigger an exercise of the ACMA’s powers to
make a code or standard.

#### Recommendations concerning the processes for development of codes and standards

LL. The Bill should provide for a minimum period of 12 -24 months for industry associations to
develop a new code, depending on the size of the industry section in scope and the number of
codes that the ACMA requests be developed concurrently.

MM. A minimum of 12-24 months should be allowed for a code to be in force before ACMA
determines that a code has failed/partially failed under sub-clause 48(1) and 49(1) as a
precondition to making a standard.


-----

# DiGi

NN.The ACMA should be required to undertake public consultation prior to making an industry
standard. This is critical when taking into account the public's interest in this Bill.

#### Recommendations about transparency and accountability requirements

00. Decisions by the ACMA concerning industry codes and standards should be conditional on the
ACMA publishing reasons for its decision, substantiated by relevant evidence (e.g a decision to
require a code/standard or deregister a code should be substantiated by evidence of systemic
failures/breaches by platforms to safeguard the community from disinformation). The
assessment should include an analysis of the records/information obtained from digital
platforms concerning the measures (systems and processes) that they have implemented to
protect the community from disinformation.

PP. The registration of codes and the making of standards by the ACMA should also be subject to
additional human rights impact assessments.

QQ. Rules and standards made by the ACMA under this legislation should be disallowable
instruments.

RR. The Bill should make provision for a statutory review after 12 months to assess its impact on
freedom of speech and communication, privacy, and other human rights.

#### Recommendations concerning coordination between government agencies on industry codes

SS. The Government should consider if there is a need for additional structures to facilitate cross
portfolio cooperation and to enable codes and standards for the online industry to be developed
under a more streamlined process, for example using a common framework or model definitions.

#### Recommendations concerning penalties

TT. Penalties under the Bill should be comparable to those in the _Broadcasting_ _Act_ _1992_ _(Cth),_
including for breaches of broadcast codes and standards. The maximum penalties under the Bill
should be limited to systemic breaches of standards concerning disinformation and capped at
10,000 penalty units. The Bill should define a 'systemic breach' by reference to the criteria by
which such breach will be assessed.

UU. The Bill should include a provision that makes the mass marketing or sale of disinformation
campaigns a crime.


-----

# DiGi

### Section 2: Discussion of DIGI's key issues with Bill

###### 1. Process of development for the Bill.

1.1. **_Key_** **_Issue:_** **_It_** **_is_** **_not_** **_evident that_** **_there_** **_is_** **_a_** **_need_** **_for_** **_the_** **_broad_** **_range_** **_of_** **_services_** **_and_**
**_harmful_** **_content_** **_in_** **_scope_** **_of_** **_the_** **_Bill_** **_to_** **_be_** **_subject_** **_to_** **_such_** **_an_** **_extensive_** **_suite_** **_of_** **_additional_**
**_regulatory_** **_interventions_** **_given_** **_the_** **_range_** **_of_** **_existing_** **_laws_** **_that_** **_cover_** **_the_** **_harms_** **_in_** **_scope_** **_of_**
**_the_** **_Bill._**

1.2. As drafted, there appears to be a considerable overlap between the powers afforded to
the ACMA to regulate misinformation and disinformation under the Bill and laws of
general application that for example, prohibit cyber-abuse ,4 use of a carriage service to
menace, harass or cause offence,5 pro-terror conduct and materials6 , misleading or
deceptive conduct and false statements in trade or commerce,7 defamation8 , hate
speech9 and scandalising contempt that is either ‘scurrilous abuse’, or ‘excites misgivings
as to the integrity, propriety and impartiality brought to the exercise of judicial office’10.

1.3. There is a problematic area of direct overlap between this Bill and the Basic Online Safety
Expectations Determination 2022 (Cth) (BOSE) under the Online Safety Act 2021 (Cth)
because the term "harmful" in the BOSE is used in a completely undefined manner.
Arguably a provider could have BOSE obligations with respect to harmful content or
activities that are intended to be regulated under the scheme under this Bill and vice
versa. Note that clause 36 of the Bill indicates that a misinformation code or
misinformation standard will have no effect to the extent it deals with a matter covered
by the BOSE. The overlap in harms regulated by the BOSE and under this Bill is therefore a
real issue. As a practical matter, this adds an additional layer of complexity for platforms
complaint-handling processes as well as imposing significant challenges for platforms in
assessing what information they should provide to respond to requests about

‘misinformation and disinformation’ or ‘false, misleading or deceptive content’ under the
record keeping and information gathering powers set out in Division 2 and 3 of the Bill.
Given changes to the Online Safety Act 2021 (Cth)(OSA) are included in the Bill, we
recommend that the Government further consider under which legislative frameworks
and by which regulators relevant harms are best addressed.

4 For example section 36 and 37 Online Safety Act 2021 (Cth).

5 Section 474.17 of the Criminal Code Act 1995 (Cth).

6 Division 101 of the Criminal Code Act 1995 (Cth).

7 Section 18 and section 29 of the Australian Consumer Law.

8 Australian Uniform Defamation Laws.

9 Section 18 (c) Racial Discrimination act 1975 (Cth).

10 R v Kopyto (1987) 62 OR (2d) 449,910; A-G (NSW) v Mundey [1972] 2 NSWLR 887.R v Dunbabin; Ex Parte Williams

[1935] 53 CLR 434, 442 (Rich J); A-G (NSW) v Mundey [1972] 2 NSWLR 887, 910.


-----

# DiGi

**Specific** **recommendation**

A. While DIGI considers that mis- and disinformation online require concerted efforts from
industry, which has informed our development of the ACPDM, we recommend that the
Government reconsider the very broad scope of the Bill in the light of the range of existing
legislative frameworks concerning harmful and false and misleading content. Where areas of
overlap are identified, we recommend that the scope of the Bill be reduced to avoid regulatory
duplication.

###### 2. Regulatory priority of the Bill

2.1. **_Key_** **_Issue:_** **_The_** **_Bill_** **_assumes_** **_that_** **_disinformation_** **_and_** **_misinformation_** **_are_** **_equally_** **_harmful_**
**_and_** **_provides_** **_the_** **_ACMA_** **_with_** **_very_** **_broad_** **_powers_** **_to_** **_regulate_** **_both_** **_of_** **_those_** **_online_** **_activities._**
**_Disinformation_** **_should_** **_be_** **_understood_** **_as_** **_an_** **_online_** **_activity_** **_that_** **_poses_** **_a_** **_much_** **_greater_** **_and_**
**_more_** **_objectively_** **_assessable_** **_threat_** **_to_** **_the_** **_community_** **_whilst_** **_misinformation_** **_is_** **_more_**
**_complex,_** **_contestable_** **_and_** **_of_** **_a_** **_lesser_** **_threat_** **_level._** **_The_** **_powers_** **_granted_** **_to_** **_the_** **_ACMA_** **_under_**
**_the_** **_Bill_** **_should_** **_be_** **_proportionate_** **_to_** **_the_** **_greater_** **_severity_** **_of_** **_harm_** **_posed_** **_by_** **_disinformation_**
**_compared_** **_to_** **_misinformation._**

2.2. There is considerable debate, including amongst academic experts, as to the meaning of
the terms disinformation and misinformation11. However, it is widely accepted that
coordinated online activity in the form of a 'disinformation campaign1 may cause
systemic public harms if they promote false, misleading or deceptive claims at a large
scale, or if they discourage citizens from engaging with high-quality sources of
information. For example, in the United States, Russia-backed actors have promoted
pseudoscience conspiracies about government vaccination programs online, while also
attacking expert institutions that make high-quality information claims12. As explained in
the 2020 discussion paper prepared by the Centre for Media Transitions at the University
of Technology in Sydney:

_The_ _entities_ _that_ _engage_ _in_ _disinformation_ _have_ _a_ _diverse_ _set_ _of_ _goals._ _Some_ _are_
_financially_ _motivated,_ _engaging in_ _disinformation_ _activities_ _for_ _the_ _purpose_ _of_
_turning_ _a_ _profit._ _Others_ _are_ _politically_ _motivated,_ _engaging_ _in_ _disinformation_ _to_

11 Andrea Carson and Liam Fallon _Fighting_ _Fake News:_ _A_ _Study_ _of_ _Online_ _Misinformation_ _Regulation_ _in_ _the_ _Asia_ _Pacific,_
(La Trobe University, 2021).
12DiResta, R., K. Shaffer, B. Ruppel, D. Sullivan, R. Matney, R. Fox, J. Albright, and B. Johnson. 2018. _The_ _Disinformation_
_Report:_ _The_ _Tactics_ _&_ _Tropes_ _of_ _the Internet_ _Research_ _Agency._ _Austin:_ _New_ _Knowledge._
[https://disinformationreport.blob.core.windows.net/disinformation-report/NewKnowledqe-Disinformation-Report-Whi](https://disinformationreport.blob.core.windows.net/disinformation-report/NewKnowledqe-Disinformation-Report-Whi)
tepaper-121718.pdf In 2016 The Mueller Report established the social media campaign by Russian actors included a
hacking operation against the Clinton Campaign which released stolen documents in Robert S Mueller III, _Report_ _on_
_the investigation_ _into_ _Russian_ _Interference_ _in_ _the_ _2016_ _Presidential_ _Election,_ Volume 1, March 2019. See
[https://www.justice.gov/storage/report.pdf,](https://www.justice.gov/storage/report.pdf) Page 4.


-----

# DiGi

_foster_ _specific_ _viewpoints_ _among_ _a_ _population,_ _to_ _exert_ _influence_ _over_ _political_
_processes,_ _or_ _for_ _the_ _sole_ _purpose_ _of_ _polarising_ _and_ _fracturing_ _societies.13_

2.3. Misinformation, in contrast, is much more complex and subjective as the concept of
misinformation is fundamentally linked to people's beliefs and value systems.14 A
coordinated disinformation campaign by foreign actors during an election is of different
order of harm to the community than a group of citizens using a digital platform to
challenge prevailing or 'orthodox' views on Covid 19 or climate change, which may be
treated as misinformation under the current definitions in the Bill. The question of
assessing the extent to which there is a need for misinformation to be regulated in the
Australian context is itself very challenging. For example, the Australian population's
self-reported exposure to misinformation or levels of concern about misinformation are
not robust metrics for assessing the actual impact of misinformation on the community
because of widely different perceptions of the meaning of the term and the fact that
individuals make value-laden assessments of the validity of online content based on their
personal characteristics including their political preferences.15 In a worst-case scenario,
heavy-handed intervention by the Government requiring wide-spread removal of

’misinformation1 from online services could conceivably fuel conspiracy theories and
contribute to erosion of trust in democratic institutions. A 2022 report from the Royal
Society, the United Kingdom's National Academy of Science, for example, says 'there is
little evidence that calls for major platforms to remove scientific misinformation content
will limit scientific misinformation's harms' and warns such measures could even 'drive it
to harder-to-address corners of the internet and exacerbate feelings of distrust in
authorities1.16

2.4. Further, as defined in clause 7(1) of the Bill, misinformation includes content that may be
posted by a user as a result of an honest mistake without any intention to cause harm.
This makes misinformation an inherently difficult issue for the Government to effectively
address by regulation (as opposed to through other mechanisms such as implementing
media literacy initiatives or providing funding to support the development of
fact-checking organisations).

2.5. _Because_ _disinformation_ _campaigns_ _pose_ _a_ _different_ _order_ _of_ _harm_ _to_ _the_ _community_ _than_
_misinformation,_ _the_ _regulatory_ _priority_ _of_ _the_ _Bill_ _should_ _be_ _to_ _regulate_ _disinformation,_
_proportionate_ _to_ _the_ _greater_ _severity_ _of_ _harm_ _posed_ _by_ _disinformation_ _compared_ _to_
_misinformation._ _We_ therefore recommend that the Bill should be primarily focused on

13 University of Technology Sydney, Centre for Media Transition, _Discussion_ _Paper_ _on_ _An_ _Australian_ _Voluntary_ _Code_ _of_
_Practice_ _for_ _Disinformation,_ October 16,2020, Page 18.

14 Ecker, U.K.H., Lewandowsky, S., Cook, J. _et al._ The psychological drivers of misinformation belief and its resistance
to correction.National _Review_ _Psychology,_ Volume 1,2022, Pages 13-29.
[(https://doi.Org/10.1038/S44159-021](https://doi.Org/10.1038/S44159-021) -00006-y'.

15 Ibid.

16 Royal Society, _The_ _online_ _information_ _environment_ _Understanding_ _how_ _the_ _internet_ _shapes_ _people’s_ _engagement_ _with_
_scientific_ _information_ (United Kingdom, 2022).


-----

# DiGi

providing the ACMA with powers to ensure high risk platforms adopt adequate measures
(systems and processes) to protect the community from disinformation (subject to
definitional changes suggested in Section 3 of this submission). In our analysis, there is
insufficient evidence to support strict Australian regulation of misinformation via
enforceable codes and standards at this time. The granting of powers to the ACMA to
require enforceable codes and standards concerning misinformation is also likely to
encourage over-removal of lawful online communications.

2.6. We therefore recommend that the powers of the ACMA to make codes and standards
should be limited to disinformation (being content that poses the greatest risk of harm to
the community), not misinformation. The Government should further consider whether
other policy interventions might be more appropriate for addressing misinformation such
as increased funding for media literacy programs and fact-checking organisations.

**Specific** **recommendations**

B. Tackling disinformation rather than misinformation should be the Government's regulatory
priority. As a general principle, the Bill should primarily focus on providing the ACMA with
powers to ensure platforms implement adequate measures (systems and processes) to
protect the community from disinformation, and grant the ACMA a more limited set of
oversight powers with respect to platforms1 responses to misinformation.

C. We _recommend_ _that_ _the_ _codes_ _and_ _standards-making_ _powers_ _of_ _the_ _ACMA_ _in_ _the_ _Bill,_ _should_ _be_
_limited_ _to_ _disinformation,_ _not_ _misinformation._

D. We support the ACMA being granted power under the Bill to require platforms to keep and
provide records and information concerning misinformation and disinformation on their
services, although we recommend that these powers be better scoped.

###### 3. Scope of content subject to the Bill

3.1. **_Key_** **_Issue:_** **_The_** **_bar_** **_for_** **_online_** **_content_** **_that_** **_is_** **_in_** **_scope_** **_of_** **_the_** **_Bill_** **_is_** **_too_** **_low,_** **_and_** **_is_** **_based_** **_on_**
**_an_** **_overly_** **_subjective_** **_test_** **_which_** **_captures_** **_material_** **_that_** **_may_** **_contribute_** **_to_** **_harms_** **_in_** **_the_**
**_distant_** **_future_** **_which_** **_are_** **_difficult_** **_to_** **_predict_** **_and_** **_vaguely_** **_defined._**

3.2. The intention of the Government, as set out in the _Guidance_ _Note_ on the Bill, is to require
platforms to implement measures in the form of systems and processes to tackle
misinformation and disinformation. In exercising its powers under the Bill, the ACMA
would not determine truth nor be empowered to regulate individual pieces of content.
This is intended to strike an appropriate balance between the public interest in
combatting the serious harms that can arise from the propagation of misinformation and
disinformation, with rights of free speech and expression. But this statement of intention


-----

# DiGi

in the _Guidance_ _Note_ is potentially at odds with the broad scope of the powers given to
the ACMA under the Bill. The Bill should be amended so that it is clear that the ACMA’s
powers do not extend to regulating individual instances of misinformation or
disinformation, for example by being able to take-down any content, or to request access
to content, consistent with the statement of intention in the _Guidance_ _Note._

3.3. The ACMA will inevitably need to make judgments about whether individual items of
content or topics of online debate are false, misleading or deceptive in determining
whether to exercise its broad powers under the Bill. This is because the subject matter of
the Bill (i.e.misinformation and disinformation) are defined by reference to the false,
misleading or deceptive nature of online content17. For example, in making a
determination on the need for codes the ACMA will likely form a view that specific
platforms' current systems and processes are inadequate to deal with disinformation and
misinformation. Presumably, this requires an evaluation of whether the systems and
processes of the platforms are working to protect users from being exposed to content
that the ACMA considers to be instances of disinformation and misinformation.To _ensure_
_that_ _the_ _ACMA_ _and_ _digital_ _platforms_ _services_ _in_ _scope_ _of_ _the_ _Bill_ _have_ _a_ _common_
_understanding_ _about_ _the_ _content_ _that_ _is_ _to_ _be_ _the_ _subject_ _of_ _regulation,_ _the_ _government_
_needs_ _to_ _rethink_ _the_ _proposed_ _scope_ _of_ _'disinformation'_ _and_ _'misinformation'_ _as_ _defined_
_under_ _the_ _Bill._

3.4. _The_ _definitions_ _of_ _misinformation_ _and_ _disinformation_ _should_ _be_ _amended_ _to_ _more_
_objectively_ _differentiate_ _the_ _two_ _concepts._ DIGI acknowledges that defining
misinformation and disinformation for the purposes of regulation is hard, not the least
because there is no universally accepted definition of these terms, and they are often
used to attack opponents' positions in value-laden debates on matters of public concern
about which reasonable people may disagree. This understanding is reflected in the
_Australian_ _Code_ _of_ _Practice_ _on_ _Disinformation_ _and_ _Misinformation_ (ACPDM).18

3.5. A common element of misinformation and disinformation as defined in sub-clause

7(1 )(a) and 7(2)(a) of the Bill is that it is 'false, misleading or deceptive.' As a matter of
principle we consider that the definitions of misinformation and disinformation in the Bill
should not encompass communications between citizens that fundamentally concern
issues of opinion. In defining misinformation and disinformation for the purposes of the
Bill it is important to acknowledge that public debate on contentious topics is integral to
well-informed and legitimate public decision making in democratic societies. Public
opinion on complex social issues including their factual premises is contestable, evolving
and changing over time as is exemplified by the lessons of the Iraq war in which the
'coalition of the willing1 including the Australian government, used a false narrative

17 clause 7, _Communications_ _Legislation_ _Amendment_ _(Combating_ _Misinformation_ _and_ _Disinformation)_ _Bill_ _2023_ (the
Bill).

18 See section 1.2 of the _Australian_ _Code_ _of_ _Practice_ _on_ _disinformation_ _and_ _Misinformation_ (ACPDM).


-----

# DiGi

concerning 'weapons of mass destruction' to justify their involvement in the war.19 More
recently, public understandings about the origin of the Covid 19 pandemic, including
questions around whether the virus was released because of a 'laboratory accident' or
'from human contact with an infected animal', are still evolving.20 These examples also
demonstrate the inherent challenges faced by platforms in verifying whether contentious
online content is 'false, misleading, or deceptive' at a particular moment in time. At the
very least, sub-clauses 7(1 )(a) and 7(2)(a) of the Bill should limit the definition of
misinformation and disinformation respectively to information that is _reasonably_
_verifiable_ _as_ false or misleading or deceptive. This is consistent with the approach in the
ACPDM.

3.6. The key distinguishing feature between misinformation and disinformation in the Bill is
whether 'the person disseminating, or causing the dissemination of the content intends
that the content deceive another person.'2122This is problematic as the intention of persons
'disseminating or causing the dissemination' of online content cannot be reliably
assessed by digital platforms either on an individual basis or at scale. The tactics used to
propagate seriously harmful false, misleading or deceptive content at scale are more
readily observable by providers of digital services and for example, include the use of
trolls, spam bots, false identity accounts known as sock puppets, paid accounts and
artificial intelligence to increase the volume and speed via which content is spread online.

DIGI recommends that the definition of disinformation in sub clause 7(2)(e) of the Bill
should be amended to replace the intentionality requirement with a requirement that the
content is disseminated by 'deceptive, manipulative or bulk, aggressive behaviours (which
may be perpetrated via automated systems) and includes behaviours which are intended
to artificially influence users' online conversations and/or to encourage users of digital
platforms to propagate content122. This would extend to the use of artificial intelligence to
deliberately deceive other users. We think this approach would enable 'disinformation' to
be more clearly differentiated from misinformation, consistent with recommendation B of
this submission.

3.7. _The_ _Bill_ _seeks_ _to_ _regulate_ _content_ _that_ _may_ _contribute_ _to_ _harms_ _in_ _the_ _distant_ _future_ _that_
_cannot_ _be_ _predicted._ _The_ _Bill_ _should_ _instead_ _be_ _limited_ _to_ _foreseeable_ _threats_ _of_ _serious_
_harm._ To qualify as misinformation or disinformation under the Bill it is sufficient that 'the
provision of the content on the digital service is reasonably likely to cause or contribute

19 Owen D Thomas 'Good faith and (dis)honest mistakes? Learning from Britain's Iraq War Inquiry', _Politics,_ Volume 37,
Issue 4 (2017); Sir John Chilcot's public statement in _The_ _Iraq_ _Inquiry_ [online], (July 6,2016). Available at:
[http://www.iraqinquiry.org.uk/the-inquiry/sir-john-chilcots-public-statement](http://www.iraqinquiry.org.uk/the-inquiry/sir-john-chilcots-public-statement) .
20See for example 'COVID origin still a mystery despite release of intelligence report' _Sydney_ _Morning_ _Herald,_ (June
24,2023),
[https://www.smh.com.au/world/north-america/no-direct-evidence-covid-started-in-wuhan-lab-us-intelligence-report-2](https://www.smh.com.au/world/north-america/no-direct-evidence-covid-started-in-wuhan-lab-us-intelligence-report-2)
0230624-p5dj4z.html

21 Clause 7 of the Bill.

22 Section 3.5, ACPDM.


-----

# DiGi

to serious harm'.23 The Bill’s treatment of ’misinformation1 and 'disinformation' therefore
requires digital platforms and the regulator to make 'crystal ball’ assessments concerning
whether instances of lawful but 'wrong' speech (whether or not these are likely to be
disseminated at scale on a platform) may ’contribute to’ harm in the distant future.

3.8. DIGI considered whether the bar for harm in the ACPDM should be lowered in its 2022
review of the code, by removing the requirement that misinformation or disinformation
pose an 'imminent and serious threat of harm' so that a broader scope of future 'chronic
harms' would be in scope. While recognising the need for a less strict approach to the
harm threshold, we concluded:

_The_ _difficulty_ _with_ _that_ _approach_ _is_ _that_ _it_ _is_ _extremely_ _difficult_ _for_ _platforms_ _to_
_foresee_ _when_ _an_ _accumulation_ _of_ _instances_ _of_ _misinformation_ _on_ _a_ _given_ _topic_ _is_
_likely_ _to_ _result_ _in_ _harm_ _that_ _may_ _be_ _years_ _or_ _decades_ _away._ _Further,_ _many_ _examples_
_of_ _chronic_ _harm_ _resulting_ _from_ _misinformation_ _and_ _disinformation_ _are_ _contentious._
_For_ _example,_ _lack_ _of_ _trust_ _by_ _citizens_ _in_ _democratic_ _institutions_ _is_ _a_ _complex_
_phenomena_ _which_ _cannot_ _readily_ _be_ _assigned_ _to_ _any_ _single_ _cause_ _or_ _point_ _in_ _time._

_On_ _the_ _other_ _hand,_ _platforms_ _recognise_ _that_ _an_ _accumulation_ _of_ _misinformation_
_can_ _pose_ _an_ _imminent_ _threat,_ _for_ _example,_ _to_ _public_ _health_ _during_ _a_ _pandemic_ _and_
_that_ _this_ _threat_ _can_ _be_ _ongoing_ _and_ _persistent_ _in_ _nature_ _that_ _needs_ _to_ _be_ _addressed_
_on_ _an_ _ongoing_ _basis24._

For these reasons, DIGI updated the harm threshold to cover credible threats of harms
but required that these should be foreseeable to meet the definitional threshold for
misinformation or disinformation. We think this approach should be replicated in the Bill
i.e the 'contributes to' serious harm element of content in scope of the Bill should be
removed.

3.9. _The_ _serious_ _harm_ _threshold_ _for_ _content_ _in_ _scope_ _of_ _the_ _Bill_ _should_ _be_ _better_ _defined._ DIGI
supports the requirement to have a serious harm threshold for content in scope of the
Bill. The factors that are relevant to the assessment of serious harm are listed in
sub-clause 7(3) of the Bill, however, these factors should not individually, or in
combination, be the sole determinants of whether content is in scope. The _Guidance_ _Note_
says that it is intended that a serious harm 'must have severe and wide-reaching impacts
on Australians'. DIGI recommends that serious harm must be defined as a harm that 'has
severe and wide-reaching impacts on the community which are reasonably foreseeable’
and includes consideration of the factors set out in sub-clause 7(3) of the Bill which
should be non-exclusive.25

23 Clause 7 of the Bill.

24 DIGI, _2022_ _Review_ _of_ _the_ _Australian_ _Code_ _of_ _Practice_ _on_ _Disinformation_ _and_ _Misinformation;_ _Discussion_ _paper,_ (June
2022).
25See section 3.4, ACPDM.


-----

# DiGi

3.10. _The_ _types_ _of_ _harms_ _in_ _scope_ _of_ _the_ _Bill_ _should_ _be_ _more_ _specifically_ _defined._ The list in
clause 2 of the types of harms in scope of the Bill are problematic as they are broad and
undefined and require highly subjective and therefore contestable judgements. In the
absence of a clear justification for the range of harms in scope of the Bill, we
recommended that the list of harms in scope of the Bill (clause 2) should be limited to
very specifically and objectively determinable harms, informed by evidence and
experience to date, rather than the hypothetical examples in the _Guidance_ _Note_ such as:

a. undermining the integrity of Australian democratic processes (such as elections
and referenda); and

b. endangering the health, safety or security of Australians.

3.11. _Harm_ _to_ _institutions_ _should_ _not_ _be_ _in_ _scope_ _of_ _the_ _Bill.The_ inclusion of institutions in the
list of harms associated with misinformation and disinformation as defined under the Bill
could be interpreted as discouraging public criticism of government institutions online,
when in fact such criticism is an important aspect of democratic discourse. The concept
of 'institutions' is undefined in the Bill and would arguably include institutions such as the
executive government, ministerial offices, government departments and agencies as well
as constitutional institutions such as 'responsible government', 'federalism' and even the
constitution itself. Furthermore, we are concerned that platforms and the ACMA could be
placed in an untenable position if they must adjudicate on whether trust in institutions
may potentially be harmed by online discourse, for example, based on complaints made
by the executive government of the day or public officials before or after an election.

3.12. In addition there is no clearly established nexus between misinformation and 'harm to the
integrity of institutions'. Clause 2 of the Bill could be interpreted to include diminished
public trust in particular government institutions. However, the trust of citizens in
government institutions is highly complex, constantly in flux and dependent on multiple
factors.26 There is a strong body of research that suggests that trust in government
primarily reflects public perceptions of governmental performance, rather than being
determined by the content that is disseminated on the world wide web27. In the Australian
context there is also evidence that, rather than moving inexorably downward, political
trust is cyclical28. For example, there is evidence to suggest that in Australia and New
Zealand public trust rose in 2021 because of citizens' confidence in the government's

26 See for example Lee Rainie, Scott Ketter and Andrew Perrin, Pew Research Center (22 July,1919), _Trust_ _and_ _distrust_
_in_ _America,_ [https://www.pewresearch.org/politics/2019/07/22/trust-and-distrust-in-america.](https://www.pewresearch.org/politics/2019/07/22/trust-and-distrust-in-america) Also for a more recent
Australian example see 'Scott Morrison's secret ministries undermining trust in government' _The_ _Guardian,_ August
17,2022).
[,https://www.thequardian.com/commentisfree/2022/auq/17/scott-morrisons-secret-ministries-undermine-public-trus](https://www.thequardian.com/commentisfree/2022/auq/17/scott-morrisons-secret-ministries-undermine-public-trus)
t-and-we-should-be-deeply-concerned,

[https://www.lowyinstitute.org/publications/morrison-s-secret-appointments-are-slippery-slope](https://www.lowyinstitute.org/publications/morrison-s-secret-appointments-are-slippery-slope) .

27 Lee Rainie, Scott Ketter and Andrew Perrin, Pew Research Center (22 July, 1919), _Trust_ _and_ _distrust_ _in_ _America,_
[https://www.pewresearch.org/politics/2019/07/22/trust-and-distrust-in-america.](https://www.pewresearch.org/politics/2019/07/22/trust-and-distrust-in-america)

28Clive Bean,'Party Politics, Political Leaders and Trust in Government in Australia' _Political_ _Science,_ Volume 53, Issue

1 (2002) Pages 17-27.


-----

# DiGi

management of the pandemic29. Factors such as institutional design and regulatory
settings are important factors in ensuring the public confidence in the decisions made by
certain types of institutions such as electoral bodies responsible for electoral
processes.30

3.13. We therefore recommend that harm to 'the integrity of Commonwealth, State, Territory or
local government institutions' should be removed from the list of harms in clause 2 of the
Bill. While DIGI considers that harm to the integrity of institutions should be out of scope
of the Bill, it is open to the government to regulate denigrating speech that can seriously
damage public confidence in the proper functioning of specific institutions on a case by
case basis. Australian laws currently prohibit 'scandalising contempt" that denigrates
judges or the court so as to undermine public confidence in the administration of
justice31. If, for example, the government considers that there is a need to protect the the
Australian Electoral Commision from denigrating commentary during electoral periods,
this could best be achieved by changes to the Electoral Act 1918 (Cth).

3.14. _Harm_ _to_ _the_ _economy_ _should_ _not_ _be_ _in_ _scope_ _of_ _the_ _Bill._ DIGI also recommends that harm
to the economy should be out of scope of the Bill as it is being addressed by the
government through consumer laws such as section 18 of the Australian Consumer Law
and policy interventions to target scams currently under development, including industry
codes.

3.15. _Harm_ _to_ _the_ _environment_ _should_ _not_ _be_ _a_ _separate_ _harm_ _in_ _scope_ _of_ _the_ _Bill._ The most
serious potential harms to the environment are adequately dealt with under 'harms to the
health, safety, or security of Australians'. We are concerned that the inclusion of
environmental harm implies that platforms should have a role in censoring important
scientific and public debate on issues around the rate, and impact of climate change.

3.16. _Hate_ _speech_ _should_ _not_ _be_ _treated_ _as_ _a_ _form_ _of_ _disinformation_ _or_ _misinformation_ _and_
_should_ _be_ _removed_ _from_ _the_ _scope_ _of_ _the_ _Bill._ The list of harms in section 2 of the Bill
included ‘hatred against a group in Australian society on the basis of ethnicity, nationality,

race, gender, sexual orientation, religion or physical or mental disability.’ Hate speech" is
already regulated via the OSA and BOSE as well as under a patchwork of State, Territory
and Commonwealth laws of limited scope.32 It's worth pointing out that hate speech is a

29 Shaun Goldfinch, Ross Taplin, Robin Gauld,'Trust in government increased during the Covid-19 pandemic in
Australia and New Zealand' _Australian_ _Journal_ _of_ _Public_ _Administration,_ Volume 80, Issue 1 (March 2021),
Pages1-160.

30 Sarah Birch, 'Electoral institutions and popular confidence in electoral processes: A cross-national analysis'
_Electoral Studies_ Volume 27, Issue 2, (June 2008), Pages 305-320

31 N 11, We note that scandalising contempt has been criticised in the United Kingdom as an untenable limitation on
freedom of expression. Committee on Contempt of Court (UK), Report of the Committee on Contempt of Court
(December 1974), Pages 69,162.

32 Note that the _Racial_ _Discrimination_ _Act_ 1975 (Cth), makes it unlawful for a person to insult, humiliate, offend or
intimidate another person or group in public on the basis of their race colour or national or ethnic origin. Victims of


-----

# DiGi

denial of the values of tolerance, inclusion, diversity and the very essence of human rights
norms and principles.33 The potential for hate speech to cause harm online is not related
to the extent the speech is true or false and so its inclusion in a Bill regulating false,
misleading or deceptive material is inappropriate. We also understand that the broader
question of how hate speech laws can be strengthened will be considered by the
government. In March 2023 the government presented its Response to the House of
Representatives Select Committee on Social Media and Online Safety report34, which
stated that the government would consider what more can be done to address group
hate speech online and would take a principled and evidence-based approach to ensure
the response is effective and supports members of our community who are targeted by
online hate35. This seems a sensible way forward. We acknowledge that hate speech has
been a politically charged field of regulation for the Australian government but that does
not justify the approach in this Bill which focuses exclusively on platforms but exempts
the perpetrators of hate speech from liability.

**Specific** **recommendations**

E. The Bill should be amended to be consistent with the Government's intent to make it
exceedingly clear that the ACMA’s powers do not extend to regulating individual instances of
misinformation or disinformation, for example, by being able to take-down any content, or
obtain access to non-public content, consistent with the statement of intention in the _Guidance_
_Note._

F. In order to ensure that the ACMA and digital platform services in scope of the Bill have a
common understanding about the content that is to be the subject of regulation, the
Government needs to reconsider the proposed scope of 'disinformation' and 'misinformation'
as defined under the Bill. To address this need we have proposed specific amendments in G to
0 of this submission.

G. The definitions of misinformation and disinformation in clause 7(1) and (2) of the Bill should be
amended to ensure that they apply to content that digital platforms can 'reasonably verify1 to be
false, misleading or deceptive.

H. _The_ _definition_ _of_ _disinformation_ _should_ _be_ _subject_ _to_ _an_ _objective_ _test,_ _which_ _enables_ _platforms_
_to_ _more_ _readily_ _determine_ _whether_ _content_ _is_ _disinformation_ _in_ _practice._ Specifically, sub-clause
7(2) of the Bill should be redefined to remove the requirement in sub-clause 7(2)(e) that the
person disseminating or causing the dissemination of the content must 'intend to deceive
another person’. This should be replaced with a requirement that the content be disseminated

online racial hatred within the scope of the Racial Discrimination Act can make complaints to the Human Rights and
Equal Opportunity Commission through a conciliation-based mechanism.

33 See Preamble of _United_ _Nations_ _Declaration_ _of_ _Human_ _Rights,_ and preamble _International_ _Convention_ _on_ _the_
_Elimination_ _of_ _All_ _Forms_ _of_ _Racial_ _Discrimination._
34 Presented on 15 March 2022

35 The Australian Government Response to the House of Representatives Select Committee on Social Media and
Online Safety Report, (March 2023), Page 5.


-----

# DiGi

by 'deceptive, manipulative or bulk, aggressive behaviours (which may be perpetrated via
automated systems) and includes behaviours which are intended to artificially influence users’
online conversations and/or to encourage users of digital platforms to propagate content1.

1. The ‘contributes to1 serious harm element of the definitions of misinformation and
disinformation in the Bill in sub-clauses 7(1)(d) and (2)(d) of the Bill should be removed as this
is a highly subjective test and impractical for platforms to apply.

J. _The_ _term_ _'serious_ _harm’_ _should_ _be_ _defined_ _as_ _a_ _harm_ _that_ _causes_ _'severe_ _and_ _wide-reaching_
_impacts_ _on_ _the_ _community_ _which_ _are_ _reasonably_ _foreseeable._ _We_ support the requirement to
have a 'serious harm' threshold for content in scope of the Bill. Clause 7(3) of the Bill lists the
factors that are relevant to the assessment of serious harm, however, these factors should not
individually, or in combination, be the sole determinants of whether content is in scope.

K. _The_ _types_ _of_ _harms_ _attributed_ _to_ _misinformation_ _and_ _disinformation_ _in_ _clause_ _2_ _of_ _the_ _Bill_ _should_
_be_ _limited_ _to_ _harms_ _that_ _can_ _be_ _objectively_ _assessed_ _by_ _providers_ _of_ _digital_ _platform_ _services_
_based_ _on_ _evidence_ _of_ _actual_ _harms_ _caused_ _by_ _the_ _dissemination_ _of_ _false,_ _misleading_ _or_
_deceptive_ _content_ _online,_ _rather_ _than_ _by_ _reference_ _to_ _the_ _hypothetical_ _examples_ _in_ _the_ _Guidance_
_Note._ For examples, this list might be restricted to:

a. undermining the integrity of Australian democratic processes (such as elections and
referenda).

b. endangering the health, safety, or security of Australians.

To further ensure the ACMA and regulated platforms have a common understanding of the
harmful content in scope of the Bill, the criteria for the types of content that falls in scope of
the definitions of misinformation and disinformation should be further explained in regulatory
guidelines.

L. Harm to the’ ‘integrity of Commonwealth, State, Territory or local government institutions
should be excluded from the lists of harms in scope of the Bill (clause 2), as it implies that
platforms should take measures to protect public institutions (including, for example, the
executive government and ministerial offices) from public criticism, when this is vital to robust
political debate.

M. Harm to the economy related to mis- and disinformation online being otherwise addressed by
the Government through competition and consumer laws, and initiatives concerning scams,
and should be out of scope of the Bill.

N. Harm to the environment does not need to be explicitly within the scope of the Bill, as the most
serious harms to the community that concern false, misleading or deceptive information that
could harm the environment are those which can endanger the health, safety, or security of
Australians and are covered by recommendation I.

O. DIGI supports strengthened regulation of existing State, Territory and Commonwealth laws
concerning hate speech via legislation of general application. Hate speech should not be
regulated as a type of misinformation or disinformation (as provided for in clause 2 (a) of the
Bill) because the harm it causes is not related to matters of truth or falsity; such an approach
would not be comprehensive in addressing harms related to hate speech.


-----

# DiGi

###### 4. Scope of services subject to the BiLL

4.1. **Key** **_Issue:_** **_The_** **_scope_** **_of_** **_services_** **_regulated_** **_by_** **_the_** **_Bill_** **_is_** **_unclear_** **_and_** **_all_** **_services_** **_in_** **_scope_**
**_are_** **_regulated_** **_in_** **_the_** **_same_** **_way_** **_regardless_** **_of_** **_their_** **_risk_** **_profile._** **_It_** **_is_** **_unclear_** **_how_** **_the_** **_Bill_**
**_applies_** **_to_** **_different_** **_types_** **_of_** **_online_** **_services._** **_The_** **_scope_** **_of_** **_the_** **_services_** **_should_** **_be_** **_clarified_**
**_and_** **_the_** **_powers_** **_of_** **_the_** **_ACMA_** **_to_** **_regulate_** **_individual_** **_online_** **_services_** **_should_** **_be_**
**_proportionate_** **_to_** **_a_** **_service's_** **_risk_** **_profile._**

4.2. _The_ _different_ _categories_ _of_ _services_ _set_ _out_ _in_ _the_ _Bill_ _are_ _difficult_ _to_ _apply_ _to_ _different_ _types_
_of_ _services_ _and_ _potentially_ _unworkable_ _if_ _rules,_ _codes_ _or_ _standards_ _concerning_
_misinformation_ _and_ _disinformation_ _are_ _developed_ _using_ _these_ _terms._ The powers of the
ACMA to make rules, codes and standards in the Bill apply to different sections of the
industry and categories of services:

a. digital platform providers who provide content aggregation services;

b. digital platform providers who provide connective media services; and
c. digital platform providers who provide media sharing services36.

These sections of the industry and categories of services in scope of the Bill are very
broadly defined.The 'tiered approach1 to defining the services in scope of this Bill has the
potential to extend this regulatory scheme to an extremely broad range of services (in the
interests of future proofing the regulation of misinformation and disinformation) and
leaves considerable discretion to the ACMA to determine which services/service
categories will eventually be subject to codes/standards.

4.3. _Low_ _risk_ _services_ _should_ _not_ _be_ _regulated_ _under_ _the_ _Bill._ As a general rule the categories of
services currently in scope of the Bill (clauses 4, sub-clause 30 (2)) should be limited to
those types of services that host information at the user's request and disseminate it to
the public i.e making it available to a potentially unlimited number of persons at the user's
request, in line with the approach in the EU under the Digital Services Act. In order to
avoid imposing broad obligations on a range of low risk platforms, the Bill should make
this a baseline threshold for a service to be in scope of the regulatory scheme. Given that
the focus of the Bill should be on digital platform services that are at risk of being the
target of disinformation campaigns, we also recommend that the risk of
virality/widespread ease of distribution of disinformation via public means on platforms
with a large user base should be a key indicator of services in scope of the Bill. These
requirements are particularly important if the broad categories of services are retained.
Our comments below regarding the sub-categorisation of in-scope service should be read

in light of this overarching recommendation.

4.4. We also recommend that the Bill be amended to differentiate between more specific
subsets of each of the currently proposed categories of services so that appropriate

36 clause 4 and sub-clause 30(2) of the Bill.


-----

# DiGi

codes/standards can be more readily developed for 'like' services. The development of
useful sub-categories should be informed by industry's recent experience of developing
industry codes under the OSA in which industry was more readily able to develop codes
for the better defined and analogous category of services such as social media services
and ISPs compared to the broader and more diverse categories of services such as
'designated internet services' which covered diverse services such as file storage
services and most apps and websites.

4.5. _The_ _basic_ _test_ _for_ _assessing_ _whether_ _a_ _service_ _belongs_ _to_ _a_ _section_ _of_ _the_ _industry_ _is_
_confusing._ This is of concern as it is possible that an individual online service may in
future be covered by several different, and potentially conflicting, codes and standards as
the sections of the industry set out in clause 30(2) of the Bill are specifically not 'mutually
exclusive' (sub-clause 30(7)). Further, the Bill distinguishes between the different
categories of services based on a 'a primary function1 test that confusingly assumes a
service may have one or more 'primary functions.'37 This test does not provide platforms
with enough clarity as to the section of the industry to which they belong. We recommend
that the 'a primary function test' should be replaced with a test that is based on The
primary function1 of the service.

4.6. _The_ _definition_ _of_ _content_ _aggregation_ _services_ _in_ _sub-clause_ _4_ _(2)_ _of_ _the_ _Bill_ _is_ _too_ _broad_
_and_ _unclear._ _A_ digital service is a content aggregation service if 'a primary function of the
digital service is to collate and present to end-users content from a range of online
sources, including sources other than the digital service'. This definition potentially
captures all websites, software and apps that provide online content. This definition
should be specifically targeted at digital services that have The primary function of
creating a collection of content that the provider has sourced from multiple locations on
the world wide web web to enable users of the service to view the collection of content
on the service in an aggregated format'. Further, services should be excluded from the
scope of the Bill where the primary function of the service is to create collections of
excluded content such as professional news. We think this approach is consistent with
the intent expressed in the _Guidance_ _Note._

4.7. _Internet_ _search_ _engine_ _services_ _should_ _form_ _a_ _specific_ _stand_ _alone_ _category_ _of_ _regulated_
_service._ In addition, it is unclear whether search engine services are in scope of the
definition of content aggregation services, although we understand from the _Guidance_
_Note_ that is the intention of the Government that they be so38. We would recommend that
search engine services should be a separate category of services as they have a primary
function of enabling users to conduct searches for information utilising an index of
materials on the world wide web which is different to what the industry typically
understands to be the function of a content aggregation service. We recommend that an

37 sub-clause 4(2)(a), 4(3)(a) and 4(4)(a) of the Bill.

_38_ _Guidance_ _note,9._


-----

internet search engine service is defined as an electronic service that satisfies all the
following conditions:

a. the service is designed to collect, organise (index) and/or rank material on the
world wide web;

b. the service has the primary function of enabling end-users to search the service's
index of material on the world wide web for relevant results in response to the
end-user's queries; and

c. the service returns search results in response to end-user queries.

4.8. _The_ _definition_ _of_ _connective_ _media_ _services_ _in_ _sub-clause_ _4(3)_ _requires_ _clarification_ _and_
_encompasses_ _low_ _risk_ _services._ A digital service is a connective media service if:

a. a primary function of the digital service is to enable online interaction between
two or more end-users;

b. the digital service allows end-users to link to, or interact with, some or all of the
other end-users; and

c. the digital service has an interactive feature.

This is a broad definition and may capture many digital services that pose a low risk of
widespread and viral dissemination of misinformation and disinformation. For example,
this definition captures all websites and apps with a comment or forum feature even if
the website or app is for an enterprise, public service, or specialised purpose (e.g.,
academic research, customer service etc.). There is also potential duplication and lack of
clarity between the three limbs of this definition. We recommend that, at a minimum, the
definition be amended so that it only captures services where the primary function is to
enable online interaction between end-users via an interactive feature.

4.9. _The_ _definition_ _of_ _media_ _sharing_ _services_ _in_ _sub-clause_ _4(4)_ _is_ _overly_ _complex_ _and_
_encompasses_ _low_ _risk_ _services._ _A_ digital service satisfies the definition of a media
sharing service if 'a primary function of the digital service is to provide audio, audio-visual
or moving visual content to end-users'. DIGI recommends that the definition of media
sharing service is amended so that it only covers services that have an interactive
feature. In other words, the carve out from sub-clause 6(1 )(b) should be incorporated into
the definition itself, in order to align with the approach taken for connective media
services.This broad definition and may also unintentionally capture services that have a
low risk of widespread and viral dissemination of misinformation and disinformation. For
example, a media sharing service’s primary function may not be interactive, but there
could be a limited function that would satisfy this definition (such as a subscription video
on demand streaming service with a chat functionality or the ability to leave reviews). See
our general recommendation in para 4.4 above regarding the need to exclude low risk
services from the scope of the regulatory scheme.

4.10. _Exemptions_ _for_ _professional_ _news_ services. The Bill excludes professional news content
from the definition of misinformation in clause 2 .This in effect exempts providers of


-----

# DiGi

professional news services from being subject to regulation concerning misinformation.
These services are already subject to regulation under a range of other industry codes or
standards. As researchers from the University of Technology Sydney (UTS) have
observed, the regulatory framework for news media is 'highly fragmented.' In December
2018, research by UTS identified 14 different codes that oversee Australia's journalists
and news companies: seven codes for broadcasters; four codes for narrowcasters; the
codes of the APC and the Independent MediaCouncil; and the MEAA Journalist Code of
Ethics.39

4.11. There is evidence that news outlets in Australia have contributed to amplifying
misinformation into online spaces, and led to harmful assumptions which can open the
way for agents of disinformation to use professional news content to push their own
agenda40. In 2021 the Australian Senate committee handed down its report following a
year-long inquiry into media diversity, and concluded that the current regulatory
arrangements were deficient and recommended the establishment of a judicial inquiry,
with the powers of a royal commission, into media diversity, ownership and regulation:

_Media_ _convergence_ _due_ _to_ _technological_ _change_ _has_ _greatly_ _strengthened_ _the_
_argument_ _in_ _favour_ _of_ _a_ _single_ _regulator_ _across_ _all_ _platforms._ _As_ _a_ _consequence,_
_the_ _committee_ _further_ _recommends_ _that_ _the_ _judicial_ _inquiry's_ _terms_ _of_ _reference_
_include_ _consideration_ _of_ _a_ _single,_ _independent_ _media_ _regulator_ _to_ _harmonise_ _news_
_media_ _standards_ _and_ _oversee_ _an_ _effective_ _process_ _for_ _remedying_ _complaints41._

DIGI recommends that the Government consider how the existing self and co-regulatory
frameworks for professional news can be updated to address mis- and disinformation in

a manner that is equivalent to the proposed regulation of digital platforms under the Bill's
proposed regulatory scheme.42

4.12. _Exemptions_ _for_ serv/ces.The concept of 'excluded content' as compared to 'excluded
services' and the interaction of the two definitions in the Bill should also be clarified. If a
service is limited to providing excluded content the service should be treated as an
excluded service and be out of scope of the Bill.

39 Wilding, Derek, Fray, Peter, Molitorisz, Sacha and McKewon, Elaine, _The_ _Impact_ _of_ _Digital_ _Platforms_ _on_ _News_ _and_
_Journalistic_ _Content,_ University of Technology Sydney, NSW 2018.

40 See for example* Carlotta Dotto and Jack Berkefeld, 'From coronavirus to bushfires, misleading maps are
distorting reality' _First_ _Draft_ _News,_ (February 2020,)
[News.https://firstdraftnews.org/latest/from-coronavirus-to-bushfiresmisleading-maps-are-distorting-reality/.](news://News.https://firstdraftnews.org/latest/from-coronavirus-to-bushfiresmisleading-maps-are-distorting-reality/)

41 Parliament of Australia, _Media_ _Diversity_ _in_ _Australia:_ _Executive_ _Summary.'_ (2021).
[https://www.aph.gov.au/Parliamentary_Business/Committees/Senate/Environment_and_Communications/Mediadiv](https://www.aph.gov.au/Parliamentary_Business/Committees/Senate/Environment_and_Communications/Mediadiv)
ersity/Report.

42 Some researchers have concluded that journalists from mainstream news outlets played major roles in promoting
disinformation, in the 2016 US election advancing partisan interests. See Benkler, Y, R. Faris, and H. Roberts. _Network_
_Propaganda:_ _Manipulation,_ _Disinformation,_ _and_ _Radicalization_ _in_ _American_ _Politics._ (Oxford University Press, 2018).


-----

# DiGi

4.13. _Treatment_ _of_ _Private_ _communications._ As discussed in paragraph 4.3 above above, DIGI
recommends that the services in scope of the Bill should be limited to those types of
services that host information at the user's request and disseminate it to the public ie
making it available to a potentially unlimited number of persons at the user's request, and
to services where there is a high risk of content becoming viral. Correspondingly most
forms of private communications services that allow communications between a finite
number of users should be out of scope of the Bill. The intention of the current drafting of
the Bill in relation to private communications is very unclear e.g., the _Fact_ _Sheet_ indicates
that the Bill is intended to exclude the ACMA from being able to access the content of
private messaging, and will not require breaking of encryption. However, the way this is
drafted and its potential impact on potential code obligations of providers is unclear; see
sub-clause 34 which states that a code must not have a requirement that "relates to" the
content of private messages or their encryption. Yet, all code measures arguably relate in
some way to content so it is unclear how this is intended to be applied. We suggest that
private messaging services akin to SMS or MMS or that do not have the scope of wide
scale potential distribution (i.e. limited numbers of messaging recipients) to be a target
of disinformation campaigns, should be clearly excluded from the Bill.

**Specific** **recommendations**

P. The "tiered approach" to defining the services in scope of this Bill has the potential to extend
this regulatory scheme to an extremely broad range of services and leaves considerable
discretion to the ACMA to determine which services/service categories will eventually be
subject to codes/standards. As _a_ _matter_ _of_ _principle,_ _we_ _think_ _that_ _the_ _scope_ _of_ _services_ _subject_
_to_ _the_ _Bill_ _should_ _be_ _clearly_ _dealt_ _with_ _by_ _parliament_ _via_ _the_ _provisions_ _of_ _the_ _Bill._ _To_ _the_ _extent_
_decisions_ _about_ _the_ _services_ _subject_ _to_ _regulation_ _are_ _delegated_ _to_ _the_ _ACMA_ _the_ _Bill_ _should_ _set_
_out_ _the_ _relevant_ _criteria_ _that_ _will_ _be_ _used_ _to_ _make_ _such_ _decisions._

Q. To ensure low risk services are excluded from the regulatory scheme, the categories of
services currently in scope of the Bill should be limited to those types of services that:

a. host content at the user's request and disseminate such content to the public i.e
making the content available to a potentially unlimited number of persons at the user's
request

b. enable widespread ease of distribution (virality) of content via public means.

R. The Bill be amended to create more specific proposed categories of services than the broad
categories listed in sub-clause 30(2) so that codes/standards can be more readily developed
for ’like’ services.

S. The definition of 'content aggregation services' in scope of the Bill (sub-clause 4 (2)) should be
amended so that it relates to services which have ‘the primary function of creating a collection
of content (excluding exempted content such as professional news) that the provider has
sourced from multiple locations on the world wide web to enable users of the service to view
the collection of content on the service in an aggregated format’.


-----

# DiGi

T. If it is intended that search engines are in scope of the Bill, search engines should be treated as
a separate service type as they have different primary function to content aggregation services.
We recommend that an internet search engine service is defined as an electronic service that
satisfies all the following conditions:

a. the service is designed to collect, organise (index) and/or rank material on the world
wide web;

b. the service has the primary function of enabling end-users to search the service's index
of material on the world wide web or relevant results in response to the end-user's
queries; and

c. the service returns search results in response to end-user queries.

U. The definition of 'connective media services' in scope of the Bill (sub-clause 4(3)) should be
amended so that it only captures services where the primary function of the service is to
enable online interaction between end-users via an interactive feature.

V. The definition of 'media sharing services' in scope of the Bill (sub-clause 4 (4)) should be
amended so that only services that have an interactive feature fall within this category.

W. The Government should consider if the current self regulatory and co-regulatory codes that
apply to providers of professional news online should be updated to address misinformation
and disinformation.

X. The concept of ‘excluded content’ in clause 2 of the Bill as compared to ‘excluded services’ and
the interaction of the two definitions should also be clarified. If a service is limited to providing
excluded content the service should be treated as an excluded service and be out of scope of
the Bill.

Y. The scope of the exemptions for private communications services should be clarified so that
the Bill excludes all types of services that enable communication between a finite group of end
users similar to the approach in the EU under the Digital Services Act. For example,
communication services that are analogous to SMS and MMS services should be excluded
from the scope of the Bill.

###### 5. The exercise of the ACMA’s regulatory powers

5.1. **_Key_** **_issue:_** **_There_** **_should_** **_be_** **_better_** **_incentives_** **_in_** **_the_** **_Bill_** **_to_** **_encourage_** **_a_** **_graduated_**

**_approach_** **_of_** **_self-regulation,_** **_co-regulation,_** **_and_** **_standards_** **_as_** **_set_** **_out_** **_in_** **_the_** **_Guidance_** **_Note_**
**_and_** **_clear_** **_articulation_** **_of_** **_the_** **_thresholds_** **_for_** **_the_** **_exercise_** **_of_** **_the_** **_ACMA's_** **_powers43._**

43 See _Guidance_ _note,_ 6.


-----

# DiGi

5.2. There are various advantages and disadvantages of different regulator mechanisms for
regulating disinformation and misinformation. A key benefit of a self-regulatory code
such as the ACPDM is that it can be updated to address advances in threats and
technology faster than legislation or a co-regulatory code. A self regulatory code requires
constant efforts by signatories to meet their commitments, and to adapt their responses
to emerging threats. It incentivises proactive, rather than reactive action in a rapidly
changing environment. In contrast, the process for amending co-regulatory mechanisms
and legislation is slower and more cumbersome which may make these mechanisms
insufficiently flexible to adapt to the rapid environmental changes that characterise the
digital information ecosystem.

5.3. _DIGI_ _considers_ _that_ _it_ _is_ _important_ _the_ _ACMA_ _does_ _not_ _take_ _pre-emptive_ _action_ _to_ _require_
_codes_ _and_ _standards_ _under_ _the_ _Bill,_ _given_ _the_ _substantial_ _progress_ _made_ _by_ _the_ _signatories_
_oftheACPMD_ _to_ date. The DIGI-administered ACPDM is just over two years old, and in our
view, industry has met its commitment to strengthen its measures and improve its
approach to providing transparency concerning its efforts on an ongoing basis. In
October 2021, DIGI put in place governance arrangements to strengthen the ACPDM and
its effectiveness. DIGI appointed an independent complaints committee to resolve
complaints about possible breaches by signatories of their code commitments, and
created a portal on DIGI's website for the public to raise such complaints. We appointed
an independent reviewer to fact check and attest all signatories' transparency reports
prior to publication, who also developed best practice reporting guidelines to drive
improvements and consistency in 2022 transparency reports.44 In its June 2021 _Report_ _to_
_government_ _on_ _the_ _adequacy_ _of_ _digital_ _platforms_ _disinformation_ _and_ _news_ _quality_
_measures45_ (ACMA Report to Government), released in March 2022, the ACMA reviewed
the ACPDM, finding that 'the code objectives and principles meet the government
objective of striking a balance between encouraging platform interventions and
protecting freedom of expression, privacy and other rights.

5.4. In June 2022, DIGI commenced a review of the code, which included close consideration
of the ACMA's recommendations as well as submissions received as part of a public
consultation process.46 Some of the key changes DIGI made as part of the 2022 review of
the code included:

a. encouraging greater participation in the code by smaller digital platforms,
including by modifying the transparency reporting requirements for services with
less than one million active monthly users in Australia.This acknowledged the
likelihood for misleading content to proliferate elsewhere online as mainstream
platforms strengthened approaches to tackling misleading content;

44 DIGI (2023), _Governance,_ [https://digi.org.au/disinformation-code/governance/](https://digi.org.au/disinformation-code/governance/)

45 ACMA (21/03/2022), _Report to government_ _on_ _the_ _adequacy_ _of_ _digital_ _platforms’_ _disinformation_ _and_ _news_ _quality_
_measures,_
[https://www.acma.gov.au/report-government-adequacy-digital-platforms-disinformation-and-news-quality-measures](https://www.acma.gov.au/report-government-adequacy-digital-platforms-disinformation-and-news-quality-measures)

46 DIGI (2022), _Code_ _Review,_ [https://diqi.orq.au/disinformation-code/code-review/](https://diqi.orq.au/disinformation-code/code-review/)


-----

# DiGi

b. an updated definition of 'harm' in relation to mis and disinformation, addressing
stakeholders' concerns that the threshold of 'serious and imminent' threat of
harm was too high. The new threshold is 'serious and credible' threat of harm.

c. additional commitments reflecting updates to the strengthened EU Code of
Practice in relation to recommender systems, and deterring advertisers from
repeatedly placing digital advertisements that propagate mis- and disinformation;

d. there are also updates to further clarify that both sponsored content and paid for
advertising are in scope of relevant commitments on demonetising mis- and
disinformation;

e. retaining the pre-existing exclusion of professional news content from being
treated as misinformation under the code, and the pre-existing obligation for
signatories to address this content when it is being disseminated as
disinformation; and

f. requiring greater transparency around the specific products and services that are
within scope of the signatories' code commitments, through updates to the code,
transparency reporting requirements and the DIGI website.

DIGI is currently working with signatories to the code to consider the assessment and
recommendations made by the ACMA in its second report to the government concerning
digital platforms' efforts under the ACPDM.47 48

5.5. The _Guidance_ _Note_ says that the Bill provides a 'graduated set of powers that allows the
ACMA to act if voluntary efforts are inadequate481. As drafted, the statement of
parliamentary intention in clause 32 is that _industry_ _should_ _develop_ _one_ _or_ _more_ _codes_
_(misinformation_ _codes)_ _that_ _require_ _participants_ _in_ _those_ _sections_ _of_ _the_ _digital_ _platform_
_industry_ _to_ _implement_ _measures_ _to_ _prevent_ _or_ _respond_ _to_ _misinformation_ _and_
_disinformation_ _on_ _digital_ _platform_ _services._ This creates considerable uncertainty
regarding the status of the ACPDM i.e. whether DIGI should apply to the ACMA for the
current code to be registered or if it is intended that it be replaced. This should be
addressed in clause 32.

5.6. The Bill as currently drafted would not prevent the ACMA from bypassing both
self-regulatory codes and industry codes in order to make a standard under Division 5 of
the Bill. The powers granted to the ACMA to make standards in clause 50 of the Bill for
'exceptional and urgent circumstances' should be removed because of the risk that they
may be exercised to incentivise platforms to censor public communications about
sensitive issues at times when open debate is critical. At a very minimum, there should
be a clear definition of what types of circumstances constitute 'exceptional and urgent
circumstances' (which as drafted is subjective and open to multiple interpretations).
There should also be a sunset clause in the Bill that limits the time period for which such
standards can be in force to the period in which the exceptional and urgent
circumstances are extant.

47 ACMA, _Digital_ _platforms’_ _efforts_ _under_ _the_ _Australian_ _Code_ _of_ _Practice_ _on_ _Disinformation_ _and_ _Misinformation_ _Second_
_report_ _to_ _government_ (July 2023).

48 Ibid


-----

# DiGi

5.7. _The_ _Bill_ _fails_ _to_ _articulate_ _clear_ _preconditions_ _that_ _must_ _be_ _satisfied_ _before_ _the_ _ACMA_ _can_
_exercise_ _its_ _code_ _and_ _standard_ _making_ _powers._ The _Guidance_ _Note_ provides that 'Where
voluntary efforts provide inadequate protection and the ACMA is satisfied that it is
necessary to address systemic issues in relation to misinformation or disinformation on
digital platform services, the ACMA will be able to request the industry make a new
code.49 However, this is not reflected in sub-clause 38(3)(a) which lists the bases upon
which the ACMA can request codes of a section of the industry including where it deems
the development of the code is _necessary_ _or_ _convenient_ in order to prevent or respond to
misinformation or disinformation.

5.8. We recommend that sub-clause 38(3) should be amended so that the ACMA can only
request a code where it has determined that an industry code is needed to address
_systemic_ _failures_ by a section of the industry to implement adequate measures to protect
the community relation to _disinformation_ (consistent with our Recommendation C that
the ACMA's powers to request codes/make standards should be limited to
disinformation).50 Similarly, platforms within a section of the industry that are subject to a
registered industry code should not be subject to a standard unless ACMA has
determined that a standard is needed to address _systemic_ _breaches_ by those platforms
concerning the implementation of measures under a code in relation to disinformation51.

**Specific** **recommendations**

Z. The statement of parliamentary intent in clause 32 should be amended to make clear whether
parliament intends that the signatories to the ACPDM should request that it be registered or
that it should be replaced by a new code.

AA. The powers to make standards in section 50 of the Bill for 'exceptional and urgent
circumstances' should be removed, or at least 'exceptional and urgent' circumstances should
be defined clearly and any standards made in these circumstances subject to a sunset clause
so that the standards do not remain in place longer than necessary .

BB. Consistent with the _Guidance_ _Note,_ the ACMA should only be empowered to request an
industry code where it has determined that the code is needed to address systemic failures of
a section of the industry to implement adequate measures to protect the community in relation
to _disinformation_ (consistent with our recommendation in C that the ACMA's code and
standards making powers should be limited to disinformation).

CC. Platforms within a section of the industry subject to a registered industry code should not be
subject to a standard unless the ACMA has determined that a standard is needed to address

_49_ _Guidance_ _Note,5._

50 Note: the governance arrangements for the ACPDM establish an independent complaints committee that has
power to determine if a signatory has committed material breaches of the codes and systemic breaches.

51 clause 48 of the Bill provides that the ACMA's power to make a standard is contingent on a code being assessed to
be _totally_ _deficient_ in that it is not operating to adequately safeguard the community from misinformation or
disinformation, on the services. We suggest this be amended so this is intended to apply to a systemic failure of the
industry participants regulated by the code to implement the measures required under the code.


-----

# DiGi

systemic breaches by those platforms concerning the implementation of measures under a
code.

DD. DIGI requests further clarity about how the ACMA's powers to make standards for services in
particular sections of the industry will apply where a service in that section is an ACPDM
signatory and is complying with its commitments.

##### 6. Powers of the ACMA to obtain information, request codes and
#### make standards

6.1. **_Key_** **_Issue:_** **_In_** **_order_** **_to_** **_strengthen_** **_protections_** **_for_** **_free_** **_speech,_** **_the_** **_powers_** **_of_** **_the_** **_ACMA_** **_to_**
**_obtain_** **_records_** **_and_** **_other_** **_information_** **_and_** **_require_** **_codes_** **_and_** **_make_** **_standards_** **_concerning_**
**_misinformation_** **_and_** **_disinformation_** **_must_** **_be_** **_tied_** **_to_** **_a_** **_dear_** **_public_** **_purpose._**

6.2. The Bill gives the ACMA broad and open-ended power under Part 2, Division 2 to make
rules that require digital platforms to keep and supply records and under Part 2, Division 3
to provide information about various matters including:

a. misinformation or disinformation on a service; measures implemented by the
provider to prevent or respond to misinformation or disinformation on the service
(including the effectiveness of the measures); and

b. the prevalence of content containing false, misleading or deceptive information
provided on the service.

6.3. _The_ _information_ _gathering_ _powers_ _of_ _the_ _ACMA_ _are_ _too_ _broad_ _and_ _onerous.The_ Bill gives
the ACMA broad powers to make rules that require record keeping and to obtain records
and information from platforms (clauses 14, and 18) that enable the ACMA to initiate
frequent and onerous requests for information, simply to satisfy an interest in learning
more about the subject of mis and disinformation online. We consider these powers
should only be exercised for a clearly defined public purpose, proportionate to the
different level of harms posed by misinformation and disinformation. We suggest that
this be for the purpose of determining if:

a. providers of platforms have implemented measures (systems and processes)
that are adequate to protect the community from disinformation and
misinformation on the service;

b. enabling the ACMA to publish information about the extent platforms have
implemented measures (systems and processes) that are adequate to protect
the community from disinformation and misinformation; and

c. In the case of disinformation, enabling the ACMA to determine if there is a need
for industry codes or standards.


-----

6.4. We further recommend that types of records and information that the ACMA may require
be further specified in a way that is proportionate to the different level of harms posed by
misinformation and disinformation. For example, in relation to disinformation the ACMA's
powers could be limited to requiring platforms to keep records and provide information
concerning the type and range of measures that digital platforms have in place to protect
the community from disinformation and information about the impact of those
measures.

6.5. We recommend that the cadence for demands by the ACMA on digital platforms for
records, the time period for platforms to respond and the period for which records must
be retained should be set out in the legislation.

6.6. _Platforms_ _should_ _not_ _be_ _required_ _to_ _collect_ _and_ _provide_ _information_ _about_ _false,_ _misleading_
_or_ _deceptive_ _content_ _on_ _their_ _services._ Clause 14(1)(e) and clause 18(2)(c)of the Bill give
the ACMA broad powers to obtain records and collect information from an digital
platforms concerning the prevalence of content containing false, misleading or deceptive
information on the service (other than excluded content for misinformation purposes) i.e.
any 'untrue content' on a service, regardless of whether it is harmful. It is highly
problematic to give the ACMA powers that in effect require platforms to monitor the
truthfulness of all the content that is provided on a platform. DIGI recommends that the
references to the prevalence of false, misleading or deceptive content clause 14(1 )(e)
and clause 18(2)(c) should be replaced with the 'prevalence of misinformation and
disinformation'.

6.7. Currently clauses 18 and 19 of the Bill enable the ACMA to obtain any type of information
about 'dis and misinformation' on a digital platform from individuals and digital
platforms. We recommend that the Bill be amended to provide stronger safeguards in
Part 2, Division 3 of the Bill for requests by the ACMA for information that is confidential
or commercially sensitive. For example, there should be a process whereby a platform
can identify that certain information is not publicly disclosed.

6.8. _Powers_ _to_ _gather_ _information_ _from_ _any_ _individual_ _are_ _excessive._ The information gathering
powers afforded to the ACMA in clause 19 of the Bill extend to any Australian that may
have information about mis- or disinformation on a service. These powers apply to
individuals that have a key role in strengthening the information ecosystem: professional
researchers, fact checkers, free-lance journalists, authors, and expert contributors to
online publications. The first category are key partners in the fight against misinformation
and disinformation under the ACPDM. DIGI is concerned that targeting these
stakeholders may have an adverse impact on cooperation between platforms and these
individuals, which we consider to be contrary to the aims of ACPDM. The latter category,
alongside mainstream news, also plays a critical role in increasing the diversity of
information sources in the Australian market. There is no clear rationale to subject an
unlimited category of individuals to these powers. To the extent individuals fall within the
scope of sections of the industry, they should be subject to the powers of the ACMA


-----

# DiGi

concerning the relevant industry section. For example, a podcaster might be required to
keep records concerning misinformation or based on risk indicators published in
regulations, and may also be subject to a code or standard concerning disinformation.
We therefore recommend that clause 19 be deleted.

6.9. The content of codes and standards should also be clearly limited to the purpose of
ensuring platforms in scope of the legislation have measures in place that provide
appropriate protections to the community from disinformation on their services. As
stated in recommendation B and C, we consider that, at this time, misinformation should
not be in scope of the powers to make codes and standards. The scope of codes and
standards for disinformation should be further confined to a specific list of matters such
as measures to reduce the spread of disinformation and not open-ended as currently
drafted. Specifying the subject- matter of codes and standards will further safeguard
legitimate freedom of expression.

6.10. We also recommend that the Government give further consideration to how the ACMA
will determine that a code or standard is required. For example, we have recommended
that under clause 38(a) the powers of the ACMA should be conditional on a
determination that there has been a systemic failure by a section of the industry to
implement adequate measures (systems and processes) to protect the community.
Similarly, we have recommended that platforms that are subject to a registered industry
code should not be subject to standard unless ACMA has determined that a standard is
needed to address systemic breaches concerning the implementation of measures
(systems and processes) under a code in relation to disinformation. The Bill should
define 'systematic' failures or breaches or at least the considerations that the ACMA
must take into account in determining whether there are systemic issues, for example,
the frequency and severity of breaches over a given time-period.

**Specific** **recommendations**

EE. The power of the ACMA to make rules that require platforms to keep/provide records and the
power to obtain additional information should be limited to the purpose of:

a. determining if providers of platforms have implemented measures (systems and
processes) that are adequate to protect the community from disinformation and
misinformation on the service;

b. enabling the ACMA to publish information about the extent platforms have
implemented measures that are adequate to protect the community from
disinformation and misinformation; and

c. in the case of disinformation, enabling the ACMA to determine if there is a need for
industry codes or standards.


-----

# DiGi

FF. The scope of the ACMA's powers in relation to requiring and obtaining records and information
should be further limited to specific types of records and information, proportionate to the
different level of harm posed by misinformation and disinformation.

GG. The cadence for demands by the ACMA on digital platforms for records, the time period for
platforms to respond, and the period for which records must be retained should be set out in
the legislation.

HH.The references to the prevalence of false, misleading or deceptive content in sub-clause
14(1 )(e) and sub-clause 18(2)(c) should be replaced with the 'trends relating to misinformation
and disinformation1. The scope of these clauses, as currently drafted, is highly problematic as
in effect gives ACMA power to require platforms to monitor the truthfulness of all the content
that is provided on a platform. In our view, this would be impossible for platforms to
implement.

II. The open-ended powers of the ACMA to obtain additional information about misinformation
and disinformation from individuals in clause 19 of the Bill are excessive and should be
removed.

JJ. The content of codes and standards should be clearly limited to the purpose of ensuring
services that are at risk of disseminating disinformation (in accordance with criteria set out in
the legislation) have measures in place that provide appropriate protections for the community
against disinformation on their services. As recommended in Q, the risk of virality/widespread
ease of distribution of disinformation via public means should be a key marker of a service in
scope of the Bill.

KK. The Government should give further consideration to specifying the criteria for 'systemic1
failures, breaches or issues (as the case may be) that will trigger an exercise of the ACMA's
powers to make a code or standard.

###### 7. The process for making codes and standards

7.1. **_Key_** **_issue:_** **_The_** **_digital_** **_platforms_** **_in_** **_scope_** **_of_** **_this_** **_regulation_** **_are_** **_very_** **_diverse._** **_Therefore,_**

**_code_** **_making_** **_across_** **_an_** **_online_** **_industry_** **_section_** **_as_** **_defined_** **_in_** **_the_** **_Bill_** **_will_** **_be_** **_complex,_** **_time_**
**_consuming_** **_and_** **_will_** **_also_** **_require_** **_extensive_** **_collaboration_** **_amongst_** **_industry_**
**_participants.The_** **_Bill_** **_needs_** **_to_** **_ensure_** **_that_** **_the_** **_process_** **_for_** **_making_** **_codes_** **_and_** **_standards_** **_is_**
**_fair_** **_and_** **_allows_** **_sufficient_** **_time_** **_for_** **_code_** **_and_** **_standards_** **_development._**

7.2. The need for adequate time to make codes for the online industry sections is reinforced

by DIGI's experience co-leading the development of eight industry codes under the Online
Safety Act 2021 (Cth) which has taken more than 20 months since regulatory guidance
was first released. As discussed in section 5 of this submission, the Bill should be
amended to reflect the graduated approach to the exercise of the ACMAs powers and the


-----

# DiGi

relationship between those powers i.e. codes and standards should in accordance with
the intention in _Guidance_ _Note_ only be made where existing regulatory mechanisms have
failed, informed by evidence.

7.3. The timelines in the Bill for making codes and standards are much too short to achieve
appropriate and effective regulation in such a challenging policy space. It is not realistic
to expect an industry section to make a code within 120 days.52 Given the broad and very
diverse sections of the industry that may be requested to make codes, we suggest that a
minimum of 12 -24 months would be required for industry associations to develop a
code, depending on the size of the industry section in scope and the number of codes
under development at any given time. Drawing from DIGI's direct experience with code
development for the digital industry, allowing more time allows for public consultation
and other meaningful stakeholder engagement that yields better outcomes.

7.4. Further we consider that the 180 day timeline in which the ACMA can make a
determination that a code has failed/partially failed under sub-clause 48(1 ) and 49(1) as

a precondition to making a standard is insufficient and should be extended to at least
12-24 months.

7.5. The ACMA should be required to undertake public consultation prior to making an
industry standard. This is consistent with the approach taken under the OSA (section
148) and the Telecommunications Act 1997 (section 132), which requires the eSafety
Commission and the ACMA, respectively, to conduct public consultation of at least 30
days prior to making an industry standard. Given the implications for Australians freedom
to communicate online and the substantial penalties for non-compliance, it is important
that stakeholders have an opportunity to comment on a proposed standard. It is also not
clear why industry associations developing an industry code should be required to
publicly consult on a draft code (sub clause 37(1 )(f)) while the ACMA is only required to
consult with respective industry associations, and therefore their codes would not be
subject to public consultation. This is critical given the public interest in this Bill.

**Specific** **recommendations**

LL. The Bill should provide for a minimum period of 12 -24 months for industry associations to
develop a new code, depending on the size of the industry section in scope and the number of
codes that the ACMA requests be developed concurrently.

MM. A minimum of 12-24 months should be allowed for a code to be in force before ACMA
determines that a code has failed/partially failed under sub-clause 48(1) and 49(1) as a
precondition to making a standard.

52 Clause 38(2) of the Bill.


-----

# DiGi

NN.The ACMA should be required to undertake public consultation prior to making an industry
standard. This is critical when taking into account the public's interest in this Bill.

##### 8. Transparency and accountability requirements

8.1. **_Key_** **_Issue:_** **_The_** **_ACMA's_** **_exercise_** **_of_** **_powers_** **_under_** **_the_** **_Bill_** **_should_** **_be_** **_subject_** **_to_** **_greater_**

**_accountability_** **_to_** **_parliament_** **_and_** **_the_** **_public_** **_to_** **_strengthen_** **_the_** **_Bill's_** **_protection_** **_for_** **_freedom_**
**_of_** **_speech._** **_DIGI_** **_supports_** **_policy_** **_settings_** **_that_** **_encourage_** **_diversity_** **_and_** **_innovation_** **_in_** **_the_**
**_digital_** **_information_** **_ecosystem._** **_The_** **_Bill_** **_may_** **_limit_** **_many_** **_valuable_** **_sources_** **_of_** **_online_**
**_information_** **_and_** **_may_** **_discourage_** **_media_** **_diversity_** **_and_** **_innovation_** **_in_** **_the_** **_Australian_** **_market._**

8.2. Given the broad scope of this Bill and its impact on lawful communications online across
services which facilitate vital every day communications of Australians the exercise of
the powers afforded to the ACMA needs to be subject to greater scrutiny by the public
and legislature. While alleged misuses of these powers by the regulator could be
challenged by larger platforms under judicial review this is a costly, uncertain and reactive
process.

8.3. Further, diverse sources of online information are in scope of the Bill and may be treated
as 'misinformation1 including content created by freelance journalists, researchers,
podcasters, and expert commentators for a range of other types of online publications
that encourage public debate on social issues. This broad scope may discourage
distribution of these materials to the Australian market. This reinforces the need to limit
the scope of the powers of the ACMA to require codes or make standards to
disinformation and for greater accountability measures regarding the ACMA's exercise of
its powers. We suggest that the Government give further thought to ensuring the Bill
provides the public and parliament with additional accountability and transparency
measures which can give the public greater confidence in the implementation of the
proposed regulatory scheme.

**Specific** **recommendations**

00. Decisions by the ACMA concerning industry codes and standards should be conditional on the
ACMA publishing reasons for its decision, substantiated by relevant evidence (e.g a decision to
require a code/standard or deregister a code should be substantiated by evidence of systemic
failures/breaches by platforms to safeguard the community from disinformation). The
assessment should include an analysis of the records/information obtained from digital


-----

# DiGi

platforms concerning the measures (systems and processes) that they have implemented to
protect the community from disinformation.

PP. The registration of codes and the making of standards by the ACMA should also be subject to
human rights impact assessments.

QQ. Rules and standards made by the ACMA under this legislation should be disallowable
instruments.

RR. The Bill should make provision for a statutory review after 12 months to assess its impact on
freedom of speech and communication, privacy, and other human rights.

##### 9. Coordination between government agencies on industry codes

9.1. **_Key_** **_issue:_** **_There_** **_are_** **_a_** **_large_** **_number_** **_of_** **_legislative_** **_initiatives_** **_currently_** **_in_** **_train_** **_in_** **_different_**
**_policy_** **_portfolios_** **_that_** **_contemplate_** **_potentially_** **_overlapping_** **_sets_** **_of_** **_industry_** **_codes_** **_or_**
**_standards_** **_that_** **_apply_** **_to_** **_providers_** **_of_** **_online_** **_platforms,_** **_including_** **_codes_** **_concerning_** **_online_**
**_safety,_** **_privacy_** **_and_** **_scams._**

9.2. We suggest that there is a need to ensure that regulators take consistent approaches to
the development of these codes and standards and coordinate the timing to avoid a
situation where multiple codes and standards are being developed at the same time. This

is important, to maximise the engagement of the gamut of businesses impacted by
codes or standards.

9.3. DIGI recommends that consideration be given to additional mechanisms for cross
portfolio cooperation53 to enable codes and standards for the industry to be developed
under a more streamlined and consistent process, for example using a common
framework or model definitions. We note that the industry found it useful to use a
common set of Head Terms to enable the simultaneous development of eight Class 1
Content codes under the OSA.54 Another initiative that is helpful in thinking about how the
terminology for regulation of harmful online content can be made more consistent is the
Digital Trust and Safety Partnership's recently published glossary of terms55.

53 Australian National University, Tech Policy Design Centre, _Cultivating_ _Coordination_ _Research_ _Report_ February 2023.

54 Available at [https://onlinesafety.org.au/codes/.](https://onlinesafety.org.au/codes/)

55 Digital Trust & Safety Partnership, _Trust_ _&_ _Safety_ _Glossary_ _of_ _Terms,_ July 2023.


-----

# DiGi

**Specific** **recommendations**

SS. The Government should consider if there is a need for additional structures to facilitate cross
portfolio cooperation and to enable codes and standards for the online industry to be
developed under a more streamlined process, for example using a common framework or
model definitions.

##### 10. Penalties

10.1. **_Key_** **_issue:_** **_As_** **_drafted,_** **_the_** **_threat_** **_of_** **_high_** **_penalties,_** **_the_** **_broad_** **_power_** **_of_** **_the_** **_ACMA_** **_to_** **_make_**
**_standards,_** **_combined_** **_with_** **_the_** **_low_** **_bar_** **_for_** **_misinformation_** **_and_** **_disinformation,_** **_collectively_**
**_incentivises_** **_platforms_** **_to_** **_take_** **_down_** **_speech_** **_that_** **_runs_** **_contrary_** **_to_** **_government_** **_or_** **_regulator_**
**_positions_** **_e.g._** **_issues_** **_of_** **_public_** **_concern_** **_like_** **_climate_** **_change_** **_policy._**

10.2. The Bill provides that any breach of a code or standard incurs a maximum of 10,000
penalty units ($2.75 million in 2023) or 2 percent of global turnover (whatever is greater)
for corporations or 2,000 penalty units ($0.55 million in 2023) for individuals. The
rationale for this high penalty is that it provides a clear signal that systemic
non-compliance with obligations to prevent and respond to misinformation and
disinformation is unacceptable and that platforms need to protect end-users.

10.3. The _Guidance_ _Note_ compares these penalties with the penalties in the _Criminal_ _Code_
_Amendment_ _(Sharing_ _ofAbhorrent_ _Violent_ _Material)_ _Act_ _2019._ However, this Act concerns
_seriously_ _illegal_ _materials,_ where the Bill concerns _lawful_ _materials._ Further, as noted
above the harms of disinformation and misinformation are of a different level of severity.
The maximum penalties should be limited to systemic breaches concerning standards
concerning disinformation and should not exceed 10,000 penalty units. The Bill should
further define a 'systemic breach’ by reference to the criteria by which such breach will be
assessed, for example the frequency and severity of breaches during a defined
period.Other breaches should be subject to lesser fines, comparable to those for beaches
of codes/standards in the _Broadcasting_ _Act_ _1992_ _(Cth)._

10.4. The Bill makes platforms solely accountable for addressing misinformation and
disinformation, while exempting third parties who market, sell or intentionally promote or
coordinate disinformation campaigns from liability.56 The Guardian reported in 2022 how

‘for-profit’ disinformation networks have seized on the popularity of conspiracy
movements and far-right groups online, creating content aimed at anti-vaccine protesters

56 See Josh A. Goldstein and Shelby Grossman, _How_ _disinformation_ _evolved_ _in_ _2020_
[https://www.brookinqs.edu/articles/how-disinformation-evolved-in-2020/.](https://www.brookinqs.edu/articles/how-disinformation-evolved-in-2020/)


-----

# DiGi

and QAnon followers57.This should be addressed by the introduction of a provision in the
legislation that makes the mass marketing, sale and promotion of disinformation
campaigns illegal.

**Specific** **recommendations**

TT. Penalties under the Bill should be comparable to those in the _Broadcasting_ _Act_ _1992_ _(Cth),_
including for breaches of broadcast codes and standards. The maximum penalties under the
Bill should be limited to systemic breaches of standards concerning disinformation and capped
at 10,000 penalty units. The Bill should define a 'systemic breach’ by reference to the criteria by
which such breach will be assessed.

UU. The Bill should include a provision that makes the mass marketing or sale of disinformation
campaigns a crime.

57 'Disinformation for profit: scammers cash in on conspiracy theories ' Guardian, 21 February 2022
[https://www.theguardian.com/media/2022/feb/20/facebook-disinformation-ottawa-social-media.](https://www.theguardian.com/media/2022/feb/20/facebook-disinformation-ottawa-social-media)


-----

