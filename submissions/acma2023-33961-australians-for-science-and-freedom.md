###### ---------------FOR ---------

#### AUSTRALIANS

###### ---------------FOR ----------
 SCIENCE & FREEDOM

# ASF

##### Submission to Department of

#### Infrastructure, Transport,

##### Regional Development,

#### Communications and the Arts

##### on Communications

 Legislation Amendment

#### (Combatting Misinformation

 and Disinformation) Bill 2023


-----

###### AUSTRALIANS
------------FOR ------------
SCIENCE & FREEDOM

## ASF

August 20? 2023

The Director Genera)

Department of Infrastructure, Regional Development,
Communications and the Arts
CANBERRA ACT 2600

Dear Director General,

This submission has been made on behalf of the Australians for Science and Freedom.

The organisation is an Australian association which has the purpose of promoting proper
scientific inquiry and was founded by a number of academics and public intellectuals,
initially as a response to policy adopted during the COVID-19 pandemic.

The ASF has concerns about the proposed amendments to the act and believes it would be
against the interests of scientific advancement and democracy for it to be passed.

Our submission follows.

Yours faithfully,

**Rebekah** **Barnett** **Graham** **Young**

1 | Page


-----

**1.** **Onus** **on** **proponents** **to** **show** **the** **need** **for** **the** **legislation.**

**a.** **Not** **established** **on** **the** **research** **commissioned** **by** **ACMA**

In seeking to limit freedom of speech the onus should be on ACMA to sufficiently
demonstrate that there is a problem. They have failed to do this. In the first place, as
discussed in section 1.B., the initial research on which the legislation is based is
flawed. It doesn’t demonstrate any increase in misinformation and disinformation
(however they are defined)1, nor does it demonstrate that significant harms flow from
any misinformation and disinformation currently available on digital platforms.

Further there is a need to show that alternative and currently existing legislation is
not adequate to police false and misleading information and any harms stemming
from it.

We currently have trade practices, competition law, and other legislation which
should be used to police the material disseminated by companies on digital
platforms. For example a real danger could be said to exist from false advertising of
products, but this is a trade practices matter. Or someone might be ramping a share
price, but this can be handled by the securities code.

If that policing is not being done effectively at the moment, the question should be
raised as to why that problem wouldn’t be best tackled by resourcing the existing
agencies designed to tackle it, rather than implementing entirely new legislation and
giving resources to an organisation not designed to police it.

To the extent that the material covers matters of personal reputation or harm, then
the defamation, human rights and anti-discrimination laws would appear to give
individuals and corporations the ability to enforce their rights including having
material removed.

This is self-regulating and removes the matter to the legal system, which is where
appropriate judgments about harm and damage can most appropriately be made,
along with preservation of legal rights, including that of due process.

**b.** **No** **further** **research** **is** **relied** **upon,** **but** **there** **is** **nothing** **which** **we**
**are** **aware** **of** **which** **justifies** **this** **legislation.**

In a report2 to the Australian Government justifying the need for its proposed
expanded powers, ACMA does not sufficiently demonstrate the scale and volume of
misinformation and disinformation nor the nature of the harm with which it can be
associated.

ACMA states that, “the true scale and volume of misinformation in Australia in

1 Australians for Science and Freedom does not accept the definitions of misinformation and disinformation
put forward by ACMA, which is inconsistent with the dictionary definitions of these terms, as addressed in
Section l.C. Accordingly, any further mention of misinformation or disinformation are as 'however defined' and
should not be taken to indicate that we accept the definitions put forward by ACMA.

2 ‘A report to government on the adequacy of digital platforms’ disinformation and news quality measures’, June 2021,
[https://www.acma.gov.au/sites/default/files/2021-](https://www.acma.gov.au/sites/default/files/2021-)
11/Adequacy%20of%20digital%20platforms%20disinformation%20and%20news%20quality%20measures.pdf

2 |


-----

currently unknown.” The report references “increasing concern” about a perceived
increase in “misinformation” online, measured by survey respondents reporting how
much misinformation they believe they have seen. However, this conflates _reports of_
misinformation with _actual_ misleading or factually inaccurate information, failing to
demonstrate that the concern is founded.3

Conflating subjective user reports with actual instances of misleading material and
online harm is common in government and peak body reporting in this field. Other
potential factors that may give rise to an increase in reports of misinformation and
online hate, such as increased social sensitivity, better promotion of reporting tools,
and the impacts of cultural developments (e.g.: political polarisation) are rarely
explored.

It bears noting that government officials frequently stress that reports of perceived
physical harms on pharmacovigilance databases associated with, say, Covid
vaccines, should not be misconstrued as instances of _actual_ harm. Alternative
explanations for reports of perceived harm are typically proffered, with the onus of
proof being put onto those who wish to demonstrate a causal link between reports of
harm, and actual harm.

By the same token, it is incumbent on ACMA to demonstrate that perceptions of an
increase in misinformation online, and perceptions of resultant harm, correspond
with an actual increase in misinformation and harm.

Furthermore, research underpinning ACMAs findings is based on an error of
categorisation. Content that contradicts the official position on a range of issues is
categorised as misinformation, regardless of its veracity or contestability. This will be
discussed further in section 1 .C.

To demonstrate the harmful impact of online misinformation and disinformation,
ACMA references the US riot on 6 Jan 2021. However, its quantification of the harm
caused by this event includes misinformation      - ACMA attributes the unrelated deaths
of several people who died of natural causes to the riot        - raising questions about
ACMAs ability to reliably discern true information from misinformation.4

ACMA refers to research showing that anti-vaccine content, even if true and
accurate, can sway people’s vaccination intentions, but does not demonstrate how
this causes harm, and to what extent.

A case study on the real-world impacts of anti-5G content makes a more convincing
demonstration of fiscal harm resulting from information classified by researchers and
ACMA as misinformation.

However, it is unclear as to how the proposed measures in this bill will prevent such
harm      - there appears to be an inherent presumption that online censorship of certain
information will reduce real world harm, but research shows that censorship simply

3 P. 21, Finding 3. [https://www.acma.gov.au/sites/default/files/2021-](https://www.acma.go)

11/Adequacv%20of%20digital%20platforms%20disinformation%20and%20news%20quality%20measures.pdf

4 P.30 [https://www.acma.gov.au/sites/default/files/2021-](https://www.acma.gov.au/sites/default/files/2021-)

11/Adequacy%20of%20digital%20platforms%20disinformation%20and%20news%20quality%20measures.pdf
[https://www.snopes.com/fact-check/capitol-riot-deaths/](https://www.snopes.com/fact-check/capitol-riot-deaths/)


-----

encourages users to find work-arounds, a fact acknowledged by ACMA in the
report.5

Moreover, as the ACMA bill does not include content produced by the government in
its definitions of misinformation and disinformation, it will not address the arguably far
more considerable harms perpetrated by misleading information disseminated by the
government. For example, case studies quantifying the impacts Australian
Government’s Covid response, including the propagation of misinformation such as
‘the vaccines will prevent transmission’ and ‘lockdowns save lives’ have
demonstrated astronomical fiscal, life-year and social harm, yet under this bill, such
harm would not be mitigated.6

**c.** **Circular** **definition** **of** **misinformation** **and** **disinformation**

The bill relies on a circular definition of misinformation and disinformation, whereby
the official position is the ‘true’ one, and contradictory information is ‘misinformation
or disinformation.’

This circular definition is arrived at in two ways:

1. First, the study by the News & Media Research Centre (University of Canberra)7
that was commissioned by ACMA to inform the development of the bill
categorises beliefs that are contradictory with official government advice as
‘misinformation’, regardless of the veracity of the advice. For example, in Table
14 showing the coding of‘Misinformed Groups’, respondents are coded as
misinformed if they:

a) Agree that wearing a mask does not significantly reduce your risk of infection
or spreading the virus;
b) Disagree that the Covid-19 vaccines that are approved by the health
authorities are safe; or,

c) Agree that in most cases, Covid-19 can be prevented or treated by taking
vitamins and supplements or other over the counter medicines.

As all three of these positions are supported by peer-reviewed scientific literature,
it is incorrect to categorise these respondents as misinformed. A better
description would be to categorise these respondents as believing information
that contradicts the official position.

5 [https://pubmed.ncbi.nlm.nih.gov/36250528/](https://pubmed.ncbi.nlm.nih.gov/36250528/)

6 https:/ ipa.orq.au/wp-content/upioads/2022/09/220921-IPA-Report-Hard-Lessons-Reckoning-the-economic-social-and-
humanitarian-costs-of-zero-COVID.pdf, Do Lockdowns and Border Closures Serve the ‘Greater Good'?
[https://www.thegreatcovidpanic.com/_files/ugd/23eb94_33b4f30ef8fa4e6eaf1a7e62d571a9a7.pdf](https://www.thegreatcovidpanic.com/_files/ugd/23eb94_33b4f30ef8fa4e6eaf1a7e62d571a9a7.pdf)

7 [https://apo.org.au/sites/default/files/resource-files/2022-03/apo-nid316582.pdf](https://apo.org.au/sites/default/files/resource-files/2022-03/apo-nid316582.pdf)


-----

**TABLE** **14** MISINFORMED GROUPS RECODING

Percentage in the sample

Disagree Neither Agree Don't know

Wearing a mask does not significantly reduce your risk of infection or spreading
60 17 **21** 2
the virus.

Covid-19 vaccines that are approved by the health authorities in Australia are safe.
**9** 28 56 8
<reverse>

1 am confident that official medical guidelines and treatment for Covid-19 in my
**4** 16 74 3
State or Territory are based on evidence and best practice. <reverse>

The risks posed by Covid-19 are being exaggerated by people in power who want
53 20 **24** 3
to take advantage of the situation.

In most cases Covid-19 can be prevented or treated by simple remedies such as
66 15 IS 3
taking vitamins and supplements or other over the counter medicines.

It is noteworthy that the second publicly-funded study commissioned by
ACMA, a social media content and network analysis by creative consultancy
We Are Social, remains inaccessible to the public on the justification that it
“contains sensitive information pertaining to public figures and user
accounts.”8 This underscores the existing asymmetry of the information
environment, wherein public resources are used to produce and conceal
information, while at the same time, ACMA seeks more power to demand
transparency from and control over digital platforms and their users.

However, the few snippets of the We Are Social report shared by ACMA
indicate that the researchers make the same error as the N&MRC study, by
mislabelling scientifically-supported concerns as ‘conspiracy’ and
‘misinformation.’9 For example, ACMA details four “misinformation narratives”
examined in the We Are Social study, including ‘anti-lockdown conversation’
and ‘anti-vax conversation’ (by which we presume that We Are Social has
forgone the traditional meaning of ‘anti-vax’          - anti-all vaccines          - for the new
meaning in common parlance, i.e., ‘sceptical of the safety and/or efficacy of
Covid vaccines’, which is itself disinformation).

The fact that at least two of the four identified “misinformation narratives” are
supported by a body of scientific literature and observational reports, such as
cost-benefit analyses, again highlights the faulty logic on which the research
informing the misinformation and disinformation bill is based.

8 https://www.acma.gov.au/sites/default/files/2022-03/ACMA%20misinformation%20report_Fact%20sheet%201%20-
%20key%20research%20findings.pdf

9 Page 23, Fig. 12, [https://www.acma.gov.au/sites/default/files/2021-](https://www.acma.gov.au/sites/default/files/2021-)

11/Adequacy%20of%20digital%20platforms%20disinformation%20and%20news%20quality%20measures.pdf


-----

**Figure** **12:** **Share** **of** **conversation** **by** **selected** **narrative** **within** **selected**
**conspiracy-driven** **groups** **and** **accounts,** **April** **2020** **to** **April** **2021**

Anti-vax

{ conversation

18.1%

QAnon
f conversation

16.5% 10.6%

10.7%

5.5%

Anti-lockdown
conversation 5.2% 2.0%

1.3%

8.8% 1.1%

\2.7%

0.9% 4.7%

4.8%

Anti-5G 7.3%
conversation

_Source:_ We _Are_ _Social,_ _Social_ _media_ _insights_ _into_ _how_ _online_ _misinformation_ _and_ _disinformation_ _are_ _being_
_spread_ _across_ _social_ _platforms_ _in_ _Australia,_ _May_ _2021_ _[unpublished]._

_Note:_ _Based_ _on_ _share_ _of_ _conversation_ _across_ _a_ _sample_ _of_ _100_ _Facebook_ _groups,_ _100_ _Facebook_ _pages_ _and_
_91_ _Instagram_ _accounts._ _Diagram_ _is_ _illustrative_ _and_ _not_ _proportionate._ _Does_ _not_ _equal_ _100%_ _due_ _to_ _rounding._

ACMA states that, “Belief in COVID-19 falsehoods or unproven claims
appears to be related to high exposure to online misinformation and a lack of
trust in news outlets or authoritative sources.” This should be rephrased,
“Belief in positions alternative to the official position appears to be related to
high exposure to alternative viewpoints and a lack of trust in news outlets or
authoritative sources.”10 We suggest that serious introspection by the latter on
why this is so would be the better remedy.

Thus, it is clear that the conceptual foundation for ACMA’s definition of
misinformation and disinformation is ‘information which contradicts the official
position.’

2. Second, the bill explicitly excludes content produced by government,
accredited educational institutions, and professional news from the definition
of misinformation and disinformation. This is a departure from the traditional
definitions for misinformation and disinformation, which encompass _all_
information that is false or misleading, either unknowingly (misinformation) or

10 [https://www.acma.gov.au/sites/default/files/2021-](https://www.acma.gov.au/sites/default/files/2021-)
11/Adequacy%20of%20digital%20platforms%20disinformation%20and%20news%20quality%20measures.pdf


-----

with the intention to deceive (disinformation), and do not exclude
information/content based purely on its source.

Why misinformation and disinformation disseminated by government, accredited
institutions and professional news outlets should be excused from laws
purportedly intended to minimise the digital proliferation of such content is not
justified within the bill. Nor is a rationale offered for the bill’s departure from the
traditional definitions of misinformation and disinformation.

The specification that misinformation and disinformation are content that could
imply a broad range of harms further compounds the problem. Harm by what
measure? If a government says its policies save lives or perform some other
social good, and content produced by the government is immune from
misinformation and disinformation regulation, it is highly likely that ‘harm’ in the
context of this bill will be determined to be any outcome that runs counter to that
intended under the policies of the government of the day.

**2.** **Legality**

**a.** **Breaches** **implied** **freedom** **of** **political** **expression**

The High Court has found an implied freedom of political expression which is
based on the idea that for a democracy to function there must be a free
exchange of ideas.

While the bill attempts to sidestep this freedom it is highly likely that there will
be challenges, and that these challenges will be successful. Classifying
government information as free from an imputation of misinformation or
disinformation would seem to guarantee that, as it would potentially
disadvantage the communications of those who wish to challenge the
government, but challenge of government is of the very essence of
democracy.

**b.** **Breaches** **natural** **rights** **and** **due** **process**

The bill sets up a situation where what is, or is not, misinformation or
disinformation is determined by codes of conduct applied by either industry
organisations, or social media platforms. These may be quite arbitrary and
given the potential punishments for publication of opinions the ACMA regards
as untrue, are likely to be conservatively framed and favour official narratives,
no matter how unlikely these narratives may be.

The amount of data that social media platforms are required to police means
that “infringements” will be determined in the first place, and in most cases
and probably ultimately, by Al. The lack of human judgment means that
natural rights and due process will not be given to most of those who produce
material deemed to contravene the legislation.

It should be noted that the Robodebt scheme was deemed illegal because it
alleged a debt on the basis of statistical likelihood. What is the difference
between that, and how social media will need to police this law?


-----

**3.** **Anti-Democratic**

**a.** **Platforms** **will** **play** **safe**

ACMA says that it will not be the arbiter of what is true and what is
misinformation or disinformation, outsourcing the adjudication of content to
the platforms. However, platforms will be penalised if ACMA determines them
to be in breach of industry standards and guidelines set by ACMA, by allowing
misinformation and disinformation to be disseminated on their platforms.
Therefore, platforms will be incentivised to ‘play it safe’.

In practice, this means that digital platforms will remove or restrict content that
counters official positions, or that falls into the ‘grey’ area between obviously
true vs. obviously false. Platforms will take the official government position as
_de_ _facto_ for ‘true information’, judging anything that contradicts the official
position therefore as ‘misinformation.’ This is already happening.

As example, YouTube’s medical misinformation policy defines misinformation as,
“content that poses a serious risk of egregious harm by spreading medical
misinformation that contradicts local health authorities' (LHAs) or the World Health
Organization's (WHO) guidance about specific health conditions and substances."11

The real-world consequence of platforms taking official positions as _de_ _facto_
for ‘true information’ is the censoring of valid, and often true information. For
example, YouTube cited this policy as grounds for the removal MP John
Ruddick’s maiden speech to the New South Wales Parliament from its
platform.12

In another example, platforms including Twitter, Facebook and Instagram
censored over 4,000 social media posts during the pandemic years at the
behest of the Australian Government, many of which contained true (factually
correct) information.13

**b.** **Avoid** **risk** **by** **outsourcing** **to** **“fact** **checkers”** **which** **is** **dangerous**

ACMA states that it will not determine the truthfulness of individual pieces of
content, but that digital platforms will be encouraged to use a range of tools
including Fact Checkers.

Fact Checkers are engaged by digital platforms as independent arbiters of
truth. However, they are not independent, and they have no greater claim to
truth than anyone else.

11

[https://support.google.com/youtube/answer/13813322?hl=en&ref_topic=10833358&visit_id=6382810304399](https://support.google.com/youtube/answer/13813322?hl=en&ref_topic=10833358&visit_id=6382810304399)
77920-3807964568&rd=l

12 [https://www.zerohedge.com/political/youtube-censors-australian-politicians-maiden-speech-parliament](https://www.zerohedge.com/political/youtube-censors-australian-politicians-maiden-speech-parliament)

13 https://www.theaustralian.com.au/nation/many-censored-social-media-posts-did-not-contain-covid19-misinformation/news-
story/c47a8217ffada2cf576475aef3c12c63


-----

In court proceedings, Facebook has claimed First Amendment protections for
its Fact-Checker decisions, a tacit admission that fact-checks are just
opinions.14

Unfortunately, these opinions are frequently wrong on matters of fact.
Additionally, Fact Checkers routinely misrepresent contestable topics as
‘settled science’ and conflate the absence of evidence (due to undone
science) with categorical evidence of absence.

For example, AAP falsely claimed that the Australian Government had not
tried to hide reports of Covid vaccine adverse reactions.15 Documents
released under FOI request revealed that the Therapeutic Goods
Administration (TGA) did in fact hide child deaths reported following
vaccination, due to concerns that disclosure, “could undermine public
confidence.”16 In another document release, the Department of Health was
shown to have actively sought for the removal of Facebook posts describing
users’ adverse reactions to Covid vaccines.17

The RMIT Fact-Lab unit falsely debunked claims that Covid vaccines were
affecting women’s menstruation, only for the claims to be proven true when
evidence was published in the peer reviewed scientific literature.18 RMIT Fact­
Lab has not corrected its erroneous ‘debunk.’19

It has also come to light that ‘independent’ Fact Checkers are not necessarily
financially independent from commercial interests. It was recently announced
that Meta (parent company of Facebook) will pay an undisclosed amount to
Fact Checkers including AAP and RMIT Fact-Lab for the purpose of
safeguarding the online information environment in the lead up to the Voice to
Parliament referendum.20

Moreover, documents obtained in legal discovery show that Meta has, or has
had, a commercial agreement with RMIT Fact-Lab whereby RMIT Fact-Lab
receives USD $800 from Meta per fact check, for up to 50 fact check articles
per month.21 However, this commercial arrangement is not listed on RMIT
Fact-Lab’s funding disclosure page on its website.22

RMIT Fact-Lab has not responded to multiple emails asking for comment on
this issue. This kind of opacity is counter to the spirit of transparency that is
considered to be fundamental to the democratic process of testing truth
claims.

14 [https://nypost.com/2021/12/14/facebook-admits-the-truth-fact-checks-are-really-just-lefty-opinion/](https://nypost.com/2021/12/14/facebook-admits-the-truth-fact-checks-are-really-just-lefty-opinion/)

15 [https://www.aap.com.au/factcheck/hidden-covid-19-vaccine-reactions-data-is-far-from-secret/](https://www.aap.com.au/factcheck/hidden-covid-19-vaccine-reactions-data-is-far-from-secret/)

16 [https://news.rebekahbarnett.com.aU/p/breaking-australias-drug-regulator](https://news.rebekahbarnett.com.aU/p/breaking-australias-drug-regulator)

17 [https://news.rebekahbarnett.com.aU/p/breaking-the-australian-government](https://news.rebekahbarnett.com.aU/p/breaking-the-australian-government)

18 [https://bmjmedicine.bmj.com/content/1](https://bmjmedicine.bmj.com/content/1) /I/e000297

19 [https://www.abc.net.au/news/2021-04-30/coronacheck-menstruation-periods-vaccines-misinformation-facts/100099778](https://www.abc.net.au/news/2021-04-30/coronacheck-menstruation-periods-vaccines-misinformation-facts/100099778)

20 https://www.theaustralian.com.au/business/media/social-media-company-meta-said-it-will-roll-out-measures-to-stamp-out-
misinformation-in-the-lead-up-to-the-voice-referendum-vote/news-story/1c495cfe2f70f4bda5b691116b7be1f4

21 [https://twitter.eom/therealrukshan/status/1680736713851928577?s=20](https://twitter.eom/therealrukshan/status/1680736713851928577?s=20)

22 [https://www.rmit.edu.au/about/schools-colleges/media-and-communication/industry/factlab/about-rmit-factlab](https://www.rmit.edu.au/about/schools-colleges/media-and-communication/industry/factlab/about-rmit-factlab)


-----

It is thus evident that the framing of Fact Checkers as unconflicted arbiters of
truth by both digital platforms and by government is inappropriate. It is also
potentially dangerous, [and additional examples of government disinformation]

While other forms of misinformation are left to battle it out in the information
environment on their merit, misinformation and disinformation disseminated
by Fact-Checkers are conferred with a degree of legitimacy because of their
moniker, and the authority that they have been given by the platforms that
engage them. This power imbalance has the potential to make citizens more
vulnerable to harms perpetuated by misinformation and disinformation
disseminated by Fact Checkers than harms perpetuated by misinformation
and disinformation from sources perceived to be less authoritative.

**4.** **Impractical** **and** **favours** **legacy** **media**

**a.** **Impossibility** **of** **policing** **sites** **fairly**

The bill puts an onus on social media platforms to moderate misinformation or
disinformation to avoid harm. But how much misinformation or disinformation
equals harm? Is this just one instance of ‘suspect’ content? Should it be
measured in absolute terms         - no more than a certain number of comments?
Or relative terms         - a certain percentage? Absolute terms might make more
sense on a large site where even a small percentage of overall comments
might represent a large enough pool to plausibly create “harm”, but
percentage might make more sense on a smaller site.

How then should platforms monitor content to avoid being fined? Al is only a
partial solution. Users of social media platforms have ways of disguising what
they are saying, or inventing novel terminologies, all of which will temporarily
defeat Al, or put innocent communications at risk because the Al net will have
to be cast too wide.

A further problem is when the ‘facts’ change. If misinformation and
disinformation are to mean anything other than ‘what is not government
information’, then understandings will change as new data and insights
emerge overtime. Are platforms to retrospectively change rulings, or will they
be subject to the risk of civil proceedings for defamation or other torts?

And as governments are deemed to be incapable of misinformation and
disinformation, what happens when there is a change of government?

**b.** **Barriers** **to** **entry** **to** **new** **entrants**

The rules will also be more onerous on small sites and new entrants who lack
scale. The cost of moderation via human or artificial means will be significant,
and the potential penalties even larger.

While professional news organisations are exempted from the laws, sites
such as those run by the Australians for Science and Freedom, while
producing high quality, evidence-based content, could be defined by the
ACMA as sources of disinformation.


-----

Open access journals or other innovations in discussing and disseminating
scientific information would also be at risk, and at a disadvantage against
established journals.

In these cases, such channels could be open to penalties that would put them
out of business. For example, for contravening an industry code the penalty
for a corporate is up to 2 per cent of annual turnover, or $2.2 million with
ACMA making the decision.

The act also allows ACMA to impose codes on digital platforms, but this does
not appear to be a risk that legacy media runs.

ACMA can also vary “misinformation standards” on grounds it finds
“reasonable” (S51). This adds an additional layer of risk to any digital media
businesses not faced by their competitors.

**c.** **Establishing** **voluntary** **rules** **of** **conduct** **as** **effectively** **legislation,**
**but** **without** **proper** **democratic** **and** **accountable** **processes** **for**
**establishing** **them.**

This bill extends a tendency in legislation to delegate to bureaucrats via
regulations powers which ought to be exercised by legislatures. This bill would
effectively allow ACMA to extend definitions of harm under various state and
federal human rights and anti-discrimination legislations, as well as potentially
encroaching on consumer and defamation law.

There is no evidence that ACMA has the expertise or is properly resourced to
do this, or that codes of conduct should be decided outside the normal
democratic process.

**5.** **Economic** **and** **scientific** **cost**

Societies work best and grow when there is an open and competitive structure.
Regulation and uncertainty are the enemies of economic growth and human
flourishing. This bill will introduce both. It will also reduce the innovation that is
the key to growth.

Digital media platforms are in one sense a source of collective thought in that
they facilitate a collective conversation where different ideas can be explored and
old ones replaced by better ones. By allowing a governmental instrumentality to
determine what is and what isn’t disinformation and misinformation rather than
the cut and thrust of conversation, and preferencing government narratives over
all others, this bill guarantees that it will be more difficult to discuss and
disseminate advances.

This will come not just at a social cost, but an economic and scientific one. In an
emergency situation, like a pandemic, it will tend to stifle the “gifted amateurs”
who blog on these issues, and who are often the best analysts of the data that is


-----

available, but who would struggle to be published in mainstream media or
scientific journals.

Even for those analysts who can find mainstream publishing outlets, it still slows
the process down. Scientific journals are notoriously slow to publish.

**6** **The** **legislation** **will** **not** **work,** **and** **will** **simply** **increase** **regulatory** **burden**
**to** **nil** **effect**

Legislation of the ACMA bill will increase regulatory burden without meaningfully
reducing the amount of misinformation and disinformation being shared online.

Misinformation research conducted during the pandemic found that increased
social media censorship of vaccine-sceptical content was associated with an
increase in subscription to vaccine-sceptical sites and news services.23 24

Similarly, ACMA reports that,

“Content removal or de-platforming feeds into the general belief that platforms
are involved in a deep-state ‘cover-up’. It also encourages members of
conspiracy-driven communities to take steps to pro-actively avoid detection or
automated content moderation tools. We Are Social found widespread use of
”24
intentionally misspelling keywords in posts, such as ‘vSccine’ and ‘vackseen’.

We predict that users will simply move to private messaging channels, the dark
web, and other avenues to continue sharing information, a possibility
acknowledged by ACMA,

'Widespread content moderation by the platforms may also drive these
conversations further underground, by encouraging mass migrations to smaller
alternative social media or encrypted messaging apps.”25

The only measurable outcome this bill can achieve is the ACMAs expansion
requiring considerable human and financial resources.

**RECOMMENDATIONS**

We recommend that the bill be abandoned entirely. The need for it has not been
sufficiently demonstrated, and the types of harms that it seeks to mitigate can, as far
as we can tell from the details provided, be dealt with in almost all cases using
existing structures in ways which guarantee individual rights are protected, without
choking the necessary free flow of information. Should there be some individual
harms, then they should be addressed directly and in legislation rather than through
an extra-parliamentary system such as this.

23 [https://pubmed.ncbi.nlm.nih.gov/36250528/](https://pubmed.ncbi.nlm.nih.gov/36250528/)

24 P. 25 [https://www.acma.gov.au/sites/default/files/2021-](https://www.acma.gov.au/sites/default/files/2021-)

11/Adequacy%20of%20digital%20platforms%20disinformation%20and%20news%20quality%20measures.pdf

25 P. 25 [https://www.acma.gov.au/sites/default/files/2021-](https://www.acma.gov.au/sites/default/files/2021-)

11/Adequacy%20of%20digital%20platforms%20disinformation%20and%20news%20quality%20measures.pdf


-----

