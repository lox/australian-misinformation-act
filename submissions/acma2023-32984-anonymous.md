Schedule 9 – Part 1 – Section 7 – The definitions of misinformation and disinformation.

  - 1(a)/2(a): “the content contains information that is false, misleading or deceptive.”

`o` **_Misinformation is information that is false, misleading, or deceptive, without_**

intending that the content be deceptive. This should be clarified in the definition.

  - 1(b)/2(b) “the content is not excluded content for misinformation purposes.”

`o` Professional news content is excluded, but no reason is provided.
`o` Professional news content is prone to misleading consumers through:

         - Selection bias due to the inherent interest people have in rare over common
events, overestimating their likelihood or importance (the availability
heuristic). Editorial policies exacerbate this and favour publishing short term
negative events rather than updates on long term positive trends.

         - Negativity bias due to negative news being more marketable than positive
news (‘if it bleeds it leads’).

         - Confirmation bias where ideologically driven positions are either heightened
or hidden depending on the news organisation.

             - These biases can be harmful as they paint a bleak picture of the world
where people are in a state of tribal fear rather than being empowered
with knowledge to make decisions to improve the world by
referencing positive trends.

`o` More thought is needed about the potential for serious harm caused by professional

news content, and whether it should be covered by the proposed legislation.

`o` It is fair and balanced to apply the same principles to professional news organisations

as to everyday Australians. If the end goal is harm reduction, then those with the
greatest ability to purvey information (professional news organisations) should be
held to the same standards when it comes to potential harms. This inconsistent
treatment must be addressed.

  - 1(d)/2(d) “the provision of the content on the digital service is reasonably likely to cause or
contribute to serious harm.”

`o` “Reasonably likely” is too ambiguous and needs to be defined.

         - “Likely” implies uncertainty. Either content has or has not contributed to
serious harm. Given that 1(c)/2(c) state that to be misinformation or
**_disinformation it needs to have been disseminated, the words “reasonably_**
likely” seem misplaced.

         - If digital platforms or ACMA are to assess likelihood of harm and can’t
verify harm in every case, research should be conducted on the degree of
harm inflicted by all types of misinformation and disinformation. This
research should aim to find the proportion of people which suffered or did not
suffer harm, so that these results can be applied to testing whether future
cases are likely to cause harm.

         - If 50% of people suffered harm and 50% did not, a discussion is needed on
the trade-offs between harm and freedom of expression. This discussion on
trade-offs should happen at every proportion – a 50/50 split is raised for
demonstration purposes only.

         - ACMA should make public a policy guidance document on how decisions
around “serious” harm are made, not only because likelihood is not defined,
but “serious” is not defined either.

`o` “Contribute to” – to what degree and how are other contributing factors assessed?

         - How much will other factors be considered in contributing to harm?

         - Other factors that contribute to harm include a lack of proper public
education about how to verify information (and therefore how much of the


-----

information a consumer will take seriously), a lack of personal responsibility
over curating information one consumes, and an unhealthy mental disposition
in response to harmful information (because of an underlying condition such
as depression or anxiety).

       - To what degree is the drafted legislation treating a symptom of harm and not
the root cause? To what degree does action taken treating symptoms of harm
spend resources which could be devoted to treating causes?

`o` The definition of harm includes harm to the integrity of Australian democratic

processes, however if political communication is exempt (Part 4 – Section 60) then
false and misleading claims can be made through political communication and harm
the integrity of Australian democratic processes. Additionally, someone may make
political comments advocating for candidates to be elected to Parliament that espouse
hatred against people based on ethnicity, nationality, race, gender, sexual orientation,
age, religion or physical or mental disability, thereby exempting this communication
from the definition of misinformation and disinformation. This inconsistency must
be addressed.

       - Like above statements on news organisations, it is fair and balanced to apply
the same principles to political communication as to apolitical
communication by everyday Australians. If the end goal is harm reduction,
then those with the greatest ability to purvey information which has the most
wide-ranging consequences (those who seek to influence elections) should be
held to the same standard when it comes to potential harms. This inconsistent
treatment must be addressed.

`o` Serious harm can be inflicted by information which is true. For example:

       - “You have been drafted to fight in a war.” (Harm to the psychological health
of Australians)

       - “Your wife has cheated on you.” (Harm to the psychological health of
Australians)

       - “You have been made redundant from your job.” (Harm to the psychological
health of Australians)

       - “You are obese, and this will reduce your life expectancy.” (Harm to the
psychological health of Australians)

       - “You son has died.” (Harm to the psychological health of Australians)

       - “Your child resents you.” (Harm to the psychological health of Australians)

       - “The transition to renewables may need fossil fuels in the medium term.”
(Harm to the environment/Economic or financial harm)

       - “Religious practice can bring people together who think similarly but exclude
people who don’t align with the religion’s values.” (Disruption of public
order)

       - “I’m going to exercise my democratic right to protest for what I believe in.”
(Disruption of public order)

       - All these situations can cause serious harm. The real intent of the legislation
should be to prevent the dissemination of false or misleading information
**_which leads to poor decisions, not to prevent serious harm in and of itself._**
This inconsistency must be addressed.

- 2(e) “the person disseminating, or causing the dissemination of, the content intends that the
content deceive another person.”

`o` How will you prove that someone intended to deceive someone else if they can deny

it and how much resourcing will be devoted to this?


-----

Part 3 – Division 3 – Section 32 – Statement of regulatory policy.

  - This section makes clear what consultation needs to occur when drafting misinformation
codes. A public consultation should be put in place for users of online platforms to assess the
suitability of misinformation codes with a view to assessing the trade-off between free speech
and poor decisions that may result from misinformation and disinformation.

Part 3 – Division 3 – Section 33 – Examples of matters that may be dealt with by misinformation
codes and misinformation standards.

  - 3(e) “preventing monetisation of misinformation or disinformation on digital platform
services.”

`o` Professional news organisations engage in misinformation on digital platform

services, however, are currently exempt. As highlighted above, this inconsistency
must be addressed.

Part 3 – Division 3 – Section 34 – private messages.

  - “The ACMA must not register a code (or part of a code), or determine a standard, under this
Part that contains requirements relating to: (a) the content of private messages.”

`o` Does this apply to posts on a wall where an account is made private? This ambiguity

should be addressed.

Part 3 – Division 3 – Section 35 – electoral and referendum matters.

  - This section precludes the application of codes where the dominant purpose of content is
influencing the way electors vote in an election, but no reason is provided for this.

  - Content which is false, misleading, or deceptive (whether intentional or not) which is drafted
to influence the way electors vote in an election is possibly the most harmful content to the
Australian public, as it influences outcomes which have the potential to negatively impact all
Australians given Commonwealth legislation applies to all Australians.

  - It is fair and balanced to apply the same principles to political communication as to apolitical
communication by everyday Australians. If the end goal is harm reduction, then those with
the greatest ability to purvey information (those who seek to influence elections) should be
held to the same standard when it comes to potential harms. This inconsistency should be
addressed.

“Political communication” is yet to be defined. It is cited:

1. Part 3 – Division 4 – Subdivision A – Section 37(1)(d)(i)
2. Part 3 – Division 4 – Subdivision B – Section 40(1)(d)(i)
3. Part 3 – Division 5 – Subdivision A – Section 45(a)
4. Part 3 – Division 5 – Subdivision B – Section 51(2)(a)
5. Part 4 – Section 60(1)

“Circumstances the ACMA considers relevant” is too broad and undefined. Circumstances should
be defined in the legislation explicitly. It is cited:


-----

1. Part 3 – Division 4 – Subdivision A – Section 37(1)(d)(ii)
2. Part 3 – Division 4 – Subdivision B – Section 40(1)(d)(ii)
3. Part 3 – Division 5 – Subdivision A – Section 45(b)
4. Part 3 – Division 5 – Subdivision B – Section 51(2)(b)

Part 3 – Division 5 – Subdivision A – Section 46(2)/(3) – ACMA may determine standards.

  - “The ACMA may, by legislative instrument, determine a standard that applies to participants
in that section of the digital platform industry and deals with that matter or those matters. A
standard under this subclause is to be known as a misinformation standard.”

`o` The ACMA must have the power, by legislative instrument, to review and repeal the

misinformation standard if it is deemed unacceptable by the Australian people. The
capability for review should be written into the Act. This also applies to Sections
48(3), 49(3), 50(2)

  - “Before determining a standard under this clause, the ACMA must consult the body or
association to whom the request mentioned in paragraph (1)(a) was made.”

`o` ACMA must consult with the Australian public on the misinformation standard. This

also applies to Sections 48(4), 49(4), 50(3).

         - The reason for consulting the Australian public is they are best placed to
provide feedback to ACMA about harm suffered (as the subjects to which
harm is applied), whether preventing harm ought to overwrite any freedom of
expression, and the degree to which the harm is seen as intentional or not.

         - It is important for a healthy society to not assume the worst of people when
they communicate, but rather to assess through research the degree of harm
inflicted by all types of potential misinformation and disinformation and
find proportions of people who suffered harm or not. This will allow ACMA
to assess the likelihood of future instances of harm.

Part 3 – Division 5 – Subdivision B – Section 51(1) and Section 52 – Variation and revocation of
misinformation standards.

  - Section 51(1) and Section 52 should be reframed or detailed with other considerations such
as:

`o` "Because the code failed to achieve its desired objective of protecting the community

from harm."

`o` "Because the code resulted in stifling of personal expression on digital platforms with

minimal benefits in reducing of harm."

`o` "Because the onus of contending with misinformation and the harm caused by it was

unjustly placed on digital platforms rather than the community or public education."

  - Again, the issue of this legislation’s intent needs to be clarified. True information can cause
serious harm therefore the real intent of the legislation should be to prevent the dissemination
of false information which leads to poor decisions.

Part 3 – Division 6 – Section 55(4)

  - “The Register is to be made available for inspection on the internet.”


-----

`o` Clarify that The Register will be made available to the Australian public and is open

to feedback which ACMA must consider. Inspection alone does not require any action
on the part of ACMA should issues be found in The Register.

Schedule 2—Consequential amendments and transitional provisions – Section 7(g)

  - “The Parliament also intends that digital platform services be regulated, in order to prevent
and respond to misinformation and disinformation on the services, in a manner that:
encourages the development of technologies relating to digital platform services.”

`o` How can prevention and response to misinformation and disinformation encourage

development of digital platform technologies if it imposes additional costs on
providing digital platform services? Additional costs include monitoring and
enforcing misinformation and disinformation and limiting the potential user base.


-----

