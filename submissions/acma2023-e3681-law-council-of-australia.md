## Law Council

**OF** **AUSTRALIA**

_Office_ _of_ _the_ _President_

**29** **June** **2023**

Mr Jim Betts
Secretary
Department of Infrastructure, Transport,
Regional Development Communications and the Arts
GPO Box 594
CANBERRA ACT 2601

By email: information.inteqrity(a)jnfrastructure.qov.au

Dear Mr Betts,

**Communications** **Legislation** **Amendment** **(Combatting** **Misinformation** **and**
**Disinformation)** **Bill** **2023—Exposure** **Draft**

The Law Council welcomes the opportunity to provide a submission to the Department of
Infrastructure, Transport, Regional Development, Communications and the Arts in relation to
the Exposure Draft of the Communications Legislation Amendment (Combatting
Misinformation and Disinformation) Bill 2023.

The Law Council’s submission is attached. The Law Council would be pleased for this
submission to be made public on the Department’s website.

The Law Council acknowledges the assistance of its National Human Rights Committee, the
Law Institute of Victoria, and the Victorian Bar in the preparation of this submission.

If you would like to discuss this submission further, please contact Mr John Farrell, Senior
Policy Lawyer, onl

Yours sincerely

**Luke** **Murphy**
**President**

**_Telephone_** +61 2 6246 3788 **_■_** **_Email_** [mail@lawcouncil.au](mailto:mail@lawcouncil.au)

PO Box 5350, Braddon ACT 2612            - Level 1, MODE3, 24 Lonsdale Street, Braddon ACT 2612


-----

# Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2023— Exposure Draft

#### Department of Infrastructure, Transport, Regional Development, Communications and the Arts


###### 29 August 2023

**_Telephone +61 2 6246 3788_**
**_Email mail@lawcouncil.au_**
**PO Box 5350, Braddon ACT 2612**
**Level 1, MODE3, 24 Lonsdale Street,**


-----

### Table of Contents

**About the Law Council of Australia ............................................................................... 3**

**Acknowledgements ........................................................................................................ 4**

**Introductory comments .................................................................................................. 5**

**About the Draft Bill ........................................................................................................10**

Record Keeping and Information-Gathering Powers .....................................................10

Industry Codes and ACMA Standards .......................................................................... 11

**Specific textual comments ............................................................................................13**

Information-gathering powers .......................................................................................13

Privilege against self-incrimination and self-exposure to penalties ............................14

Privacy and data security ..........................................................................................14

Definitions of ‘misinformation’ and ‘disinformation’ ........................................................15

Breadth of the definition of ‘misinformation’...............................................................15

The concept of ‘excluded content’ is insufficiently protective of freedom of
expression ................................................................................................................17

The definition of ‘harm’ is overbroad and does not sufficiently limit the concept of
‘misinformation’ .........................................................................................................19

The definition of ‘disinformation’ replicates and extends the difficulties Inherent in
the concept of ‘misinformation’ ..................................................................................21

Identification of ‘misinformation’ and ‘disinformation’ .................................................22

**Conclusion .....................................................................................................................23**


-----

### About the Law Council of Australia

The Law Council of Australia represents the legal profession at the national level, speaks on behalf of its
Constituent Bodies on federal, national and international issues, and promotes the administration of
justice, access to justice and general improvement of the law.

The Law Council advises governments, courts and federal agencies on ways in which the law and the
justice system can be improved for the benefit of the community. The Law Council also represents the
Australian legal profession overseas, and maintains close relationships with legal professional bodies
throughout the world. The Law Council was established in 1933, and represents its Constituent Bodies:
16 Australian State and Territory law societies and bar associations, and Law Firms Australia. The Law
Council’s Constituent Bodies are:

   - Australian Capital Territory Bar Association

   - Law Society of the Australian Capital Territory

   - New South Wales Bar Association

   - Law Society of New South Wales

   - Northern Territory Bar Association

   - Law Society Northern Territory

   - Bar Association of Queensland

   - Queensland Law Society

   - South Australian Bar Association

   - Law Society of South Australia

   - Tasmanian Bar

   - Law Society of Tasmania

   - The Victorian Bar Incorporated

   - Law Institute of Victoria

   - Western Australian Bar Association

   - Law Society of Western Australia

   - Law Firms Australia

Through this representation, the Law Council acts on behalf of more than 90,000 Australian lawyers.

The Law Council is governed by a Board of 23 Directors: one from each of the Constituent Bodies, and
six elected Executive members. The Directors meet quarterly to set objectives, policy, and priorities for
the Law Council. Between Directors’ meetings, responsibility for the policies and governance of the
Law Council is exercised by the Executive members, led by the President who normally serves a
one-year term. The Board of Directors elects the Executive members.

The members of the Law Council Executive for 2023 are:

   - Mr Luke Murphy, President

   - Mr Greg McIntyre SC, President-elect

   - Ms Juliana Warner, Treasurer

   - Ms Elizabeth Carroll, Executive Member

   - Ms Elizabeth Shearer, Executive Member

   - Ms Tania Wolff, Executive Member

The Chief Executive Officer of the Law Council is Dr James Popple. The Secretariat serves the Law
Council nationally and is based in Canberra.

[The Law Council’s website is www.lawcouncil.au.](http://www.lawcouncil.au/)


-----

### Acknowledgements

The Law Council is grateful for the contributions of its National Human Rights Committee,
the Law Institute of Victoria, and the Victorian Bar to this submission.


-----

### Introductory comments

1. The Law Council welcomes the opportunity to provide a submission to the
Department of Infrastructure, Transport, Regional Development, Communications
and the Arts in relation to the Exposure Draft of the Communications Legislation
Amendment (Combatting Misinformation and Disinformation) Bill 2023
(the Draft Bill).

2. For the purposes of the Draft Bill ‘misinformation’ is defined to include content
provided on the digital service that ‘contains information that is false, misleading or
deceptive’ and is ‘reasonably likely to cause or contribute to serious harm’.[1]
‘Disinformation’ is defined similarly, but includes the added requirement that ‘the
person disseminating, or causing the dissemination of, the content intends that the
content deceive another person’.[2]

3. The growth of digital platforms over the past two decades has fundamentally
changed the way that Australians access information. Much of this change has
been positive. Digital platforms have provided individuals with ready access to
information and the ability to expediently connect with others.[3] Digital platforms
have also allowed for greater opportunity for political engagement and debate,
creating an unprecedented ability to engage with social issues and connect with
representatives on issues relating to Australia’s democracy.[4]

4. However, the online sphere, marked with the traits of freedom and flexibility, also
presents new and evolving challenges.[5] The growth of digital platforms has
provided new opportunities for unreliable or problematic information to be
disseminated and spread. A number of recent reports have highlighted the
deleterious effect that misinformation and disinformation disseminated on these
platforms can have on democratic processes, civil society and vulnerable minority
groups.[6] This is particularly so when information is promulgated through the
targeted and narrowing information streams provided online.

5. The Australian Media and Communications Authority (ACMA), in its June 2021
_Report to government on the adequacy of digital platforms’ disinformation and news_
_quality measures—the recommendations of which underpin the proposals in the_
Draft Bill—stated that:

_Widespread belief in harmful misinformation can have serious impacts_
_on individuals and society, with the potential to cause a broad range of_

1 Exposure Draft, Communications Legislation Amendment (Combatting Misinformation and Disinformation)
Bill 2023 (Cth) (‘Draft Bill’) s 7(1).
2 Ibid s 7(2).
3 Australian Competition and Consumer Commission, Digital Platforms Inquiry (Final Report, June 2019) 1.
4 Law Council of Australia, Submission No 18 to Senate Select Committee on Foreign Interference through
Social Media, Parliament of Australia, Inquiry of the Select Committee on Foreign Interference through Social
_Media (25 March 2020) 6._
5 Ibid.
6 See, eg, Australian Media and Communications Authority, Report to government on the adequacy of digital
_platforms’ disinformation and news quality measures (June 2021); Australian Competition and Consumer_
Commission, Digital Platforms Inquiry (Final Report, June 2019); Senate Select Committee on Foreign
Interference through Social Media, Parliament of Australia, Inquiry of the Select Committee on Foreign
_Interference through Social Media (Final Report, August 2023); Senate Select Committee on Foreign_
Interference through Social Media, Parliament of Australia, Inquiry of the Select Committee on Foreign
_Interference through Social Media (First Interim Report, December 2021); Joint Standing Committee on_
Electoral Matters, Parliament of Australia, Conduct of the 2022 federal election and other matters (June 2023);
Joint Standing Committee on Electoral Matters, Parliament of Australia, Status Report: Australian Electoral
_Commission Annual Report 2017-18 (March 2019)._


-----

_harms. These harms can be acute, such as posing an immediate and_
_serious threat to an individual’s health and safety, or chronic, such as the_
_gradual undermining of trust in public institutions or authoritative sources_
_of information.[7]_

6. Importantly, Australia is far from alone in grappling with this challenge. The
Secretary-General of the United Nations has commented on disinformation in the
following terms:

_The phenomenon of disinformation poses a multiplicity of challenges in_
_different ways. The coronavirus disease (COVID-19) pandemic has_
_provided a powerful example of the potentially enormous consequences_
_of disinformation relating to health for entire societies, including the_
_possible loss of many lives. The spread of disinformation in electoral_
_contexts may diminish public trust in the credibility of processes,_
_undermining the right to political participation. Disinformation can_
_involve bigotry and hate speech aimed at minorities, women and any_
_so-called ‘others’, posing threats not only to those directly targeted, but_
_also to inclusion and social cohesion. It can amplify tensions and_
_divisions in times of emergency, crisis, key political moments or armed_
_conflict. In effect, disinformation can affect the full range of human rights_
_by disrupting people’s ability to make informed decisions about policies_
_relating to, for example, the environment, crime, migration and_
_education, among other issues of public interest and concern.[8]_

7. The spread of misinformation and disinformation online—in particular, disinformation
spread by foreign actors—was a focus of the recent inquiry of the Senate Select
Committee on Foreign Interference through Social Media.[9] The ‘paradox’ between
the potential benefits and harms created by digital platforms, in particular social
media platforms, was noted by the Australian Human Rights Commission (AHRC) in
its submission to that inquiry:

_… social media can be used for purposes that both strengthen or_
_undermine Australia’s democracy and values. On the one hand, social_
_media can be used in ways that increase access to information and_
_opportunities for the free exchange of ideas, increase the diversity of_
_voices contributing to public discussions and allow for broader public_
_participation in our democracy. On the other hand, social media can_
_also be used in ways that pose a threat to democratic processes through_
_social medial campaigns that spread misinformation and disinformation,_
_undermine trust in public institutions and exacerbate divisions within_
_society.[10]_

7 Australian Media and Communications Authority, Report to government on the adequacy of digital platforms’
_disinformation and news quality measures (June 2021) 29._
8 _Countering disinformation for the promotion and protection of human rights and fundamental freedoms:_
_Report of the Secretary-General, 77[th] sess, 287[th] mtg, Agenda Item 69(b), UN Doc A/77/287, (12 August_
2022) 2-3.
9 Senate Select Committee on Foreign Interference through Social Media, Parliament of Australia, Inquiry of
_the Select Committee on Foreign Interference through Social Media (Final Report, August 2023)._
10 Australian Human Rights Commission, Submission No 9 to Senate Select Committee on Foreign
Interference through Social Media, Parliament of Australia, Inquiry of the Select Committee on Foreign
_Interference through Social Media (16 February 2023) 3._


-----

8. In this context, the Law Council recognises the need for new responses to address
the dissemination of misinformation and disinformation online and, in principle,
welcomes regulation that would allow individuals and organisations to identify and
address such material more effectively.

9. Currently, digital platform providers monitor their own approaches to misinformation
or disinformation. This is largely directed by the voluntary Australian Code of
_Practice on Misinformation and Disinformation developed by Digital Industry Group_
Incorporated (DIGI) and signed by a number of the larger digital platform services.
Signatories to the DIGI Code of Practice commit to implementing safeguards against
harm from misinformation and disinformation and adopting a range of scalable
measures to reduce the spread and visibility of such content.[11]

10. The Law Council notes that the Draft Bill seeks to provide ACMA with several new
powers to address misinformation and disinformation, in circumstances where it
deems self-regulation by industry bodies to be inadequate or ineffective. These
powers would enable ACMA to:

   - gather information from, or require digital platform providers to keep records
regarding misinformation and disinformation;[12]

   - publish information on its website relating to misinformation or disinformation
regulation, measures to combat the issue, and the prevalence of such
content;[13]

   - request the industry develop industry codes covering measures to combat
misinformation and disinformation which would be registered and enforced by
ACMA;[14] and

   - create and enforce misinformation standards where ACMA deems an industry
code to be ineffective.[15]

11. The Law Council is aware of concerns that the practical effect of the Draft Bill in
seeking to combat misinformation and disinformation may be a problematic
incursion on the right of freedom of expression.[16]

12. Restrictions on freedom of expression should not be made lightly. The right to
freedom of expression is a democratic ideal that encourages informed decision
making. General Comment No 34 of the United Nations Human Rights Committee
highlights the importance of freedom of expression in underpinning democratic
society:

_Freedom of opinion and freedom of expression are indispensable_
_conditions for the full development of the person. They are essential for_
_any society. They constitute the foundation stone for every free and_
_democratic society. The two freedoms are closely related, with freedom_

11 Digital Industry Group Inc, The Australian Code of Practice on Disinformation and Misinformation (11
October 2021).
12 Draft Bill, s 14.
13 Ibid ss 25-28.
14 Ibid ss 37-39.
15 Ibid ss 46-50.
16 See, eg, the discussion at paragraphs [4.45]-[4.49] and [5.30]-[5.41] of Senate Select Committee on Foreign
Interference through Social Media, Parliament of Australia, Inquiry of the Select Committee on Foreign
_Interference through Social Media (Final Report, August 2023)._


-----

_of expression providing the vehicle for the exchange and development of_
_opinions._

_Freedom of expression is a necessary condition for the realization of the_
_principles of transparency and accountability that are, in turn, essential_
_for the promotion and protection of human rights.[17]_

13. The rights to freedom of opinion and expression are contained in article 19 of the
International Covenant on Civil and Political Rights (ICCPR), to which Australia is a
signatory.[18]

14. Unlike the right to freedom of opinion, the right to freedom of expression is subject to
potential limitation. Under article 19(3) freedom of expression may be limited as
provided for by law and when necessary to protect the rights or reputations of
others, national security, public order, or public health or morals. Limitations must
be prescribed by legislation necessary to achieve the desired purpose and
proportionate to the need on which the limitation is predicated.[19] Article 20 of the
ICCPR also contains further mandatory limitations on freedom of expression.

15. While the Australian Constitution does not recognise an explicit right to freedom of
expression, the High Court of Australia has held that an implied freedom of political
communication exists in recognition of Australia’s system of representative
government established by the Constitution.[20] This is a limited Constitutional
freedom in that its protective scope extends to discussion of ‘government and
political matters’. The implied freedom is not an individual right, but instead restricts
laws that interfere with free communication about government and politics. The
freedom only has practical effect if a properly constituted court determines that
legislation (or arguably an executive decision) disproportionately burdens the
relevant political speech.[21] That is, it may be limited by laws that are reasonably
appropriate and adapted to serving a legitimate end in a manner that is compatible
with Australia’s system of representative and responsible government.[22]

16. The Secretary-General of the United Nations has outlined the difficulty in
appropriately balancing efforts to combat disinformation with the right of freedom of
expression:

_While States have taken a number of helpful steps to counter_
_disinformation, many current efforts to counter disinformation raise_
_significant human rights concerns. Given the challenges in defining_
_disinformation, it is not surprising that some measures adopted by_
_States or companies in recent years to counter disinformation have_
_resulted, whether unwillingly or knowingly, in undue restrictions on_

17 Human Rights Committee, General comment No 34: Article 19: Freedoms of opinion and expression, 102nd
sess, UN Doc CCPR/C/GC/34 (12 September 2011).
18 _International Convention on Civil and Political Rights, opened for signature 16 December 1966, 999 UNTS_
171 (entered into force 23 March 1976) art 19-20.
19 See further, Attorney-General’s Department, Right to freedom of opinion and expression (Public Sector
Guidance Sheet, online) <https://www.ag.gov.au/rights-and-protections/human-rights-and-antidiscrimination/human-rights-scrutiny/public-sector-guidance-sheets/right-freedom-opinion-and-expression>.
20 _Australian Capital Television v Commonwealth (1992) 177 CLR 106; Nationwide News v Wills (1992)_
177 CLR 1.
21 The High Court of Australia has repeatedly recognised in its ‘implied freedom’ decisions that ‘political
speech’ can include unpopular and even ‘fringe’ views: see, eg, Monis v The Queen [2013] HCA 4; Coleman v
_Power [2004] HCA 39._
22 _[Attorney-General for South Australia v Corporation of the City of Adelaide (2013) 249 CLR 1, [67] (French](https://www.alrc.gov.au/publication/traditional-rights-and-freedoms-encroachments-by-commonwealth-laws-alrc-report-129/4-freedom-of-speech/protections-from-statutory-encroachment-23/#_ftnref23)_
CJ).


-----

_freedom of expression … Approaches that seek simple solutions to this_
_complex problem are likely to censor legitimate speech that is protected_
_under international human rights law. Such overbroad restrictions are_
_likely to exacerbate societal ills and increase public distrust and_
_disconnection, rather than contribute to the resolution of underlying_
_problems.[23]_

17. Former United Nations High Commissioner for Human Rights, Michelle Bachelet,
has similarly identified the important balance to be drawn by governments when
implementing reforms to combat disinformation online:

_In taking on the challenges that disinformation poses, we cannot fall into_
_the trap of trying to officially ordain what is false, and what is true, and_
_then attach legal consequences to those determinations. Our human_
_right to access and impart information is not limited to only what is_
_deemed by the State as ‘accurate’.[24]_

18. The Special Rapporteur on the promotion and protection on the right to freedom of
opinion and expression, Ms Irene Khan, has also underlined the need for balance
and caution in this regard.[25] While recognising the particular risks of disinformation
to the enjoyment of human rights, she also underlines that any restrictions must be
appropriate and proportionate to a legitimate aim, using the least restrictive means
to protect it. The prohibition of false information is not in itself a legitimate aim under
international human rights law. Further, the principle of legality requires the scope,
meaning and effect of the law to be sufficiently clear, precise and public.[26] She
recommends that any state regulation of social media should focus on enforcing
transparency, due process rights for users and due diligence on human rights by
companies, and on ensuring that the independence and remit of regulators are
clearly defined, guaranteed and limited by law.[27]

19. In light of the above, the Law Council is of the view that particular care must be
taken when seeking to implement proposals that regulate online activities to ensure
that the protection and respect of individuals’ freedom of expression and freedom to
seek, receive and impart information, is maintained. It is important that any codes or
laws aimed at combating misinformation and disinformation are carefully designed
to balance the public interest in ensuring content posted online is not contributing to
harm, with individuals’ ability to speak freely.

20. The AHRC has highlighted the difficulty in finding this balance:

_Striking the right balance between regulating online activities and_
_protecting free expression is an ongoing challenge. While there is a_
_clear need to combat misinformation and disinformation online, there is_
_also a risk that in doing so different perspectives and controversial_
_opinions may be targeted. While reasonable minds may differ on exactly_

23 _Countering disinformation for the promotion and protection of human rights and fundamental freedoms:_
_Report of the Secretary-General, 77[th] sess, 287[th] mtg, Agenda Item 69(b), UN Doc A/77/287, (12 August_
2022) 11.
24 Michelle Bachelet, UN High Commissioner for Human Rights, ‘High-level panel discussion on countering
the negative impact of disinformation on the enjoyment and realization of human rights’ (Speech, 50th session
of the Human Rights Council, 28 June 2022) <https://www.ohchr.org/en/statements-andspeeches/2022/06/high-level-panel-discussion-countering-negative-impact>.
25 _Disinformation and freedom of opinion and expression, Report of the Special Rapporteur on the promotion_
and protection of the right to freedom of opinion and expression, Irene Khan, UN Doc A/HRC/47/25, 13 April
2021.
26 Ibid, 1-9.
27 Ibid, 18.


-----

_where the line should be drawn, if we fail to ensure robust safeguards for_
_freedom of expression online, then the very measures taken to combat_
_misinformation and disinformation could themselves risk undermining_
_Australia’s democracy and values.[28]_

21. Similar remarks were made by the Parliamentary Joint Standing Committee on
Electoral Matters in its Interim Report on the Conduct of the 2022 federal election
_and other matters:_

_Action must be taken to combat the effects of misinformation and_
_disinformation, but any action must be balanced, so that freedom of_
_political communication is not inhibited or placed at risk. Legislative_
_change must be for a legitimate purpose, proportional, valid and_
_appropriate. Any changes to existing structures, institutions or legislative_
_or regulatory frameworks must therefore be carefully considered.[29]_

22. Much of the practical impact of the Draft Bill will depend on the content of the
misinformation codes/standards when developed. The Law Council notes the
difficulty in providing appropriate and relevant feedback where the relevant
misinformation codes and standards have not yet been developed.

23. However, textual flaws in the Draft Bill which will, in turn, underpin the
misinformation codes/standards when developed, will affect implementation.
In practice, the Draft Bill may be overly incursive on freedom of expression. This
may occur where digital platform services become overly careful in censoring
content on their platform to limit their risk of receiving (potentially significant) fines or
other penalties.[30] It may also occur where platform users become overly cautious in
order to avoid penalties from the platform (for example, being labelled a purveyor of
misinformation or disinformation, or having their account suspended).

### About the Draft Bill

24. The Draft Bill seeks to implement recommendations 3 and 4 of ACMA’s June 2021
_Report to government on the adequacy of digital platforms’ disinformation and news_
_quality measures (ACMA Report),[31] primarily through the introduction of a new_
schedule 9 to the Broadcasting Services Act 1992 (BSA).

#### Record Keeping and Information-Gathering Powers

25. The Draft Bill would allow ACMA to impose digital platform rules which require digital
platform providers to keep records and report on the extent of misinformation and
disinformation on the digital platform, and measures implemented to prevent or
respond to such content.[32] This information may be published online by ACMA.[33]
Further, ACMA may obtain information, documents or evidence from individuals or
digital platform providers relating to the publication of false or misleading online

28 Australian Human Rights Commission, Submission No 9 to Senate Select Committee on Foreign
Interference through Social Media, Parliament of Australia, Inquiry of the Select Committee on Foreign
_Interference through Social Media (16 February 2023) 8._
29 Joint Standing Committee on Electoral Matters, Parliament of Australia, Conduct of the 2022 federal
_election and other matters (June 2023) 105._
30 Senate Select Committee on Foreign Interference through Social Media, Parliament of Australia, Inquiry of
_the Select Committee on Foreign Interference through Social Media (Final Report, August 2023) [5.30]-[5.41]._
31 Australian Media and Communications Authority, Report to government on the adequacy of digital
_platforms’ disinformation and news quality measures (June 2021)._
32 Draft Bill, s 14.
33 Ibid s 25.


-----

content. Non-compliance with the digital platform rules may result in a civil penalty,
an infringement notice, or a formal warning for the digital platform service.[34]

26. Contravention of digital platform rules made in relation to record keeping, or a failure
to comply with remedial directions, may result in the issuance of an infringement
notice and a penalty amount of 60 penalty units ($16,500 in 2023)[35] for corporations
and 10 penalty units ($2,750) for individuals.[36] Digital platform providers that
contravene the digital platform rules, or fail to comply with remedial directions, can
also face civil penalty proceedings and may be fined up to 5,000 penalty units
($1,375,000) for corporations or 1,000 penalty units ($275,000) for individuals for
each day of the contravention.[37]

27. ACMA can also issue an infringement notice to address non-compliance with its
information-gathering powers.[38] Contravention may result in a penalty amount of
8 penalty units ($2,200) for corporations and 6 penalty units ($1,650) for individuals.
ACMA may also seek civil penalties of up to 40 penalty units ($11,000) for
corporations and 30 penalty units ($8,250) for individuals for each day of the
contravention.[39] The Law Council notes that it is difficult to assess the suitability of
digital platform rules which have not yet been drafted.

#### Industry Codes and ACMA Standards

28. The Draft Bill would also empower ACMA to require industry bodies to develop and
register codes (‘misinformation codes’) in relation to the prevention of and
response to misinformation and disinformation, with which relevant digital platform
providers must comply. If an industry-developed code is deemed deficient, ACMA
may then impose mandatory standards (‘misinformation standards’) for digital
platform providers to protect the community from misinformation or disinformation
online.[40] The Draft Bill outlines a number of examples of matters that may be dealt
with by misinformation codes and standards, including prevention of, and response
to, misinformation or disinformation; preventing monetisation or advertising of
misinformation or disinformation; supporting fact checking; and policies and
procedures for managing reports and complaints by end users regarding
misinformation and disinformation.[41]

29. The Draft Bill does not give ACMA the power to force digital platforms to remove
content or posts.[42] The Law Council understands that the proposed legislation
imposes a burden on digital platform providers to implement strategies to respond to
misinformation and disinformation but is not intended to directly censor the content
posted by individual users (although it may have this effect indirectly).

34 Ibid s 15.
35 In this submission, all of the dollar amounts given for various penalty units are for 2023.
36 Draft Bill, s 15(4). Matters to be included in an infringement notice are set out in s 205Z of the Broadcasting
_Services Act 1992 (Cth) and the amount of a penalty is set out in s 205ZA. See also, Guidance Note,_
Exposure Draft, Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill
2023 (June 2023) 24 (‘Guidance Note’).
37 Ibid ss 15(2)-(3), 16(4)-(5). See also, Guidance Note, 24.
38 Ibid ss 18(8) and 19(8). See also, Guidance Note, 24.
39 Ibid ss 18(6)-(7) and 19(6)-(7). See also, Guidance Note, 24.
40 Ibid ss 48-49.
41 Ibid s 33.
42 Guidance Note, 7.


-----

30. The Law Council notes the ‘parliamentary intention’—as set out in the proposed
amendment to insert a new subsection 4(3AC) into the BSA—that:

_… digital platform services be regulated, in order to prevent and respond_
_to misinformation and disinformation on the services, in a manner that:_

_(a)_ _has regard to freedom of expression; and_

_(b)_ _respects user privacy; and_

_(c)_ _protects the community and safeguards end-users against harm_
_caused, or contributed to, by misinformation and disinformation on_
_digital platform services; and_

_(d)_ _enables public interest considerations in relation to misinformation_
_and disinformation on digital platform services to be addressed in a_
_way that does not impose unnecessary financial and administrative_
_burdens on digital platform providers; and_

_(e)_ _will readily accommodate technological change; and_

_(f)_ _encourages the provision of digital platform services to the_
_Australian community; and_

_(g)_ _encourages the development of technologies relating to digital_
_platform services.[43]_

31. However, the potential penalties for failing to comply with the misinformation codes
or standards are significant. ACMA will be empowered to issue infringement notices
for contraventions of the codes or standards with penalty amounts of a maximum of
60 penalty units ($16,500) for corporations or 10 penalty units ($2,750) for
individuals.[44] ACMA may also seek civil penalties for breaches of the misinformation
codes or standards.[45] The Guidance Note identifies that it ‘is expected that ACMA
will actively seek penalty orders against those providers who routinely contravene
provisions in a registered code or a standard, or fail to comply with remedial
directions in particular’.[46] The maximum civil penalties in this context include:

   - non-compliance with a registered misinformation code: 10,000 penalty units
($2.75 million) or 2 per cent of global turnover (whichever is greater) for
corporations or 2,000 penalty units ($0.55 million) for individuals; and

   - non-compliance with a misinformation standard: 25,000 penalty units
($6.88 million) or 5 per cent of global turnover (whichever is greater) for
corporations or 5,000 penalty units ($1.38 million) for individuals.[47]

43 See Draft Bill sch 2 s 7 which proposes to insert a new s 3AC into the Broadcasting Services Act 1992
(Cth).
44 Draft Bill ss 43(3) and 53(3). See further, Guidance Note, 25.
45 Ibid ss 43(2) and 53(2). See further, Guidance Note, 25.
46 Guidance Note, 25
47 Ibid.


-----

32. The Guidance Notes state that the intention of the significant maximum penalties is
to ‘deter systemic non-compliance by digital platform services and reflects the
serious large scale social, economic and/or environmental harms and
consequences that could result from the spread of misinformation or
disinformation’.[48]

33. However, the Law Council is aware of concerns that these penalties may lead to
digital platform services becoming overly careful in censoring content on their
platform to limit their risk of receiving (potentially significant) fines or other
penalties.[49] This may occur in particular where compliance with the misinformation
codes or standards is made difficult by broad and imprecise definitions of key
concepts

34. While there is a need to address false or misleading digital content which may cause
harm, the Law Council is concerned that the practical effect of the regulation
proposed by the Exposure Draft is a disproportionate response to the risk, in
recognition that the Draft Bill is broad and imprecise in its terminology, may result in
confusion in its application, and is likely to impact on the freedom of expression and
privacy of Australians.

### Specific textual comments

#### Information-gathering powers

35. In principle, the Law Council acknowledges that the power to gather information
from digital platform providers or require digital platform providers to keep records
regarding misinformation and disinformation could provide greater transparency
around the extent of misinformation and disinformation on digital platforms.
Empowering ACMA to publish information on its website relating to misinformation or
disinformation regulation, is one way to combat the issue. Data evidencing the
prevalence of such content could also prove valuable for research and
policy-development.

36. However, the Draft Bill provides ACMA with significant coercive informationgathering powers that can be exercised against any person who might have
information or documents ‘relevant’ to the existence of, among other things,
‘misinformation or disinformation on a digital platform service’.[50] While the Guidance
Note suggests that the target of these powers might be ‘fact-checkers or other
third-party contractors to digital platform providers’, they are not limited in this way.[51]
For example, suspected authors or disseminators of alleged ‘misinformation’ could
be subject to the use of the proposed information-gathering powers.

37. This part of the Draft Bill is unique within its overall scheme as it is concerned with
the responsibilities of individuals, rather than services providers.

48 Ibid.
49 See, eg, Senate Select Committee on Foreign Interference through Social Media, Parliament of Australia,
_Inquiry of the Select Committee on Foreign Interference through Social Media (Final Report, August 2023)_

[5.30]-[5.41].
50 Draft Bill, s 19.
51 Guidance Note, 15.


-----

**Privilege against self-incrimination and self-exposure to penalties**

38. Unlike Part 13 of the BSA, which contains equivalent information-gathering powers
related to broadcasters, an individual called to provide information or evidence, or
produce a document under the new proposed powers, is not entitled to refuse to do
so on the basis that to comply might incriminate them or expose them to a penalty.

39. The Bill’s abrogation of the privilege against self-incrimination is purportedly
counterbalanced by the protections offered against direct and derivative use of any
compelled disclosures.[52]

40. The Law Council is concerned there is no such protection offered in consideration of
an individual’s privilege against exposing themselves to a civil penalty.[53] There is no
attempt to justify this complete and unqualified abrogation of penalty privilege, which
itself is rooted in the important idea of ensuring that those who allege criminality or
other illegal conduct should prove it.

41. The failure to offer any protections consequent upon the abrogation of penalty
privilege could lead to a position where a person is compelled to produce
documents that later expose them to significant civil penalties. This ought not to be
where there is no apparent reason, and no reason is apparent for not offering direct
and derivative protections against the use of information that might expose an
individual to civil penalty. It is not clear why reforms primarily intended to allow
ACMA to monitor and improve the responses of digital platform providers to
misinformation and disinformation, would need to curtail this privilege, including for
users of the platforms unrelated to the operation of the platforms.

42. The Law Council notes that, under Part 13 of the BSA, journalists cannot be
compelled to reveal their sources.[54] The same protection is not provided under
Part 2 of the proposed Schedule 9. Given the potential scope of ACMA’s
information-gathering powers, that protection should be included.

**Privacy and data security**

43. The Law Council notes that the proposed information-gathering powers could raise
privacy concerns, particularly where sensitive user data is collected or stored.
Although proposed section 27 of the Draft Bill outlines an obligation on ACMA to not
publish information if the information would meet the definition of ‘personal
information’ under the Privacy Act 1988 (Cth), the Law Council recommends the
further inclusion of a legislative requirement for sensitive user data to be
de-identified, where possible, and stored or accessed in strict compliance with
Australia’s privacy framework.

44. The Law Council highlights that the imposition of record-keeping requirements on
digital platform providers may impose significant administrative and financial
burdens, and may also increase data security risks if not properly secured.

52 Draft Bill s 21.
53 Ibid s 21(3).
54 See Broadcasting Services Act 1992 (Cth) s 202(4)).


-----

#### Definitions of ‘misinformation’ and ‘disinformation’

45. At the heart of the difficulties presented by the Draft Bill are the definitions of
‘misinformation’ and ‘disinformation’.

46. ‘Misinformation’ is defined to include content provided on the digital service that
‘contains information that is false, misleading or deceptive’, where ‘the content is not
excluded content for misinformation purposes’ and is ‘reasonably likely to cause or
contribute to serious harm’.[55]

47. Disinformation’ is similarly defined, but includes the added requirement that ‘the
person disseminating, or causing the dissemination of, the content intends that the
content deceive another person’.[56]

48. ‘Excluded content for misinformation purposes’ is defined under the Draft Bill[57] and
is discussed below.

49. ‘Serious harm’ is not defined under the Draft Bill, although the Draft Bill outlines a
number of factors which should be considered in determining whether content is
reasonably likely to cause or contribute to serious harm.[58] ‘Harm’ is defined.[59]
These provisions are also discussed below.

50. The relevant definitions raise at least three conceptual issues. The first is the
substantive breadth of the definitions, including the concept of ‘harm’. The second
is the limited nature of the exemptions that take content outside the definition of
‘misinformation’. The third is the statutory supposition that misinformation (however
defined) is identifiable as such; and is capable of being so identified by ACMA (or
indeed the service providers whom the Bill effectively requires to monitor the content
published via their services).

51. The definitions of ‘misinformation’ and ‘disinformation’ are fundamental to the
operation of the Draft Bill, particularly in the face of the suggestion in the Fact Sheet
accompanying the Bill that industry ‘does not need to adopt definitions in the Bill’. In
a specific sense, the scope of almost all obligations under the Draft Bill and the
concomitant scope of ACMA’s powers are hinged upon the concepts of
‘misinformation’ and ‘disinformation’.[60] In a more general sense, those concepts
define the scope of the ‘mischief’ that the statute purports/aims to remedy, and thus
will inform the interpretation of every provision of the Draft Bill. It is for these
reasons that the problems with concepts of ‘misinformation’ and ‘disinformation’ are
fundamental to the Draft Bill’s justifiability.

**Breadth of the definition of ‘misinformation’**

52. There are at least five principal respects in which the statutory definition of
‘misinformation’ can be regarded as overly broad and difficult to apply in practice.

53. First, the statutory definition requires a distinction to be drawn between ‘information’
and other forms of online content. What ‘information’ means in this context is
unclear, but it is unlikely to be limited to ‘positive claims about the truth of identified

55 Exposure Draft, Communications Legislation Amendment (Combatting Misinformation and Disinformation)
Bill 2023 (Cth) (‘Draft Bill’) s 7(1).
56 Ibid s 7(2).
57 Ibid s 2.
58 Ibid, s 7(3).
59 Ibid s 2.
60 See, eg, ss 4(1)(c), 18(2)(a) and 25(1).


-----

facts’. Much online content involves combinations of fact, opinion, commentary or
invective. Speech about political, philosophical, artistic or religious topics often
involves statements that are not straightforwardly ‘factual’, but which are not mere
statements of subjective belief. Much scientific discourse involves the testing and
rejection of hypotheses, in which even ‘true’ information is provisional or falsifiable.

54. The prospect that digital platform providers and ACMA will be required to sift
‘information’ from ‘opinion’ or ‘claims’ is itself likely to have a chilling effect on
freedom of expression; especially in sensitive or controversial areas. The effect
may be particularly pernicious if a regulator or platform is tempted to be
over-inclusive about what counts as ‘information’ rather than ‘opinion’. The risk is
that disfavoured opinions might come to be labelled and regulated as
‘misinformation’ (i.e., as misleading facts, and not as opinions).

55. Second, the statutory concept of ‘misinformation’ in the Draft Bill involves
information that is false, misleading or deceptive; not merely information that is
alleged or suspected to be so, or that is so in the opinion of a decisionmaker. The
internet contains a vast amount of information, and the Draft Bill is not confined to
information authored by Australians. The burden of identifying which of that
worldwide information is ‘misinformation’ is likely to be significant.

56. Third, the definition of ‘misinformation’ is overbroad, in that it is not confined to
straightforward positively false statements of fact. The existing law of misleading or
deceptive conduct in trade or commerce makes clear that conduct will infringe the
statutory norm in a very wide range of circumstances, particularly because the
concept of ‘misleading’ information is much broader than ‘false’ information. Here, it
is immaterial that the Draft Bill uses the language of ‘information’ rather than
‘conduct’. The heartland of misleading or deceptive conduct under existing law is
conduct that conveys inaccurate information to a recipient. Accordingly, the drafting
of the Bill is likely to encompass not merely positive false statements, but also:

(a) information that is partial or incomplete;[61]

(b) information that is silent about some relevant contextual matter;[62]

(c) information that is capable of two or more reasonable readings, only one of
which is misleading;[63]

(d) information that is literally true but that may be said to be rendered misleading
by its context;[64]

(e) information that is later rendered inaccurate by subsequent events, where the
author fails to correct the initial impression;[65] and

(f) information that causes harm to a person other than the person who is
misled.[66]

61 _Miller & Associates Insurance Broking Pty Ltd v BMW Australia Finance Ltd (2010) 241 CLR 357, [23]_
(French CJ and Kiefel J).
62 Ibid. See also Demagogue Pty Ltd v Ramensky (1992) 39 FCR 31.
63 _Tobacco Institute of Australia Ltd v Australasian Federation of Consumer Organisations Inc (1992) 38 FCR_
1, 5, 27.
64 _Porter v Audio Visual Promotions Pty Ltd (1985) ATPR 40-547._
65 _Winterton Construction Pty Ltd v Hambros Australia Ltd (1992) 39 FCR 97, 114; Thong Guan Plastic and_
_Paper Industries SDN BHD v Vicpac Industries Australia Pty Ltd [2010] VSC 11, [123]–[125]._
66 _Janssen-Cilag Pty Ltd v Pfizer Pty Ltd (1992) 37 FCR 526._


-----

57. Fourth, there is no content-based limit on the definition of ‘misinformation’. It is not,
for example, confined to information about health, finances, the environment, or the
democratic process. Whether any given information is ‘reasonably likely’ to
‘contribute’ to ‘serious harm’ of the kinds specified in the Draft Bill is a complex
interpretative question which might not readily be determined by the apparent
character of the information standing alone.

58. Fifth, the statutory definition labels content as ‘misinformation’ if it contains
information that is false, misleading or deceptive—the ‘misinformation’ is not merely
the false, misleading or deceptive information itself. There is no statutory
requirement that the content substantially consist of false, misleading or deceptive
information. This raises the prospect that the statutory category of ‘misinformation’
may be overly-inclusive.

59. Given the breadth of the definition of ‘misinformation’ as outlined above, the Law
Council cautions that a digital platform is unlikely to have sufficient expertise or
adequate resources to make accurate and completely informed determinations as to
whether content is false and, therefore, may choose to censor significant amounts of
information in order to ensure compliance and avoid incurring substantial fines. This
would, in turn, have a significant impact on freedom of expression.

**The concept of ‘excluded content’ is insufficiently protective of freedom of**
**expression**

60. The Draft Bill excludes certain categories of content disseminated online from
regulation. ‘Excluded content for misinformation purposes’ is defined to include:

   - content produced in good faith for entertainment, parody or satire;

   - professional news content;

   - content produced by or for accredited Australian or international education
providers; and

   - content authorised by Commonwealth, state, territory and local governments.[67]

61. The definition of ‘excluded content for misinformation purposes’ does not sufficiently
protect freedom of expression. The Law Council notes with particular concern that
there is no general exclusion of content that involves reasonable scientific,
academic, political, artistic or religious discussion, including factual disagreements in
respect of those topics. There is no general recognition in the Draft Bill that many
topics of public interest involve genuine and legitimate factual disagreement,
uncertainty or debate.

62. The exclusion in proposed subclause (a) (‘entertainment, parody or satire’) is
unsatisfactory and under-inclusive. Significantly, this is the only exclusion that
identifies content by its substance or character, rather than its provenance. Many
socially valuable forms of expression are not readily identified as ‘entertainment,
parody or satire’, including serious artistic expression, criticism and review, or
religious speech. Equally, the line between ‘entertainment’ and ‘information’ falling
within the concept of ‘misinformation’ is not clear given the prevalence of so-called
‘infotainment’.

67 Draft Bill, s 2 (definition of ‘excluded content for misinformation purposes’).


-----

63. The exclusion in proposed subclause (c) (‘content produced by or for an [accredited]
educational institution’) appears to be inadequate, and may lead to significant
inroads into academic freedom. First, even where an individual academic speaks or
writes for a professional purpose, and within their field of expertise, it is not
self-evident that such ‘content’ is produced ‘by or for an educational institution’.
The requirement for institutional endorsement is inconsistent with academic
freedom. Universities generally insist on the distinction between the views—
perhaps diverse, conflicting or controversial—expressed by individual academics on
the one hand, which are not taken to be the view of the university as an institution,
and the (more limited) views or policies expressed by the institution itself on the
other. Further consideration of this proposed exclusion (alongside the ‘authorised
content’ exclusion) may be needed, including by reference to recent measures
adopted under the Higher Education Support Amendment (Freedom of Speech) Act
_2020 (Cth) to promote and protect freedom of speech and academic freedom.[68]_

64. Additionally, the concept of ‘educational institution’ is itself unsatisfactory and
requires clarification. For example, it is not clear whether research institutes or
think-tanks that are not ‘accredited’ as ‘educational institutions’ would fall within this
definition.

65. The exclusion in proposed subclause (d) (foreign institutions accredited ‘to
substantially equivalent standards as a comparable Australian educational
institution’) is troublesome in two interrelated respects. Educational institutions exist
in many societies that do not share Australia’s understandings of the rule of law and
academic freedom. For example, there are a number of long-established public
research universities that are notoriously subject to ideological pressure from
authoritarian governments. Depending on the interpretation of ‘substantially
equivalent standards’, it could be argued that the views of foreign governments that
are false or misleading could be excluded from the definition of ‘misinformation’
where they are ‘filtered’ through a regime-friendly educational institution. It may also
be difficult to determine what ‘substantially equivalent standards’ means given that
educational institutions exist in a variety of different forms, which may be unfamiliar
in Australia.

66. The Law Council is aware of particular concern regarding the exclusion in proposed
subclause (e) (content that is authorised by a government). In accordance with this
exception, views authorised by government are automatically protected from
designation as ‘misinformation’. Yet the views of critics of government (whether the
political opposition, NGOs or private individuals) are at risk of precisely such a
designation. The Guidance Note provides a relatively benign example of social
media by a state transport department about an upcoming road project or health
campaign. However, it is where the views of a government are particularly
controversial or contestable that this exception, as drafted, risks making significant
inroad into freedom of expression.

68 As a result of which there are requirements upon higher education providers to have a policy that upholds
freedom of speech and academic freedom under the Higher Education Support Act 2003 (Cth), s 19-115. See
also this Act’s objects, which include supporting a higher education system that promotes and protects
freedom of speech and academic freedom: s 2-1(a)(iv).


-----

**The definition of ‘harm’ is overbroad and does not sufficiently limit the concept of**
**‘misinformation’**

67. The concept of harm is critical to the intention of the Draft Bill. The Department
noted in a hearing before the Senate Select Committee on Foreign Interference
through Social Media that the intent of the Bill is to:

_… tackle content that is reasonably likely to cause or contribute to_
_serious harm. That, in a sense, is the intent of the bill … for digital_
_platforms to take steps and responsibility for the content on those_
_platforms and, in doing so, take steps to address content that they judge_
_could be likely to cause serious harm. So the focus is on the harm and_
_the content rather than the intent or the source of the content.[69]_

68. The Draft Bill outlines that provision of the misinformation or disinformation on the
digital platform must be ‘reasonably likely to cause or contribute to serious harm’
under proposed section 7. The Law Council acknowledges that the serious harm
threshold enables digital platforms and ACMA to prioritise content that is perceived
as more dangerous to individuals, a group of people, or the public at large.
However, as noted, while ‘harm’ is defined in the Draft Bill, ‘serious harm’ is not.
Rather, the Draft Bill outlines a number of factors which should be considered in
determining whether content is reasonably likely to cause or contribute to serious
harm.[70]

69. Under proposed section 2, ‘harm’ is defined as including any of the following:

(a) hatred against a group in Australian society on the basis of ethnicity,
nationality, race, gender, sexual orientation, age, religion, or physical or mental
disability;

(b) disruption of public order or society in Australia;

(c) harm to the integrity of Australian democratic processes, or of Commonwealth,
State, Territory, or local government institutions;

(d) harm to the health of Australians;

(e) harm to the Australian environment; or

(f) economic or financial harm to Australians, the Australian economy, or a sector
of the Australian economy.[71]

70. The Law Council is concerned that the definition of ‘harm’ in the Draft Bill is
overbroad, especially when read in light of the definition of ‘misinformation’, under
which material is caught not merely when it in fact causes serious harm (however
defined) but also when it is only ‘reasonably likely’ to do so; or when it might only
‘contribute to’ such harm.[72] The width of that definition is significant, given that the
concept of ‘harm’ and ‘serious harm’ each involve value judgments that are likely to
be contestable and politically sensitive. Given that the existence of ‘harm’ is the
only substantive differentiation between ‘misinformation’ (as defined) and any other

69 Richard Windeyer, Deputy Secretary, Communications and Media Group, Department of Communications,
_Committee Hansard (12 July 2023) 25, cited in Senate Select Committee on Foreign Interference through_
Social Media, Parliament of Australia, Inquiry of the Select Committee on Foreign Interference through Social
_Media (Final Report, August 2023) 75._
70 Ibid, s 7(3).
71 Ibid s 2 (definition of ‘harm’).
72 Ibid s 7(1)(d).


-----

false, misleading or deceptive information that exists in the world, it is important that
the definition be clear, sufficient, and easy to apply.

71. Proposed subclause (a) (‘hatred against a group in Australian society’) identifies a
matter that is already captured (at least to some extent) by anti-discrimination and
anti-vilification laws, but without the calibrated exemptions to protect (for example)
artistic, academic and religious freedom that those laws typically contain.

72. Proposed subclause (b) (‘disruption of public order or society in Australia’) is
overbroad, and may capture valuable behaviour such as encouraging the lawful
exercise of the right of public assembly and peaceful protest.

73. Proposed subclause (c) (‘harm to the integrity of Australian democratic processes’)
is vague and overbroad, and is best met by specific legislation if existing electoral
law is shown to be inadequate (for example about campaign financing, or electoral
publications).

74. Proposed subclause (d) (‘harm to the health of Australians’) is overbroad, and
insufficiently calibrated. Even where scientific opinion is clear, there is a wide
continuum of conduct that involves harm to health. At the same time, it is unclear
why the health of ‘Australians’ would be the determining factor, noting that there are
also large numbers of persons resident or present in Australia (non-citizens) whose
health also deserves consideration.

75. Proposed subclause (e) (‘harm to the Australian environment’) is overbroad, and
insufficiently calibrated. The complexity of current environmental protection
legislation, and the frequent length and complexity of the proceedings in which that
legislation is applied, suggests that it may not be easy to identify what is, in fact, a
harm to the environment. There may also be competing environmental goods.

76. Proposed subclause (f) (‘economic or financial harm to Australians, the Australian
economy or a sector of the Australian economy’) is overbroad, and insufficiently
calibrated. ‘Economic or financial harm to Australians’ may arise from any number
of causes. The subclause is not, for example, confined to fraudulent conduct, or
conduct causing material financial loss to a significant number of people.
An individual Australian might experience ‘economic or financial harm’ by being
encouraged to spend money needlessly on expensive brand-name clothes, or by
being encouraged to make legitimate but unnecessarily conservative investment
choices, just as much as by being encouraged to invest in a Ponzi scheme.
A significant amount of substantively unobjectionable commercial
(and non-commercial) communication is likely to be caught by this aspect of the
definition of harm.

77. Under proposed section 7(3) of the Draft Bill, the factors that should be considered
to determine whether the content is reasonably likely to cause or contribute to
‘serious harm’ include:

(a) the circumstances of dissemination;

(b) the subject matter of the content;

(c) the potential reach and speed of dissemination;

(d) the severity of the potential impacts;

(e) the author of and purpose for the dissemination;


-----

(f) the correct attribution of a source for the information; and

(g) any other relevant matter.

78. In the Law Council’s view, the definition of ‘harm’ is not improved by the inclusion of
these contextual factors for determining ‘serious harm’. They repose significant
discretion in an executive decision-maker, including by making judgements in
respect of favoured and disfavoured ‘authors’ or ‘purposes’, without any express
obligation to have regard to freedom of expression, privacy, broader human rights or
any other countervailing public interest criteria.

79. Further, the Law Council submits that the Draft Bill should more clearly define the
concept of ‘serious harm’ to reduce ambiguity for regulatory purposes and ensure
effective enforcement of the misinformation codes or standards.

**The definition of ‘disinformation’ replicates and extends the difficulties Inherent in**
**the concept of ‘misinformation’**

80. The concept of ‘disinformation’ embeds the same difficulties that are inherent in the
definition of ‘misinformation’, with the additional problems caused by the
requirement that ‘the person disseminating, or causing the dissemination of, the
content intends that the content deceive another person’. Two difficulties are of
particular importance.

81. First, the Law Council queries by what means it will be determined that the
disseminator ‘intend[ed] that the content deceive another person’.[73] The mere
intentional act of dissemination will not suffice—proof of intention to deceive will be
needed. That will not often be apparent or inferable from the face of the allegedly
misleading content. In the absence of coercive powers and the safeguards of the
judicial process, people are not ordinarily compelled to disclose their unexpressed
intentions,[74] especially when what is alleged against them is actual deceit.

82. Second, the disseminator of content need not be its author. An author’s innocent
error may be misleading, and their content may amount to ‘misinformation’
(as defined) by reason of that innocent mistake. The content might then be
disseminated by other innocent people who are ignorant of the error. If the content
is thereafter disseminated by a malicious person who intends to deceive others,
there is a risk that the pejorative label of ‘author and disseminators of disinformation’
will be applied to innocent people. Given that the observable conduct involved in
innocent authorship, innocent dissemination and deceitful dissemination is the same
(namely, transmission of particular information), there is a real risk of over-inclusion
in any regulatory investigation into those people’s intentions and, hence, the
existence of ‘disinformation’.

73 Draft Bill, s 7(2).
74 As it appears may occur under section 19 of the Draft Bill.


-----

**Identification of ‘misinformation’ and ‘disinformation’**

83. The statutory scheme of the Draft Bill presupposes that misinformation is an
identifiable category of online material. This is inherent in the definitions of
‘misinformation’ and ‘disinformation’, which do not depend on the mere existence of
allegation, suspicion, or executive opinion that information meets the statutory
definition. Equally, it is inherent:

(a) in proposed sections 14(1)(c), 18(2)(a), 19(2)(a) and 25(1)(a), about regulating
or reporting the existence of ‘misinformation or disinformation on digital
platform services’;

(b) in proposed sections 14(1)(d), 18(2)(b), 19(2)(b) and 25(1)(b), about the
‘effectiveness’ of measures ‘to prevent or respond to misinformation or
disinformation on digital platform services’; and

(c) in proposed sections 14(1)(e), 18(2)(c), 19(2)(c) and 25(1)(c), which suppose
that it is possible to form a meaningful judgement about the ‘the prevalence of
content containing false, misleading or deceptive information’.

84. Each of these, by definition, involves an objective assessment that such content
exists.

85. The discussion at paragraphs 60-79 highlights the inherent difficulties with
appropriately identifying whether information is false, misleading or deceptive,
reasonably likely to cause or contribute to ‘serious harm’ and not subject to an
exception. As a result of this burden, and the potential for significant penalties, there
is a real risk of over-cautiousness by digital platform services, in turn limiting the
freedom of expression of users.

86. The statutory scheme means that ACMA is the ultimate decision-maker about what
is, or is not, misinformation (or disinformation), subject only to the (unexpressed)
possibility of limited judicial review in the federal courts. There are three
fundamental problems with these statutory presuppositions.

87. First, the broad definition of ‘misinformation’ requires the decision-maker to
distinguish ‘information’ (whether misinformation or not) from all other online
content, such as opinion, criticism, political commentary, creative writing, religious
expression or invective. It requires identification of the ‘true’ position against which
the alleged misinformation is shown to be false, misleading or deceptive. That is
because the statutory definitions do not concern material that is merely alleged,
suspected or believed in the opinion of the decision-maker to be misinformation.
Given the vast amount of material available online on digital platform services, each
of these aspects of the task of identifying ‘misinformation’ would be significantly
burdensome on ACMA and the digital platform services. This is especially so in light
of the High Court’s recognition of the ‘considerable difficulty’ of discerning what is,
and what is not, misleading and deceptive.[75] This issue goes to the proportionality
of the Draft Bill.

88. Second, it is not clear what justifies the statutory presupposition that ACMA and the
digital platform services will have the expertise and resources to identify and
distinguish ‘misinformation’ from other forms of online content. Taking only recent
examples of contestable online claims, these organisations may not be well-placed
to identify the economic cost-benefit analysis of major sporting events; the biological

75 _Parkdale Custom Built Furniture Pty Ltd v Puxu Pty Ltd (1982) 149 CLR 191, 197 (Gibbs CJ)._


-----

origin of novel viruses; the efficacy of newly developed medical techniques; the
extent of corruption on the part of foreign politicians; or the strategic motivations of
the protagonists in major geopolitical events— to provide simply a few examples.
They may also, in the absence of a federal human rights charter—which establishes
domestic norms of Australia’s human rights obligations, including an understanding
of how relevant rights intersect and may (in certain instances) be limited—be unable
to make informed judgments about the proportionality of limitations on the freedom
of expression or rights to privacy, alongside the rights to life or to the enjoyment of
the highest attainable standard of physical and mental health.

89. Third, the everyday experience of the courts or commissions of inquiry shows that
discerning truth from falsehood in a procedurally fair manner may be an elaborate,
costly and time-consuming process. The statutory supposition that this can be done
readily, uncontroversially, and with little effort by ACMA or by digital platform services
seems unrealistic in light of real-life experiences, for example:

(a) the truth (or otherwise) of allegations of war crimes committed in
Afghanistan;[76]

(b) the truth (or otherwise) of allegations of financial exploitation of Aboriginal
people in remote communities;[77]

(c) the truth (or otherwise) of allegations of inadequate medical care in psychiatric
hospitals;[78]

(d) the truth (or otherwise) of allegations that widely used medical devices were
unsafe.[79]

### Conclusion

90. The Law Council recognises that there is global recognition that misinformation and
disinformation can result in significant harms to the enjoyment of human rights in
particular contexts. However, this is a highly complex topic requiring a cautious
regulatory response.

91. The Law Council considers that the Draft Bill as proposed (recognising that it is an
Exposure Draft and an important means of testing stakeholder views) is overly
broad, uncertain, and may have serious unintended consequences.

92. The Law Council recommends and would be pleased to engage further with the
Australian Government on the issues canvassed above.

76 _Roberts-Smith v Fairfax Media Publications Pty Ltd (No 41) [2023] FCA 555. Cf Inspector-General of the_
_Australian Defence Force Afghanistan Inquiry Report (2020)._
77 _Australian Securities and Investments Commission v Kobelt (2019) 267 CLR 1._
78 _Herron v HarperCollins Publishers Australia Pty Ltd (2022) 292 FCR 336._
79 _Ethicon Sàrl v Gill (2021) 288 FCR 338._


-----

