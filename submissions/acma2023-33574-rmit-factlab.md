## Communications Legislation Amendment (Combatting Misinformation and
 Disinformation) Bill 2023

 Submission by RMIT FactLab

 August 2023


-----

This is a submission by RMIT FactLab in response to the draft Communications Legislation
Amendment (Combatting Misinformation and Disinformation) Bill 2023.

# Introduction
RMIT FactLab is a fact checking organisation and research hub dedicated to fact checking
viral misinformation and disinformation, and building critical awareness of the real-world
harm that they can cause.

There has been a significant increase in misinformation and disinformation spread through
social media platforms in Australia, particularly since the start of COVID-19 pandemic in
January 2020. Polarising narratives borrowed from the US and rapid developments in
artificial intelligence (AI) are exacerbating an already highly polluted online information
ecosystem. Research shows that people are finding it increasingly difficult to distinguish
between accurate, trustworthy information, and mis and disinformation. This problem
requires urgent attention as it has the potential to cause real-world harm, including
undermining democratic processes such as casting an informed vote during elections and
more recently engaging in meaningful debate ahead of the Voice to Parliament referendum.

We believe measures taken to combat misinformation and disinformation must achieve a
healthy balance between the right to free speech in Australia and community protection
against the harmful effects caused by the viral spread of false information.

# Who we are
RMIT FactLab’s work is disseminated through three channels:

  - **Fact Checks: our fact checking team works in partnership with Meta as independent**

third-party fact checkers (3PFC) who identify, review and rate content posted on
Facebook and Instagram.

  - **CrossCheck: researchers monitor and verify online content for media and community**

partners and engage in media literacy work.

  - **CheckMate: a weekly newsletter that disseminates the latest fact checks and**

analysis.

RMIT FactLab was established in January 2022 and is accredited by the International Fact
Checking Network. Since then its fact checking team has produced 122 fact check articles
[that are published on RMIT University’s website.](https://www.rmit.edu.au/about/schools-colleges/media-and-communication/industry/factlab/debunking-misinformation)

In late 2022, CrossCheck joined RMIT FactLab to continue the mission of First Draft (now at
the Information Futures Lab, Brown University). It monitors narratives, identifies data
deficits and ‘pre-bunks’ emerging narratives to help empower communicators to strengthen
their communities and audiences.


-----

[RMIT FactLab works alongside RMIT ABC Fact Check, which is a separate collaboration](https://www.abc.net.au/news/factcheck)
between RMIT and the ABC, established in 2017 to fact check claims by public figures.

# Draft legislation
The bill would give the Australian Communications and Media Authority (ACMA) reserve
powers to act, if industry efforts to tackle misinformation and disinformation are
inadequate. In summary, the proposed powers would enable ACMA to:

  - gather information from or require digital platform providers to keep certain records

about matters regarding mis and disinformation. It may also request information
from fact-checkers or other third-party contractors to digital platform providers;

  - request industry to develop a code of practice covering measures to combat mis and

disinformation on digital platforms, which the ACMA could register and enforce;

  - create and enforce an industry standard (a stronger form of regulation), should a

code of practice be deemed ineffective in combatting misinformation and
disinformation on digital platforms.

This submission seeks to address key definitions in the draft bill; how the digital platforms
industry may be able to operationalise the bill and various content exemptions; and whether
the proposed legislation strikes an appropriate balance between free speech and protection
against harmful content on digital platforms.

# Definitions
**Misinformation and disinformation**
The bill uses the following definitions:

Misinformation is online content created without an intent to deceive but can cause and
contribute to serious harm.

Disinformation is misinformation that is intentionally disseminated with the intent to
deceive or cause serious harm.

These definitions broadly align with those used RMIT FactLab. But the legislation is unclear
on who will decide what constitutes misinformation and disinformation. ACMA has made it
clear that it will not decide what constitutes mis and disinformation, nor request the
removal of individual pieces of content. It is assumed, then, that the job of assessing what
constitutes mis and disinformation will be left to the providers of digital platforms. This is
unlikely to align with community expectations.

Furthermore, if the ACMA is charged with ensuring a code of practice is met by digital
platform providers, it will necessarily have to assess what constitutes mis and
disinformation. The ACMA will, at least at an underlying level, have to decide what
constitutes mis and disinformation.


-----

The proposed legislation must make it clear who will decide what constitutes misinformation
and disinformation.

**Serious harm**
Misinformation and disinformation in the bill are intended to capture content on digital
platform services that is “reasonably likely to cause or contribute to serious harm.”

Serious harm is defined as harm that affects a significant portion of the Australian
population, economy or environment, or undermines the integrity of an Australian
democratic process.

There are considerable value judgements that would have to be made to decide what is
“reasonably likely”. Also, what would happen in the event that digital service providers, fact
checkers and the ACMA, all take different positions on what is “reasonably likely”?

Furthermore, examples of serious harm provided include “Harm to the integrity of
Australian democratic processes or of the Commonwealth, State, Territory or local
government institutions.” While it might be easier to define false information that
undermines the impartiality of an electoral management body ahead of an election or
referendum, how is harm that unfolds over a longer period of time to be assessed?

As fact checkers we encounter numerous social media posts that contain misinformation
that erodes trust in democratic processes. Each post, in and of itself, may not cause serious
harm, but the sheer volume of such posts, produced, over month could be just as damaging.
For example, social media posts that make false claims about an election or a referendum
being rigged or being faulty, could undermine public trust if such posts spread widely, rapidly
and repeatedly over a period of months ahead of voting day. What provision does the bill
make for slow-burning “serious harm”?

# Exemptions
The proposed bill states that some content is excluded for mis and disinformation purposes.
That is:

  - professional news content

  - content authorised by governments

  - content produced in good faith for the purposes of entertainment, parody or satire

  - content produced by or for an accredited education provider.

**Professional news content**
Under this category the content is defined by its provenance, rather than the actual
information it contains. We are concerned that professional news content is to be excluded
from the scope of the bill.

Professional news organisations who sign up to ethical codes and have formal complaints
procedures, such as daily newspapers, television broadcasters and online newspapers,
regularly increase the reach of their news content through social media platforms. The
community expects that these news outlets produce high quality news.


-----

But as fact checkers it is not uncommon for us to debunk false or misleading content posted
on social media platforms by professional news organisations. Whether it is using photos in a
misleading manner or using quotes out of context or articles that rely on inaccurate data,
there are many of examples that we believe have the potential to cause serious harm in
some way, shape or form. Indeed, we have found that misinformation posted on social
media by some professional news organisations can fuel further mis and disinformation,
with conspiracy theorists using professional news content to legitimise their campaigns.

Furthermore, excluding professional news content has the potential to lead to confusion
over what counts as mis and disinformation. If an ordinary person makes a false claim in a
social media post, it could be deemed to be misinformation. But the same information, if
posted by a news organisation, would not be considered misinformation. If an ordinary
person re-posts false information posted by a professional news organisations, is this
person’s post to be excluded too?

Lastly, and worryingly, it is also unclear whether smaller, independent digital news outfits
would be excluded under the proposed bill. These so-called professional news organisations
often claim to provide the real “news”. They do not abide by generally accepted norms of
Australian journalism, instead striking out as activist journalists purporting to give the public
news that the mainstream media either conceals or fails to provide. These news outlets may
operate on the fringes, but they call themselves professional news organisations
nonetheless. Will their content be excluded too?

**Content authorised by governments**
It is concerning that content produced by federal, state, territory or local governments is
excluded under the proposed bill. Here again, the content is defined by its provenance,
rather than the actual information it contains. Why should content authorised by
governments be excluded when many politicians have themselves been found by fact
checkers to have posted misinformation?

**Content produced in good faith for the purposes of entertainment, parody or satire**
Excluding this content paves the way for those who seek to publish mis and disinformation
to cloak it as entertainment or satire. The weaponization of parody already exists, but
excluding this content from the reach of the bill is likely to further fuel mis and
disinformation that hides behind humour. And who will decide whether their content is
produced in “good faith”? Such an exclusion from the bill would allow misinformation to
flourish in the form of entertainment.

# Conclusion
It is vital that action be taken to combat mis and disinformation and we welcome any
attempt to do this. As fact checkers we work on the frontline of the war against false and
misleading information that is energised by digital service providers algorithms that elevate
content that generates outrage. This is part of the business model employed by digital
service providers.


-----

As third-party fact checkers in partnership with Meta, we also see the power that Meta can
exercise, for example, to supress problematic content by downranking posts; to reject
advertising to those who promote misinformation, and to prevent those actors from
monetising mis and disinformation.

Achieving an appropriate balance between free speech and protection against harmful
content on digital platforms will not be easy. But an added lever that can be used to reach
for this fine balance is media literacy programs. Such programs play an important role in
alerting citizens to the responsibilities that come with free speech. They also build
awareness of quality information, highlight the risks and tactics used by those who seek to
misinform, and promote an understanding of algorithms and the power their wield.

The bill, as it is currently drafted, lacks focus on sustained media literacy programs. We
believe ACMA has a responsibility to take an intergenerational approach to media literacy
and to oversee the roll-out of a sustained nation-wide program that can be measured
against benchmarks.

Finally, we do not believe the current voluntary code has been taken up by sufficient digital
service providers and nor has it resulted in annual transparency reports that provide
meaningful data that clearly demonstrates their efforts to protect Australians from harmful
content. Legislation to enable the ACMA to gather information, ask industry to develop a
code of practice, and create and enforce an industry standard is a first step to address these
issues. But RMIT FactLab believes the proposed legislation requires significant tightening in
its definitions and that it should consider the practicalities of implementation if it is to work
properly.


-----

