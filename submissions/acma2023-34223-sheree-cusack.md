Submission Response - Page 1 of 11

**RE: SUBMISSION IN RESPONSE TO THE COMMUNICATIONS LEGISLATION AMENDMENT**
**(COMBATTING MISINFORMATION AND DISINFORMATION) BILL 2023**

I am writing in response to the proposed Bill as an individual who has read this Bill and it’s
supporting documentation and as a result feels compelled to make a submission.

It is expected that this proposed Bill has been drafted in plain language so that people do not need
lawyers with domain expertise to understand and apply any requirements of legislation.

**Supporting Readings**

My response to this proposed Bill has been informed by reading of various reports. A number of
these documents contained references and citations to other sources. The majority of these
references were also read to develop a clearer understanding of the basis that led to the need to
introduce such legislation and also the literature supporting this case.

A list of this documentation is attached in Appendix A.

**Basis of Powers**

The Fact Sheet associated with this Bill states that the powers are needed based upon the findings of
the ‘Report to government on the adequacy of digital platforms’ disinformation and news quality
measures’ (‘Report’) produced by the ACMA in June 2021.

After reading this Report there are a number of strong concerns as to its findings and the
methodologies set out in it. This is especially the case given that the findings and recommendations
rest, in part, on an unpublished report which is not publicly available for review.

**DIGI Code**

The Australian Code of Practice on Disinformation and Misinformation was launched by the industry
association Digital Industry Group Inc (DIGI) in February 2022. It was subsequently updated in
October 2022.

The Report produced by the ACMA was in direct response to the release of the original Code.

The consultation process undertaken by DIGI was 5 weeks long and invited submissions from 42
organisations that they had identified as relevant and ran a discussion with 12 targeted academics.
There were 17 public submissions.

The Report states that DIGI made little effort to attract a broad range of public views. ‘It did not put
out a media statement, post an update on its social media channels or upsurge any other public
communication avenues to increase awareness of the consultation process beyond those
organisations it had originally identified’ (Report, pp 41-43).

Further, the inclusion of misinformation in the final Code was a significant change between the draft
and final Code that was developed that lacked broad consultation.

Notwithstanding this, the Report determined in Finding 14 through Finding 16 that there was a
meaningful public consultation that “the significant change in scope meant that it would have been
best practice to provide stakeholders a further opportunity to comment on the final drafting prior to
finalisation.”


-----

Submission Response - Page 2 of 11

Despite this assessment, it cannot be said that there was meaningful consultation when a primary
facet of the final Code included something that had not been part of the Draft that was consulted on.
This, when combined with the limited advertising and stakeholder consultation by invite only does
lead to very real concern that this was not in fact a genuine public consultation.

Given declining levels of trust in institutions, the highest priority should have been on a process that
sought a multitude of views and a public process. This is especially so, where the late inclusion of a
topic not even consulted on initially is resulting in an entire Bill for new legislation to specifically
address it.

Mentioned in multiple locations within this Report was the short length of time to create the Code,
yet this was a deadline that was arbitrary and set by the ACMA and Government. It was within the
purview of Government to relax this timeframe to achieve a more transparent outcome. This did not
happen, in fact, there appears now a rush to produce this legislation to give the ACMA more power.

**Guidance Note**

A Guidance Note (GN) accompanying this amendment was issued with the stated purpose of
explaining key parts of the Bill. However, for reasons outlined below I don’t believe that it faithfully
summarises the Bill in several respects, and in some instances is contradictory to what is in the Bill.

**Voluntary Efforts**

The GN states that where voluntary efforts provide inadequate protection then ACMA will request
the industry to make a new code.

However, the Bill states that ACMA will have the power to draft the Standard, where it is satisfied
that is necessary or convenient to write it.

Convenience shouldn’t weigh into the decision about drafting legislation that requires compulsion to
act. There should be proper and robust discourse with a wide variety of views heard surrounding the
nature of the material, especially where it is to have the weight of law. And this should be broadly
consulted – not by invitation only and not by changing the frame or terms of reference without
effective, transparent, and complete consultation.

In the reporting timelines, the ACMA was to provide another report to government by the end of the
2023 financial year. I could find nothing online that has indicated that such a report has been made.

Given the nature and potential wide ranging impact of this Bill, which is even acknowledged in the
ACMA Report, it seems strange that the Government is insisting on strict timelines for
implementation.

It would seem that it would be in the best interest of society as a whole to make the process a lot
more transparent, which is a word used a lot in the supporting documentation but in practice
appears lacking.

One does have to question then why is there the need to develop a Bill in such a compressed
timeframe, when the further report about the adequacy of the existing code is still in question.

Julie Inman in her capacity as Australia’s eSafety Commissioner has recently rejected two of the
eight online safety codes within her purview. The reasoning behind this rejection is that feedback
was given but minimum expectations were not met because they failed to provide ‘appropriate
community safeguards’.


-----

Submission Response - Page 3 of 11

This is a tenuous statement that can mean anything. If feedback was given then surely guardrails
surrounding what was expected were given. Instead, this rejection means that she is now in a
position to draft a mandatory and enforceable industry standard. This is much the same way as these
powers are being reserved for the ACMA.

This raises a further question about whether such a Standard is already drafted by the ACMA for
either the express, or ancillary, purpose of denying a voluntary code.

There were a lot of references in the literature to consultations with other international bodies and
to the EU experience. There is even reference to harmonisation of the definitions of Misinformation
and Disinformation.

That in itself is concerning. International consultation should not be the driver for an Australian law
with such a broad potential to limit freedom of speech.

Further, this seems inconsistent in a number of places where it is stated that there is no clear cut
definition of what misinformation and disinformation actually means. In context, this is what it
means in practical effect, not the words written in statute.

**Code or Standard**

The underlying material all references that this Bill is being created as a reserve power in accordance
with Recommendation 4 of the Report.

This Bill gives the ACMA powers to register a Code that is mandatory and will bind all those defined
as a Digital Platform Provider. This will apply irrespective of whether the Provider is/are Australian,
their size, number of active participants, or their membership in an industry body.

This Bill gives the ACMA powers to establish a Standard, that is also mandatory. This Standard is able
to be set by the ACMA if convenient, or necessary where certain conditions under Division 5 of the
Bill have not been met.

This Standard has the full enforcement of law and the pecuniary penalties as set out in the proposed
Act as substantially larger for a contravention of a Standard over a contravention of a Code.

**Definitions – General**

There are a number of deficient or concerning definitions that lack clarity.

The ACMA reserves standard making powers where it has determined that there are urgent and
exceptional circumstances – yet this is not defined.

What is an urgent and exceptional circumstance that would warrant additional intervention in public
discussion? It should be explicitly clear about who has the power to determine such circumstances
and how there will be an assessment as to whether such circumstances rise to a level of ‘urgent and
exceptional’. There should also be greater clarity about the preconditions or requirements that
would necessitate such a determination.

There is substantial potential for misuse of this power and there appears no inbuilt limits,
accountability or overwatch.


-----

Submission Response - Page 4 of 11

Disinformation requires intent, yet there is nothing in there that says intent determined by who – is it
a criminal standard or common law? Given that these powers are being assigned to ACMA then it
would be presumed to be the ACMA would be the deciding authority.

However, I could identify no precedent upon how this would apply in practical effect. Will it be an
ACMA delegate who assesses this and under what type of framework or matrix will such a decision
be made. When an incorrect determination is made, what is the recourse or right to recovery?

This is yet another instance of words written in the proposed Bill that will have significant issues in
practical implementation.

The lax definitions lead a way to shut down speech on particular and timely topics until the matter is
potentially litigated. This is a process that can go very slowly, by which time the ability to take action
on the topic of discussion may have passed.

For example, the Government is holding a referendum on the Voice. It has clearly come out in
support of a Yes vote in this referendum. This is a clear signal to the DPS of the type and nature of
content that would potentially be flagged.

This Bill could be used to stifle dissenting voices for the No vote on the grounds that content
produced is misinformation. Under threat of financial penalty all DPS must remove this content, or
self censor and not allow free discussion. Where an objection is made against such an assessment
there will be time lost in the litigation and remediation process. By this time the referendum will
have passed.

This enables Government to disallow speech and open discussion on timely matters of public
concern.

**Definition - Harm**

One of the fundamental issues with this Bill is the definition of Harm. The GN states the Bill is to
address ‘serious harm’ with significant implications for the community. At issue is the Bill does not
include the qualifier term ‘serious’ in its definition harm.

Further, this is supposed to be a definition of the word Harm, which is used as the basis for the
determination entirely of whether something is mis/disinformation. Yet the definition of harm, uses
the word itself.

There are no qualifiers or objective ways to measure the impact of harm. The GN stays that the Bill
defines misinformation and disinformation with reference to its contribution to serious harm.

It further states that ‘serious harm is harm that affects a significant portion of the Australian
population, economy or environment, or undermines the integrity of an Australian democratic
process’. This would seem to suggest that there are some benchmarks in place as to what precisely
constitutes a measure of ‘significant portion’

What risk assessment process and protocols are in place to assess objectively whether something
has risen to the level of ‘serious harm that affects a significant portion …’.

That said, the word ‘serious’ is not even in the proposed Bill. This word is a qualifier and can be
objectively measured it is therefore an essential part of the definition and it should be required to be
in the Bill.


-----

Submission Response - Page 5 of 11

**Definition of Digital Platform Service (DPS)**

As stated earlier, this and the term ‘Digital Service’ are defined have been defined in s3 and s4 of the
Exposure Draft sighted.

This definition is too broad. As a simplification of section 4, a Digital Platform Service could
theoretically be taken to be anything now, or in the future, that is distributed via an internet carriage
service, is seen and/or used by Australians and contains an interactive element. This means that any
site that has comments can be included in this definition.

Subject to the Minister consulting with the ACMA first, that Minister can make a legislative
instrument to specify a that a kind of digital service is to be a digital platform service for which this
proposed Act will apply for ‘adequate protection for the community’. This is another nebulous term
that can mean anything and has no objective means of measure.

When taken in conjunction with the scoping of the Act to cover instant messaging, including private
messaging, it provides Government the ability to have access to too much information about
communication between people.

**Limitation of Application - Private Messages**

The GN states that there is a private message exemption. Yet, in the box at page 7, responses are
sought about how instant messaging services will be brought within the scope of the framework.

Instant messaging includes private messages. Private messages should never be brought within
scope, they are by definition, private.

Instant messaging are not broadcast communications insofar that people have to opt in to be part of
the group discussing that topic.

The Fact Sheet states that the ACMA powers would be used for information gathering and recording
keeping. Yet these powers would not require providers of digital platform services to reveal the
contents of private message or have requirements related to breaking encryption of private
messages.

The logical question here then becomes, without having the DPS access, read or interpret the
contents of the private message how then would they then be in a position to provide any type of
information that would satisfy the ACMA requirements?

**Information Gathering Powers**

Division 3 of the Bill is exceedingly broad. The ACMA can obtain information and documents from
digital platform providers and other persons. My main objection is to the inclusion of ‘Other Persons’
in section 19.

The only requirement for the issue of such a notice is that the ACMA has reason to believe that the
person has information or a document that is relevant to a matter of misinformation or
disinformation on a digital platform service – s19(2)(a).

This appears an incredibly draconian part of the Act. There also appears no way to obtain
compensation, remedy or to dispute the issue of such a notice. There must be recourse and a higher
standard that must be met if you are seeking to compel a person to provide such a potentially sizable
amount of information and documentation.


-----

Submission Response - Page 6 of 11

This section could be potentially abused by delegates of the ACMA and there would need to be
explicit limitation lest this be seen as retaliatory. The ACMA could tie a person, body corporate or
partnership up forever in red tape sending out multiple inquiries.

This is a civil penalty provision as well which could cause extreme hardship both in the time taken to
comply, time to appear and financial costs given the directives. People could choose to self-censor
rather than risk interrogation through this section.

There must also be limitations inbuilt into the legislation that prevents the ACMA from abusing this
provision. While it is understood why this section is included, it seems poorly drafted.

**Freedom of Speech**

The GN says that the Bill does not seek to curtail freedom of speech, nor is it intended that powers
will be used to remove individual pieces of content on a platform.

However, there is no mention of freedom of speech contained within the Bill, with the exception of
Schedule 2 – Consequential amendment and transition provisions. Under the Broadcasting Services
Act 1992 7(3AC) it is proposed that ‘The Parliament also intends that digital platform services be
regulated, in order to prevent and respond to misinformation and disinformation on the services, in a
manner that:

a) has regard to freedom of expression; and
b) respects user privacy; and …

The Bill itself states that there is freedom of political speech. However, I cannot find a definition for
this term amongst a lot of different material that I have searched and accessed.

Freedom of Speech is not codified in Australian law. The following commentary is taken from the
‘Right to Freedom of Opinion and Express – Public Sector Guidance Sheet’ issued by the Australian
Government, Attorney-General’s Department.

The right to freedom of opinion and expression is contained in articles 19 and 20 of the International
Covenant of Civil and Political Rights (ICCPR). Article 19(1) holds that the right to hold opinions
without interference cannot be subject to any exception or restriction. The right at 19(2) protects
freedom of expression in any medium. The right protects not only favourable information of ideas,
but also unpopular ideas including those that may offend or shock (subject to limitations).

The GN states that ACMA has no role in determining what is considered truthful. While that is correct
on a literal reading of the Bill, the reality is that the powers being sought under this Bill means that
the ACMA is outsourcing this role to the DPS.

Some of the false or misleading items can be blatant, but the concern isn’t so much about these – it
is about the difference in opinion and how this can be taken as ‘fake news’ – when it is no such thing.
It is simply a different view or interpretation.

Democratic society can only exist when there is the ability to hold frank and transparent discussions
to aid and support in the discovery of the truth. Many scientific discoveries of the past have gone
against the grain and were against the established thinking of the day. With this Bill they would likely
be deemed misinformation and censored.


-----

Submission Response - Page 7 of 11

This Act essentially says that if you are approved for the purposes of the Act then you are free to say
whatever you like. These exemptions are included in s2 Definitions of ‘excluded content for
misinformation purposes’. This exclusion covers content that is professional news content, produced
by an accredited education institution, satire or any content authorised by the Commonwealth,
State, Territory or Local Government.

To be clear, authorised content from any level of government will not be taken to be misinformation.
No such grace is extended to the ordinary person. There is no exclusion to allow people to comment
on and debate political matters without intervention or potentially having their commentary deemed
misinformation and censored. This is unacceptable in a free and democratic society.

In the Report there are multiple references to the Governments ‘intent’ in relation to the purpose or
need to be given such reserve powers.

While the intent of the Government may seem to be initially benevolent the reality is that this Act
will allow any powers to be expanded and potentially abused. And as a member of the public, we
wouldn’t even be allowed to discuss it in an online forum else it be deemed misinformation.

**The COVID Experience**

The ACMA Report uses COVID-19 as its base case for the ‘explosion of misinformation online’ and in
fact uses COVID-19 as one of its outcomes for ‘serious harm’. However, COVID-19, in my view, when
seen from an alternative perspective is a case study in how not to approach censorship.

This is how politicisation, how being ‘seen to be doing the right thing’, and/or desire to drive a
narrative overruled a cool headed approach to a fast evolving situation. People need to be free to
discuss, dispute and debate the known information. In a fluid situation, known information will be
updated as new information comes to light. But rather than allow this to happen, opinions (even
those of leading and well credentialed specialists) were disregarded in favour of an approved
narrative.

Social Media Platforms colluded with the Government, NGOs, and Academia to remove legitimate
scientists and others off various platforms. These platforms (or what will be defined as DPS under
this Bill) were the forums for those who were seeking to engage in scientific and robust debate or be
part of the conversation. Instead, numerous experts, specialists and ordinary people were excluded
from the public forum. This is not my opinion. This is fact that is now in the public domain.

Under Australian Freedom of Information laws, the secretary of the Department of Home Affairs
Michael Pezzullo revealed in May 2023 that between January 2017 and December 2022, his
department had referred 13,636 posts to digital platforms such as Facebook, Meta, Twitter,
Instagram and Google to review against their terms of service. Of these, 4,213 were related to Covid.

DPS’s were quick to label anything not produced by government or preferred media as being ‘false,
misleading or deceptive’. Such labels were attached to large amounts of material posted. The
passage of time has come to show that a number of these labels were false and misleading
themselves.

This labelling is an example of DPS applying policy surrounding content that contradicted Local
Health Authority (LHA) or World Health Organisation (WHO) guidance. If this Bill in its current form
were to pass, then in the event that this content was not removed by the DPS then they could be
subject to substantial fines and penalties.


-----

Submission Response - Page 8 of 11

This concern has been borne out in recent days with the update to Google’s Medical Misinformation
policy. This moved from a COVID-19 policy to a much more broad Medical Misinformation policy
where YouTube will not allow content that poses a risk “by spreading medical misinformation that
contradicts local health authorities’ (LHAs) or the World Health Organization’s (WHO) guidance
about specific health conditions and substances.”

This explicitly states that content produced that ‘contradicts’ health authority guidance will not be
allowed. So irrespective of whether the content is true or for purposes of discussion, it can be
removed under this policy. There are also reports that this policy is also being applied retrospectively.

There were instances of official guidance that required compliance that was contradictory or
nonsensical. There must be room for scientific questioning of this authority.

During COVID-19 pandemic, the New Zealand Prime Minister Jacinda Ardern said, "We will continue
to be your single source of truth," and that, "Unless you hear it from us, it is not the truth."

Is this the Government’s intent in the adoption of such a Bill? To be the single source of truth? The
powers granted under the Bill in relation to contravention and the significant penalties imposed on
Digital Platform Services (DPS), can only result in these regulated DPS adopting Government talking
points as the correct ‘view’.

If they fail to apply this government view of what is correct, then any content distributed on their
platforms will be taken to be false or misleading and therefore fall within the purview of
misinformation or disinformation.

The powers proposed under the Bill regarding systemic proliferation of information that is different
to Government messaging and the ability to apply incredibly large financial penalty would cause any
DPS to toe the line to avoid liability.

This is especially true where the definitions are ill defined and/or overly broad.

**Who determines what is False or Misleading?**

The supporting material references ‘incentivising’ behavioural change. Compelling the action through
law is not incentive. It is compulsion. Especially where the failure to comply attaches significant
financial penalties.

The DPS will be required to adjudicate what is fact because the very nature of the proposed
definition of misinformation – that it is ‘online content that is false, misleading or deceptive’. That
means that there has to be a determination of what is true, to understand whether something is
false, misleading or deceptive.

Without a robust way to civilly discuss and weigh each option, there can be no true search of fact.

Given the scale of the proposed penalties, it is foreseeable that the DPS will seek to over do the
censorship with respect to anything potentially deemed inflammatory. This means that only
government approved messaging will be adopted across the platform and as such, it is a proxy for
government censorship.

To repeat this, the ACMA does not need to direct the DPS to remove content – the DPS themselves
would seek to censor or deny the content as it was not in line with the Government approved
message. This is because a failure to remove such content would render the DPS liable to financial
penalty.


-----

Submission Response - Page 9 of 11

**What is a fact?**

What is a fact is something that has been widely disputed over the past few years. It shouldn’t be,
but it is. A view of a fact is skewed by political and personal bias. A fact should be free from bias and
is a thing known or proved to be true.

However, people’s lived experiences are now skewing their interpretation to fit their own narrative or
that of their own social group. It is no longer The Truth, it is My Truth.

This then means it if a feelings based approach, rather than a test with an objective measure that is
appropriately risk assessed and weighted.

This Act potentially removes the process of finding fact through discovery. That is, how can
something be proved to be true, or our understanding evolve, without the ability to discuss, dissect,
challenge and debate it?

There are very few things that cannot be seen from both sides and past history has shown that only
one side of the political spectrum is being largely removed for content moderation. You cannot
permit one side to have a monopoly in expressing its views. That is antithetical to a free society.

**Accountability and Transparency in Fact Checking and for the ACMA**

At section 33 of the proposed Bill, examples of matters that may be deal with in either the Codes or
Standards include supporting fact checkers at 33(3)(f).

Fact checking is also referenced numerous times in the ACMA Report. However there is no mention
in any documentation about the quality of the fact checking organisations themselves.

What accountability is there for fact checkers who continually get it wrong or who are biased in their
determinations. There are a number of organisations who already operate in this space and there is
already concern about the quality of their reviews or that they do not allow questioning of the
establishment orthodoxies lest their subject be tagged with a false rating.

There are numerous examples of posts and articles that were labelled as misinformation at the time
but which are subsequently borne out to be fact. None specifically are included in this submission in
order to keep it brief, however they can be provided upon request.

The Bill seeks to introduce extremely large penalties for material that is determined to be false and
misleading. It would appear that the determination of what is false and misleading is therefore a key
criterion under which the terms of this Act would apply.

Fact checks and assessments are often undertaken using frameworks that in themselves hold an
implicit bias.

This is clearly not the case in every instance. Balance needs to be weighed carefully on the right to
freely discuss in an open forum in order to be able to get to the bottom of events.

This requirement for Fact checking to form part of any Code or Standard is unnecessary. There
should be no third party group that makes a grading or assessment on the way that content is
presented nor on its accuracy.


-----

Submission Response - Page 10 of 11

To demonstrate the Government’s commitment to accountability and openness within it’s own
house, there should be a provision that delegates who make decisions (or agents of the ACMA) and
Heads of the ACMA be held personally responsible, outside of ordinary Government indemnification
for decisions made. This will bring greater transparency to the process which is essential given the
erosion of institutional and Government trust.

**Conclusion**

I do not believe that this amendment is about preventing misinformation. Instead, it is about
silencing dissent. Rather than doing it directly through the Government or it’s myriad of bureaucratic
bodies, this Bill seeks to enjoin the DPS as proxy for these actions.

Freedom of speech means an unregulated marketplace of ideas. It is essential that speech remain
free despite any technological change.

If this amendment bill were to pass, having less information online or only those opinions that
confirm to the Government narrative will not lead to the desired result.

Instead, it will create a vacuum and that will be filled in other ways and lead to exactly the type of
activity that this Bill is trying to prevent. People must have the ability to communicate their ideas
online, without the spectre of censorship.

Trust in online information is earned with uncensored comment and open civil debate.

In short, this is not a matter that the Australian Government should be legislating for, and I do not
believe that is in the best interest of the public for this amendment Bill to proceed.


-----

Submission Response - Page 11 of 11

**Appendix A**

**Supporting Readings**

Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2023

  - Fact Sheet

  - Guidance Note

  - Exposure Draft

ACMA – A Report to government on the adequacy of digital platforms’ disinformation and we quality
measures (June 2021) ‘Report’

  - The Report itself

  - Fact Sheet 1 – Key Research Findings

  - Fact Sheet 2 – Code Framework

  - Fact Sheet 3 – Next Steps

Note: This ACMA report relies heavily on a commissioned report which is unpublished and is not
available online. Inquiries were made of the firm who produced this and as at the time of this
submission, no documents were made available. This report by ‘We Are Social’ is titled ‘Social media
_insights into how online misinformation and disinformation are being spread across social platforms_
_in Australia – May 2021’_

ACMA – Misinformation and news quality on digital platforms in Australia - A position paper to guide
code development (June 2020)

Regulating in the digital Age – Government Response and Implementation Roadmap for the digital
Platforms Inquiry

DIGI – Australian Code of Practice on Misinformation and Disinformation

  - Code of Practice (both the February 2021 and the update of October 2021)

  - Submission Report (Feb 2021)


-----

