**Professor Emerita Anne Twomey**

15 August 2023

Information Integrity Section
Department of Infrastructure, Transport, Regional
Development, Communications and the Arts
Canberra, ACT 2600
[Information.Integrity@infrastructure.gov.au](mailto:Information.Integrity@infrastructure.gov.au)

Dear Sir or Madam,

Please accept this submission on the Exposure Draft of the Communications Legislation
_Amendment (Combatting Misinformation and Disinformation) Bill 2023 (Cth)._

I would like to preface it by acknowledging that Australia faces serious threats from the
use of misinformation and disinformation to undermine our democratic system and
processes. The reduction of the prevalence of misinformation and disinformation on
digital platforms is a worthy and important aim. But consideration also needs to be
given to the risk to free speech that may arise from making determinations that content
is misinformation or disinformation. There is a very real risk that such decisions may
result in the suppression or deletion of alternative views on digital platforms and impose
a single view of ‘truth’ in Australia in relation to a range of social, political, economic
and scientific matters. This would not only damage our democracy but also disrupt our
opportunities for advancement, including economically, that come from alternative
thinkers and innovation.

Accordingly, any legislation within this field must be closely considered and very
carefully calibrated. It is welcome that the Government has published this Bill as an
exposure draft so it can receive that level of attention. For my part, I found the Bill
confusing and concerning in its operation.

As I have no expertise in the digital technology area, I have not addressed the issues that
appear to me likely to arise from the definitions of digital service, digital platform
service and interactive feature. I hope that others with expertise will do so. I note,
however, that if the definitions and the consequential burdens trickle down to smaller
operators (eg blogs that include interactive features that allow people to comment in
relation to the content and the posts of others) the regulatory costs of the record keeping
and other obligations imposed by this Bill will encourage them to cut off all ‘interactive
features’ to avoid being classed as a ‘connective media service’ and therefore a ‘digital
platform service’ (cl 4). This is often described as the ‘chilling effect’ on free speech.

I also have not addressed the significant issues concerning the extra-territorial operation
of this proposed law, as I am not equipped to judge the extent to which digital platform
providers can geo-block content for Australia without impacting upon their provision of
content in other countries which may have different laws and codes, and indeed,
different constitutional requirements, regarding freedom of speech. I have instead
confined myself to matters concerning free speech and constitutional issues in Australia.


Sydney Law School F10
Eastern Avenue
The University of Sydney
NSW 2006


anne.twomey@sydney.edu.au
http://sydney.edu.au/law
http://ssrn.com/author=808822


ABN 15 211 513 464
CRICOS 00026A


-----

**1.** **Definitions**

**_Misinformation and ‘excluded content’_**

**_Misinformation is defined in cl 7(1) as content that:_**

a) contains information that is false, misleading or deceptive; and
b) is not ‘excluded content for misinformation purposes’; and
c) is provided on a digital service to end-users in Australia; and
d) is reasonably likely to cause or contribute to serious harm.

The application of cl 7(1)(b) is unclear to me. It refers to ‘excluded content for
**_misinformation purposes’, which is defined in cl 2 as meaning, amongst other things,_**
professional news content, content produced by or for an accredited educational
institution and content authorised by the Commonwealth, a State or Territory. The
problem is that this definition primarily relates to the source of the content, rather than
the subject-matter of the content. Hence, when it comes to the definition of
misinformation, a difficulty arises in determining whether the subject-matter of content
amounts to misinformation, as that same subject-matter could be found published by
excluded sources, such as professional news content, content produced by or for
educational institutions and government material, as well as generally circulating
misinformation on the internet. If the source is not itself identified (eg a republication
of the false, misleading or deceptive information from one of these excluded sources),
how is the digital platform service to decide whether or not it is ‘misinformation’ as
defined?

Let us take, as a practical example, the assertion that Aboriginal and Torres Strait
Islander peoples were treated, by statute, as ‘flora and fauna’. It is false and arguably
reasonably likely to contribute to serious harm. It has caused great distress over time.
It would therefore amount to misinformation, unless it falls into the category of
excluded content in cl 7(1)(b). You can find this assertion in ‘professional news
content’ (eg ‘Australia Commemorates Aboriginal referendum’, ABC RN, 25 May
2007; ‘Bandler’s faith in people drove change, The Australian, 14 February 2015; ‘The
Same, Only Different’, _Courier-Mail,_ 8 June 2013; ‘Aborigines mark the day they
became “humans”’, _The Independent_ (UK), 26 May 2007); content prepared for
educational institutions (eg _Cambridge Legal Studies,_ 4[th] ed, 2016, CUP, p 38) and
government documents (eg NSW, _Hansard,_ LA, 6 May 2003 – admittedly this is a
parliamentary document, not government material, but if one searched hard enough, I’m
confident it would also be in government documents somewhere). So when this
common assertion is repeated on social media websites, is it ‘excluded content for
misinformation purposes’ because you can find the same assertion in professional news
content and other excluded sources, or is it excluded content only if it quotes or republishes from such an exempted source, or is it misinformation if the author is an
ordinary person? How does the definition work when the same false or misleading
content is published on digital platforms but it comes from different sources?


-----

Further, how does a digital platform service provider draw such distinctions when it is
dealing with content on an automated basis? In practice, surely it will treat all subjectmatter content of the same type in the same way. While any code, rules or standards
under the current Bill would not require a digital platform service to address or remove
‘excluded content’ from these sources, it is likely to do so anyway for practical reasons.
This is already acknowledged with respect to the current industry code (as updated 22
December 2022), which states in [1.8] that signatories may ‘take a more expansive
approach’ and take measures to combat misinformation in relation to a ‘wider range of
content or products and services than is within the scope of this Code’. Further in [4.5]
it notes that signatories to the Code may, ‘in their discretion’, treat as misinformation
content which is ‘excluded from the provisions of the Code’ if they determine such
content is ‘reasonably likely to cause Harm’.

Parliament should take into consideration, therefore, that the likely effect of its
legislation will be the censorship of material regardless of its provenance, including
‘excluded content’. This may mean, for example, that digital platforms remove or reject
content such as authorised political advertisements during election periods on the basis
of the assessment by a fact-checker that it contains ‘misinformation’ or declare that
authorised government content amounts to misinformation and demote its ranking or
label it as false.

**_Content produced by or for an educational institution_**

It is not clear to me what is meant by ‘content produced by or for an educational
institution’. Does that mean any content produced by a teacher or academic employed
at an accredited school or university in their professional capacity, such as research
articles, or is it just teaching material produced for use by the school or university? For
example, if an academic employed by a university published in a commercial peerreviewed scholarly journal a controversial article which challenged the current
consensus on a health issue, and its synopsis was widely published on social media,
could it be treated as ‘misinformation’ when published on a digital platform or would it
be excluded from the definition as it was ‘content produced by or for an educational
institution’? It would not be the institution itself which created the content and if it was
research rather than teaching material, it would be unclear if it was produced ‘for’ an
educational institution.

Educational institutions (as opposed to their employees) produce little content, other
than teaching materials, unless they publish their own scholarly journals. For example,
the _Sydney Law Review_ is produced by the Sydney Law School at the University of
Sydney, but other scholarly journals, such as the Public Law Review, are produced by
commercial publishers (as are many scientific journals, such as _Nature). Would_
material from an article published in a journal produced by a University be treated
differently from the same material published in an equivalent peer-reviewed scholarly
journal published by a commercial publisher when it is communicated by a digital
platform service? If so, why?


-----

Note that the current voluntary Code refers to content produced ‘for educational
purposes’, which appears to be much wider. What was intended to be gained by
narrowing this to content produced by or for an educational institution?

**_Content produced in good faith for the purposes of entertainment, parody or satire_**

The only ‘excluded content for misinformation purposes’ that is actually directed at the
substance of the content, rather than its provenance, is ‘content produced in good faith
for the purposes of entertainment, parody or satire’. First, there is the problem of how a
digital platform provider (or the ACMA) is to assess ‘good faith’ in this context.

Second, it simply encourages those who are purveying misinformation to do so in an
entertaining or satirical form. Much of the misinformation on the internet is already
conveyed in this manner by personalities who have become popular due to their
capacity to convey material in the form of parody, satire and entertainment. That is how
they enhance their audience already. This exception would simply encourage it and
have the effect of building wider audiences for it.

Third, it does not encompass other forms of expression, such as artistic works. Would
the ‘musical’ work of rappers have to be examined for misinformation, particularly in
relation to misogynistic statements about women or statements about police or race
relations – or would this be treated as ‘entertainment’ made in ‘good faith’?

**_‘Is reasonably likely to cause or contribute to serious harm’_**

Content is only treated as ‘misinformation’ if its provision on a digital service is
‘reasonably likely to cause or contribute to serious harm’. This involves a lot of value
judgments. Is it ‘reasonably likely’ that it will ‘cause’ harm? Is it ‘reasonably likely’ to
‘contribute to’ pre-existing harm? How does one assess that the harm is ‘serious’?

How can a digital platform provider be expected to make such assessments on
contentious matters in any objective and consistent way? If it makes a determination,
there is such room for different judgments that it is likely that one digital platform
provider will decide that something is misinformation because it is ‘reasonably likely to
cause or contribute to serious harm’, whereas three others might decide that it is not.
Equally, the ACMA could itself take a different view on any of these judgment-calls
about causation, contribution, what is reasonably likely and what amounts to ‘serious’
harm. The resulting conflict would likely be newsworthy.

This leads to the further possibility of harm being exacerbated due to the widespread
publication by professional news organisations of the claimed misinformation content in
reporting the disagreement amongst digital platform providers or with the ACMA as to
whether the content amounts to misinformation or disinformation. It has not been
unknown for organisations to make provocative claims for the purpose of causing their
censorship and then creating a newsworthy cause célèbre which massively increases the
spread of their original provocative claim, at no financial cost to the organisation that
originated it. In short, the regime proposed by this Bill could be used by malicious


-----

actors for the purpose of spreading misinformation more widely than it might otherwise
reach, resulting in greater harm.

**_Disinformation_**

The definition of disinformation in cl7(2) provides in paragraph (e) that ‘the person
disseminating, or causing the dissemination of, the content intends the content to
deceive another person’. Who is the ‘person disseminating’? I assume it is not the
digital platform on which it is disseminated. Clause 7(3)(e) refers to the ‘author of the
information’. Is this different from the ‘person disseminating’ it? Some further clarity
is needed regarding these definitions.

Further, how is a digital platform service (or, indeed, the ACMA) to determine what a
person ‘intends’? Is any procedural fairness required? This law appears to put digital
platform providers in a place of judgement akin to that of a court, but without the
judicial independence or the procedural and evidential constraints. This is a matter of
concern. If a digital platform service decides that content is ‘disinformation’ because it
takes the view that the person disseminating it ‘intends’ to ‘deceive’, would that amount
potentially to defamation?

**_Harm_**

The reference to ‘hatred’ in para (a) of the definition of ‘harm’ includes hatred on the
basis of ‘gender’ and ‘sexual orientation’, but not ‘sex’. Given the increasing focus
today on the difference between the meaning of ‘gender’ and ‘sex’ (whereas once they
may have been regarded as the same thing), it seems rather odd that ‘sex’ is excluded.

Paragraph (c) refers to harm to ‘the integrity’ of Australian democratic processes or of
Commonwealth, State, Territory or local government institutions. In this context
‘integrity’ seems to be too narrow. Surely this should extend to harm to the democratic
system itself – not just to the integrity of processes such as elections or the integrity of
government institutions? I am assuming that ‘integrity’ in this context means both
‘wholeness and unity’ (i.e. the opposite of fragmentation), and the capacity to operate in
the intended manner without disruption or damage. Surely misinformation can cause
serious harm by undermining public trust in the democratic system without harming the
‘integrity’ of democratic processes or governmental institutions?

**Assessment of misinformation and disinformation**

The ‘Fact Sheet’ for the exposure Bill states at p 9 that the ‘ACMA would have no role
in determining truthfulness, nor will it have a role in taking down or requesting action
regarding individual pieces of content’. The exposure Bill effectively outsources
responsibility for censorship to digital platform providers. However, the entire
exposure Bill is predicated upon the ACMA making assessments of the effectiveness of
codes in dealing with misinformation and disinformation, and whether additional codes
or standards are required. This cannot be achieved without the ACMA itself making an
assessment as to whether particular content circulated by digital platform providers


-----

amounts to misinformation and disinformation that has not been adequately addressed
by them.

In addition, paras 10(1)(me) and (mf) will be inserted in the Australian Communications
_and Media Authority Act 2005 (Cth), conferring on the ACMA the functions to ‘conduct_
investigations relating to misinformation and disinformation on digital platform
services’ and ‘to inform itself and advise the Minister in relation to misinformation and
disinformation on digital platform services’. The ACMA will, therefore, be an arbiter
of truth to the extent that it fulfills its role in monitoring and imposing additional
requirements upon digital platform providers.

How can a platform know with certainty what is ‘misinformation’ and what if it has a
different view from the ACMA about what constitutes misinformation in a specific
case? For example, even when what is at issue is scientific fact, while there might be a
consensus view of scientists, that consensus might change, after being challenged and
debated. That is how science progresses. The initial consensus in relation to COVID19 was that it was spread by droplets and not airborne, until it was finally accepted
around 2 years later by the WHO that it actually is an airborne disease. Should an
initial consensus lead to the online suppression (i.e. the demotion of ranked content) or
removal of alternative scientific views and prevent them being debated publicly on
digital platforms, when those presenting the unorthodox view might turn out to be right?
Might this in itself lead to serious harm? Sometimes ‘group-think’ can lead to the
wrong answers and robust public debate is often necessary to advance learning.

The voluntary code currently adopted by industry defines misinformation and
disinformation by reference to content that is ‘verifiably’ false misleading or deceptive.
This, at least, recognises that a significant amount of contentious information cannot be
‘verified’ one way or the other. In the current debate on the Voice referendum, while
there is a broad consensus of constitutional lawyers in relation to some matters, there
are other constitutional lawyers who take a different view, and no one could provide a
‘verifiable’ answer, other than the High Court if the matter were to be litigated before it.
The Bill, in defining ‘misinformation’, states that it is information that ‘is false,
misleading or deceptive’. It does not confine itself to verifiable matters and does not
identify how this assessment is to be made, either by the digital platform or by the
ACMA.

In many cases, in relation to particularly contentious matters, truth is only able to be
ascertained decades after the event. This is almost always the case, for example, in
relation to wars, where facts will be hidden from public view for security reasons, there
will be limited and controlled access by journalists to places and people, and
propaganda will be used by all sides to advance their interests. While this means there
will be much misinformation in circulation, it makes it extremely difficult to ascertain
the truth (eg which side blew up the crucial bridge, did country X deliberately target a
hospital, did country Y execute civilians, etc). Even in relation to scientific matters,
access to information may be limited (see the controversies about the origins of
COVID-19 and the side-effects and safety of various vaccines) so that assessments of
what is true, false or misleading cannot accurately be made.


-----

To what extent can foreign digital platform providers be trusted to determine truth and
misinformation, especially when they may be subject to a range of political pressures
outside of Australia?

When it comes to how misinformation is to be identified, it appears that reliance is to be
placed upon ‘fact-checkers’, although this is only indirectly implied (eg ‘supporting
fact-checking’ is an example of what may be dealt with by misinformation codes and
standards – cl 33(f)). I often provide responses to questions from fact-checking
organisations, so I have experience of the practice. The quality of a fact-checker’s
report is dependent upon which experts the fact-checker asks (not to mention who
responds – because not everyone has, or makes, the time to respond to these many
requests), as well as the capacity of the fact-checker to understand, synthesise and
explain the result clearly. I have sometimes seen fact-checker reports that have
completely misunderstood what I have told them, or the other material the fact-checker
has collected, resulting in an unclear or inaccurate assessment. In other words, factcheckers are useful, but not infallible.

Some matters are clearly verifiable and can be determined as a matter of fact, while
other matters concern predictions or views that may be disputed and which cannot be
determined, even though they may be regarded as ‘misleading or deceptive’. Much
‘misinformation’ is not conducive to definitive fact-checking.

I have also found that in recent times, as the number of ‘fact-checks’ has proliferated,
their quality has diminished. A long time ago, the fact-checker would ask the expert
they sought advice from to approve the final version of their report to ensure that it is
accurate. This no longer happens. In my view, while it is useful to have fact-checking
bodies reporting on verifiable facts, they cannot be treated in all cases as an accurate
arbiter of misinformation.

**_Electoral and referendum content_**

One of the most sensitive areas for both free speech and misinformation is the public
debate concerning elections and referendums. Clause 35 of the Bill provides that the
ACMA must not register a code or determine a standard that contains requirements
relating to ‘electoral and referendum content’ unless they concern ‘disinformation’
(being misinformation that is intended to deceive) and they ‘do not relate to authorised
content’.

The definition of authorised content under current law is eclectic. It covers: (a) paid
advertisements; (b) stickers, fridge magnets, leaflets, flyers, pamphlets, notices and
how-to-vote cards; and (c) communications by or on behalf of political parties,
candidates, associated entities and certain third parties who incur electoral expenditure
above the disclosure threshold (described as ‘disclosure entities’). It does not cover
individuals (who are not candidates) who communicate political material through social
media where no expenditure is involved.


-----

‘Electoral and referendum content’ is defined as including matter communicated for the
dominant purpose of influencing the way electors vote in an election or referendum, but
(according to a note) this does not include communication ‘whose dominant purpose is
to educate their audience on a public policy issue, or to raise awareness of, or encourage
debate on, a public policy issue’.

The upshot is that the ACMA cannot register a code or determine a standard that
contains requirements relating to misinformation or disinformation by political parties,
candidates and certain third party campaigners engaged in electoral expenditure. It
seems that they can say what they like, however false or misleading. However,
communications with the dominant purpose of educating audiences on public policy
issues or encouraging a policy debate can be subject to codes and standards if regarded
as misinformation (as they do not fall within the definition of electoral and referendum
content) and other communications containing electoral and referendum content on
digital platforms that are not paid advertisements can be subject to codes and standards
if regarded as disinformation.

The policy basis for these distinctions is unclear. It also raises an issue with the implied
freedom of political communication, as it favours the political communications of
parties and candidates (even if they contain false, misleading or deceptive information),
while potentially permitting the suppression or removal of political communications by
others. (See the discussion about the implied freedom below.)

Further, these distinctions raise the problem discussed above of the difference between
content and source. For example, if Party A runs political advertisements containing
claim X, which is false, misleading or deceptive, no code or standard would require its
removal or suppression, but what happens then if Joe Blow repeats claim X on his
Twitter account and it is spread widely? Does it matter, if in doing so, he quotes the
source of the statement as Party A, or political candidate B? Does the digital platform
have to make an assessment as to whether claim X is misinformation or disinformation
when it is communicated by persons who are not ‘disclosure entities’? How is this
supposed to work in practice?

The ‘Guidance Note’ states at p 14 that the ACMA will be able to use its information
powers in relation to election and referendum matters to request information on the
length of time it takes for platforms to respond to complaints about an electoral matter
during a federal election. This would presumably place pressure on digital platform
providers to act quickly in response to complaints. It would therefore incentivise
political parties to encourage their supporters to make complaints about
communications on digital platforms of claims made by their opponents when repeated
by persons who are not ‘disclosure entities’, and we would then have the political
spectacle, during the course of an election campaign, of Facebook or Twitter making
findings that particular claims are disinformation and removing or suppressing them,
which could potentially have its own significant electoral effect. Has anyone thought
this through?


-----

Clause 19 would also appear to permit the ACMA to require political parties to provide
information and documents regarding misinformation published on digital platforms in
their advertisements, and require individuals to appear before the ACMA to give
evidence, either orally or in writing. The exemption in cl 35 for disclosure entities does
not appear to apply to the ACMA’s compulsion powers in cl 19. Is this intended?

**Powers of the ACMA**

The Guidance note to the Bill, when addressing the ‘information gathering’ powers of
ACMA keeps referring to the ACMA ‘requesting’ information from digital platform
providers and third parties (see pp 15-16) whereas in fact the powers ‘require’ providers
and third parties to give the ACMA information and documents, and to ‘appear before
the ACMA at a time and place specified in the notice to give any such evidence, either
orally or in writing, and produce any such documents’. This power of compulsion is
backed by civil penalties for each day during which the contravention occurs.

Given that the ACMA has a range of roles and powers under the legislation, including
investigation, requiring the keeping of records, compelling oral and written evidence,
determining standards, issuing formal warnings, determining contraventions of the Code
and issuing remedial directions, as well as issuing penalty notices, great care needs to be
taken to be clear about the limits on its powers and the basis upon which they may be
exercised. It remains unclear how the ACMA is to assess that content amounts to
misinformation or disinformation for the purpose of determining whether a Code has
been breached or is inadequate and should be replaced or made subject to new
standards.

**Implied freedom of political communication**

The aim of the Bill is to encourage or require digital platform providers to address
misinformation or disinformation, including by: censoring it (i.e. removing it from the
platform, sometimes described as ‘take-downs’); suppressing it by adjusting their
algorithms to make it much harder to see (sometimes described as ‘demoting’ the
material); refusing to accept advertisements that include misinformation or
disinformation; and preventing the monetisation of misinformation and disinformation
so that people can’t profit from it. Assuming that some, or perhaps most, of the
communications that are removed, demoted or denied a platform are political
communications, such actions will ‘effectively’ burden the constitutionally implied
freedom of political communication in its terms, operation or effect.

It is arguable that simply ‘encouraging’ digital platform providers to adopt a ‘voluntary’
code would not involve the law itself burdening the implied freedom. However, the
other measures of the Bill, which allow the ACMA to impose a code or standards,
would be likely to be regarded as burdening the implied freedom.

There is also a question as to whether the constitutionally implied freedom protects
communications that amount to misinformation and disinformation. There have been
conflicting views on the issue. For example, Gaudron J said in the Australian Capital


-----

_Television case in 1992 that ‘as the freedom of political discourse is concerned with the_
free flow of information and ideas, it neither involves the right to disseminate false or
misleading material nor limits any power that authorizes laws with respect to material
answering that description’.

In contrast, Justices Deane and Toohey JJ said in the Nationwide News (1992) case that:

freedom of political discussion necessarily involves freedom to maintain and
consider claims and opinions about political matters notwithstanding
their unpopularity among either the general populace or those in government or
that they may ultimately be shown to be mistaken. That being so, the fact that
particular assertions, opinions or criticisms about matters relating to government
are rejected by government or are found by the courts or proved by subsequent
events to be mistaken does not, of itself, suffice to establish that the suppression
of their expression is or was consistent with the effective functioning of
representative government.

Justice McHugh, in _Levy v Victoria_ (1997), stated that the constitutional implication
does more than protect rational argument and peaceful conduct. It also protects false,
unreasoned and emotional communications.

The nature of the relationship between the implied freedom and misleading or deceptive
communications remains unclear. On the one hand, there is a strong legitimate purpose
in preventing the Australian people from being misled when they exercise their vote in
elections and referenda (see, eg Mulholland v AEC). On the other hand, the High Court
might rightly be concerned that laws directed at removing or suppressing
‘misinformation’ might in fact be used to remove or suppress contrary political views
and views on contentious matters that might initially appear to be false or deceptive but
later be found to be true.

A further complication is the specific treatment of ‘electoral and referendum content’,
as discussed above. It protects communications by political parties and candidates, by
not allowing them to be treated as misinformation, regardless of the falsity of their
content, but permits political communications by others (who are not ‘disclosure
entities’) to be suppressed or demoted if regarded as ‘disinformation’. This imbalance
in treatment is likely to raise red flags (see Unions NSW (No 2)).

Similarly, all government ‘authorised’ content is excluded from the definition of
misinformation, which privileges government material, even if it is false or misleading,
over material produced by its opponents (outside of authorised electoral content), not to
mention third parties. On this basis, if the Government makes false claim X in its
advertising and it is reasonably likely to cause serious harm, it could not be treated as
misinformation for the purposes of the Bill, but if the Opposition made the same claim
(outside of authorised electoral content), its advertisements containing that claim could
be refused by digital platforms, and its content containing the claim could be removed
or demoted by digital platforms.


-----

It was an imbalance of treatment of campaigners in elections and the advantaging of
political incumbents that originally provoked the High Court to identify the implied
freedom of political communication in the Australian Capital Television case in 1992.
Treating political communication differently in this way, based upon its provenance,
without adequate justification is likely to lead to constitutional invalidity.

Due to all these uncertainties, two measures have been taken to protect the validity of
the proposed law from the implied freedom. First, the ACMA is in some cases required
to consider the impact upon the implied freedom. For example, clauses 37 and 45
provide that before the ACMA registers a misinformation code or determines a
standard, it must consider: (i) whether the code burdens freedom of political
communication; and (ii) if so, whether the burden is reasonable and not excessive,
having regard to any circumstances the ACMA considers relevant. This is not identical
to the more complex test applied by the High Court which requires the identification of
a purpose that is compatible with the constitutionally prescribed system of
representative and responsible government and the application, at least according to the
majority, of a structured proportionality test (see the full test in _Brown v Tasmania_
(2017) 261 CLR 328, 364 [104]). The basis upon which the ACMA is making its
assessment is therefore different to the basis upon which the High Court would make its
assessment which is potentially problematic.

The second measure is cl 60. It states that the provisions of this law, any rules made
under it, any registered misinformation code and any misinformation standard ‘have no
effect to the extent (if any) that their operation would infringe any constitutional
doctrine of implied freedom of political communication’. Similar provisions may be
found in s 44 of the SPAM Act 2003 (Cth), s 138 of the Telecommunications Act 1997
(Cth), s 287AC of the _Commonwealth Electoral Act 1918_ (Cth), s 61QE of the
_Interactive Gambling Act 2001_ (Cth), and s 233 of the _Online Safety Act 2021_ (Cth),
amongst others.

The effect of this clause is to leave digital platform providers uncertain as to whether
the codes or standards apply or not, depending upon the application of the implied
freedom. It takes the burden of crafting constitutionally valid laws from the
Commonwealth and imposes a burden instead on the persons to whom the law purports
to apply to assess whether or not it does in fact apply. If they take the view that it does
not, they then risk High Court litigation to determine the matter, if action is taken
against them for non-compliance. It would seem that the Government is working on the
assumption that digital platform providers will comply with the law, even if it is not
operational because of its breach of the implied freedom, because compliance is the
easier and cheaper route. The ‘constitutional risk’ is effectively transferred from the
maker of the law to the subject of the law. The appropriateness of this transfer is
dubious, albeit it is becoming more widespread.

**Other constitutional escape hatches**

The same approach has been taken in cl 61 in relation to the compulsory acquisition of
property. It states that provisions of the law, rules made under it and misinformation


-----

codes and standards have no effect to the extent (if any) to which their operation would
result in an acquisition of property other than on just terms (within the meaning of s
51(xxxi) of the Constitution).

Interestingly, this is different from the original ‘historic shipwreck clause’ (s 21 of the
_Historic Shipwrecks Act 1976_ (Cth)) and the standard clause set out in the Office of
Parliamentary Counsel’s ‘Drafting Direction No 3.1 – Constitutional Law Issues’ which
instead provides that if an acquisition would occur other than on just terms, for the
purposes of s 51(xxxi), then the Commonwealth is liable to pay a reasonable amount of
compensation to the person. Such clauses have previously been found effective by the
High Court (eg in Wurridjal v Commonwealth and Telstra Corp Ltd v Commonwealth).
In contrast, cl 61 denies effect to the provisions of the law, rules, etc, to the extent that
they would result in an acquisition of property otherwise than on just terms. This
undermines the certainty of the law, and hence the principle of the rule of law, as the
citizen is left uncertain as to which parts of the law are operative and must be obeyed.

Clause 63 also seems to be seeking to apply a constitutional escape hatch to the
_Melbourne Corporation_ doctrine by providing that powers under this law must not be
exercised in such a way as to prevent the exercise of the powers, or the performance of
the functions, of the government of a State, the Northern Territory or the Australian
Capital Territory. In this case, the wording seems to be wider than the constitutional
doctrine, as it appears to apply to _any_ powers or functions of a State or Territory
government.

**Conclusion**

This exposure draft Bill is at best half-baked. While one can sympathise with its aim, it
is hard to believe that anyone has seriously thought through how it would operate in
practice and the likely consequences. It also raises a significant risk that the ‘cure’ is
worse than the disease – i.e. that the damage to the democratic system of government
and Australians generally arising from the restrictions on free speech on digital
platforms may be greater than the damage caused by misinformation and
disinformation. If it is to proceed, it needs a lot more work to make it acceptable.

Yours sincerely,

Anne Twomey
Professor Emerita, University of Sydney
Consultant, Gilbert + Tobin Lawyers[*]

- This submission is a personal view. It does not constitute legal advice and does not represent the views
of the University or Gilbert + Tobin Lawyers.


-----

