-----

_Fergus Ryan is a Senior Analyst and Albert Zhang is an Analyst at the Australian Strategic Policy Institute._
_We research disinformation and the deliberate manipulation of the information environment to achieve_
_strategic goals—what we will refer to here as influence operations. This submission does not reflect a_
_single ASPI perspective and is the opinion of the two individual authors._

The authors of this report welcome the opportunity to provide feedback on the exposure draft of the
Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2023. We
make 18 recommendations across this submission designed to strengthen the Bill. They are as follows:

1. Content moderation decisions should be made transparent to users and mechanisms for users to

appeal those decisions should be established.

2. Digital platforms must be mandated to respect media freedom.
3. Digital platforms should be obligated to maintain public repositories containing government

takedown requests.

4. Democratic oversight should extend to all areas where digital platforms are intervening, including

political advertising and professional news content.

5. Media outlets lacking editorial independence from a government should be clearly labelled by digital

platforms.

6. The ACMA should be empowered to revoke licences or operational rights of foreign digital platforms

that pose a threat to Australia's national interest.

7. Digital platforms must publicly disclose instances of coordinated inauthentic behaviour occurring on

their platforms.

8. The ACMA should use its information-gathering powers to assess the resources employed by digital

platforms to counter foreign interference activities.

9. The ACMA's rule-making authority concerning digital platforms' records should be expanded.
10. The ACMA should expand the scope of the Bill to consider all types of serious harms including

content, conduct and contact-based harms.

11. Third-party researchers should be granted access to digital platform data in order to enhance

effective and transparent enforcement.

12. The Bill should explicitly name a public office holder responsible for assessing digital platforms’

compliance.

13. The ACMA should also require all large digital platforms to establish an Australian corporate

presence.

14. The ACMA should work with the Treasury to create tax policies that incentivise compliance with

misinformation codes.

15. Whistleblower protection schemes should be established, allowing employees of significant digital

platforms to report malfeasance to regulatory bodies, the government, and civil society.

16. Formal working groups, comprising key stakeholders, should be established to devise best practice

measures, collaborative processes, and to facilitate information and research sharing.

17. Digital platforms should be required to actively share information about cross-platform influence

operations and foreign interference.

18. Digital platforms of a certain size should be mandated to provide open data access for non
commercial research purposes.


-----

# 1. Introduction

The Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2023
is a moderate and measured effort to strengthen and enforce the social media misinformation and
disinformation code. The Bill's objective is modest: to ensure that digital platforms have systems in place
to deal with mis- and disinformation and that these systems are transparent. While this is a positive step
forward, more can be done to strengthen the bill, as long as appropriate safeguards are included.

The Bill is timely, given mis- and disinformation is on the rise and our current regulatory approach is
inadequate. ASPI research has published evidence of CCP-linked covert networks that continue to
propagate disinformation across multiple digital platforms in 2023, including platforms headquartered in
China, such as TikTok, and platforms headquartered in the US, such as X (formerly Twitter)[1]. Most
platforms are not clearly disclosing those networks’ ongoing activities. TikTok and WeChat have never
disclosed Chinese government activity on their platforms, while Google last made a disclosure in January
2023, Meta in May 2023 and Twitter (now X) back in October 2022.[2]

With the new powers in the draft Bill, the ACMA will be able to gather information from digital
platforms and request changes to their processes for dealing with mis- and disinformation. It allows the
regulator to hold the platforms to account if their efforts prove inadequate or untimely. The ACMA will
also be able to set industry standards as a last resort. Crucially, these powers will apply to all relevant
digital platforms, even those that are not currently signatories to the voluntary industry code.

The regulatory powers in the Bill do not represent a radical departure from previous approaches. It
models similar frameworks that the ACMA already has in its remit, like broadcasting and
telecommunications. The new powers represent an evolution from the current self-regulation model to
a co-regulation model that allows for more government input and oversight over content moderation
decisions on digital platforms.

This is a step in the right direction, but more can be done to ensure a more diverse set of stakeholders
are involved in oversight, that independent researchers are given access to digital platform data to
improve transparency, that serious harm from online conduct and contact, in addition to content, is
considered and that a comprehensive whistleblower protection mechanism is integrated into the Bill so
that tech company workers can alert the government and civil society to any malfeasance.

1 Albert Zhang and Danielle Cave, China’s cyber interference narrows in on Australian politics and policy,
[July 2023, online.](https://www.aspistrategist.org.au/chinas-cyber-interference-narrows-in-on-australian-politics-and-policy/)
2 Albert Zhang and Danielle Cave, China’s cyber interference narrows in on Australian politics and policy,
[July 2023, online.](https://www.aspistrategist.org.au/chinas-cyber-interference-narrows-in-on-australian-politics-and-policy/)


-----

# 2. Freedom of expression

When digital platforms refrain from moderating content, harmful content proliferates. Conversely, as
platforms become more active in filtering content, there's an increased risk that legitimate expression is
stifled. Regulating this area requires striking a delicate balance.

The current voluntary code includes a commitment to maintaining free speech online. The first guiding
principle in the code, which should be maintained, is that the government or other parties should not be
able to compel the platforms to remove false or misleading content "if the content would not otherwise
be unlawful".[3] The Bill explicitly rules out the power for the ACMA to mandate the removal of individual
posts from digital platforms and specifically enshrines the freedom of political communication in section
60.

In the main, the draft Bill strikes a good balance between free speech and moderation of content, as
content is only expected to be moderated if it is reasonably likely to cause or contribute to serious harm.
Serious harm is defined as affecting a “significant portion of the Australian population, economy or
environment, or undermines the integrity of an Australian democratic process”- a high enough
threshold to not incentivise false positives.

However, more can be done to guard against any unintended consequences of the Bill. This would
include requirements that content moderation decisions be made transparent to users and that users
have mechanisms for appeal (Recommendation 1). Provisions in the EU Digital Services Act (DSA)
require digital platforms to make abundantly clear to users the reasons why certain moderation
decisions are made and what opportunities they have to contest those decisions.[4] Such measures would
act as a counterweight against potential overreach by the platforms.

Another measure in the EU DSA that the ACMA should consider for the Bill is the requirement that
digital platforms respect media freedom (Recommendation 2). This is important when regulating
platforms like WeChat where media freedom is impossible due to excessive censorship and even
platforms like X (formerly known as Twitter), where under Elon Musk, journalists and researchers have
been suspended for unclear reasons.[5]

Finally, digital platforms should make government takedown requests from any country immediately
transparent, so that the public can be aware of state efforts to influence what is said online
**(Recommendation 3).** As an example, Microsoft’s code-sharing and development platform, GitHub,
publishes verbatim government requests to remove content that has been declared unlawful in their
local jurisdictions.[6] Digital platforms should also create similar public repositories of government
takedown requests, subject to national security and privacy considerations.

[3 Australian Code of Practice on Misinformation and Disinformation, DIGI, 2022, online.](https://digi.org.au/wp-content/uploads/2022/12/Australian-Code-of-Practice-on-Disinformation-and-Misinformation-FINAL-_-December-22-2022.docx.pdf)
[4 Digital Services Act, EUR-Lex, October 2022, online.](https://eur-lex.europa.eu/EN/legal-content/summary/digital-services-act.html)
[5 Twitter suspends journalists who wrote about owner Elon Musk, AP, December 2022, online.](https://apnews.com/article/elon-musk-technology-business-dac21de7abb6167bb604f5317aeda10a)
[6 Gov-takedowns, GitHub, updated 29 July 2023, online.](https://github.com/github/gov-takedowns/tree/master)


-----

# 3. Scope of the powers

The scope of powers in the Bill should be expanded to include the full range of areas already being
addressed by the digital platforms. For example, some platforms are already addressing mis- and
disinformation in political advertising and professional news content. As a general principle, democratic
oversight should be extended to cover any and all areas in which platforms are already intervening
**(Recommendation 4).**

Another area in which digital platforms are already intervening, albeit inconsistently, is in labelling
‘state-controlled’ media outlets and their employees. The draft Bill gives the ACMA powers to address
mis- and disinformation from media outlets that lack editorial independence. However, requiring
platforms to label media outlets that lack editorial independence from a government will help users to
make more informed judgments about the credibility and potential biases of the content they're
consuming (Recommendation 5).

The ACMA should also be given the authority to withdraw the licence or operational rights of digital
platforms in Australia if the Minister for Foreign Affairs is of the opinion that the digital platform is
contrary to Australia’s national interest. This could include instances where the platform is deemed to
be under the editorial control of a foreign government (Recommendation 6).

Further to this, the ACMA should mandate that digital platforms, including social media platforms,
publicly disclose coordinated inauthentic behaviour, which may include attributable state-backed
influence operations, as they become aware of them (Recommendation 7). At the same time, the ACMA
should use the information-gathering powers afforded by the Bill to seek information on the staffing and
resources that platforms are, or are not, allocating to countering foreign interference and other
information-manipulation activities (Recommendation 8). The ACMA should explicitly add powers to
make digital platform rules in relation to records in section 14(1) because it is unclear if those proposed
powers are directly related to mis- or disinformation on the service (Recommendation 9).

Those records should be made public to hold those platforms to account. This will require stronger
international governance, including through mechanisms such as the G7, to form global digital standards
(akin to requirements for financial reporting, for example) in which the conditions for running a digital
platform include transparent governance and disclosure of influence operations.

As it is currently drafted, the Bill is narrowly limited to content-based harms, such as mis- and
disinformation. The ACMA should adopt a similar approach to World Economic Forum’s Global Coalition
for Digital Safety, which categorises online harms into three areas: content, contact and conduct
(Recommendation 10).[7] The Coalition defines ‘content’ as relating to harm originating from problematic
online material, including mis- and disinformation, ‘contact’ as harm occurring through online
interactions and ‘conduct’ as harmful behaviours enabled by digital technology.

[7 Creating a common language to address online harm, World Economic Forum, August 2023, online.](https://www.weforum.org/agenda/2023/08/common-language-online-harm-typology)


-----

For example, ASPI has published evidence of state-backed influence operations amplifying negative but
factual content, often without context, that harms Australian institutions and democratic processes.[8] In
that case, the content (criticisms of the Australian government and its decisions) was often legitimate
but the conduct (use of inauthentic accounts) was misleading. ASPI has also observed coordinated
networks abuse social media complaint mechanisms to wrongly silence legitimate users reporting on
China.

The Online Safety Act enables the eSafety Commissioner to regulate some online harms not considered
by the ACMA’s Bill but the Online Safety Act’s definition of harm is limited to serious physical harm or
serious harm to a person’s mental health only and does not include coordinated inauthentic behaviour,
like CCP influence operations, that harms a significant portion of the Australian population, economy or
environment, or undermines the integrity of an Australian democratic process.

The ACMA should actively monitor the threat landscape as malicious actors adapt their tactics to the
new regulatory environment and shift to more conduct-based or contact-based operations. Especially as
virtual or augmented reality platforms become more popular and conducive for conduct-based and
contact-based harms which will make the Bill outdated.

# 4. Enforcement mechanisms

Under the draft Bill, the ACMA lacks immediate punitive authority, but it retains the capacity to pursue
legal action through the judicial system when platforms repeatedly breach regulations. The ACMA’s
enforcement of the powers will be applied in a graduated manner depending on the breach and its
related harm.

The ACMA’s enforcement actions range from formal warnings, infringement notices and remedial
direction to court-enforceable undertakings and civil penalties or injunctions. However, it is unclear
from our reading of the Bill under what circumstances and on whose authority these graduated
enforcement actions will be triggered and taken.

Will platforms only be fined if the information they provide to the ACMA is untimely or inadequate? Or,
will the ACMA take action if it finds a substantial amount of mis- and disinformation on the platforms? If
the latter, the ACMA will inevitably have to form a view on what does and what does not constitute misand disinformation–something it has been at pains to say it would not do.

Effective and transparent enforcement would be enhanced by granting third-party researchers access to
platform data (Recommendation 11). Using third-party researchers, in consultation and collaboration
with a diverse set of stakeholders, would ensure that the ACMA maintains an appropriate distance from
decisions on specific online content.

[8 China’s cyber interference narrows in on Australian politics and policy, ASPI, 24 July 2023, online.](https://www.aspistrategist.org.au/chinas-cyber-interference-narrows-in-on-australian-politics-and-policy/)


-----

It is unclear how the ACMA will enforce the industry code and standards. The legislation should explicitly
name a public office holder - such as the ACMA Chair or Minister of Communications - responsible for
assessing platforms’ compliance and taking enforcement action when necessary, based on an agreed set
of thresholds or triggers (Recommendation 12).

The ACMA should also require all large digital platforms to have an Australian corporate presence
(Recommendation 13).[9] An Australian presence would give the public and government more options to
work with platforms to counter mis- and disinformation. Having a local presence would help platforms
navigate and comply with Australian regulations, and/or develop misinformation codes relevant to the
Australian context. In times of crises or emergencies, having a local team could also ensure timely and
accurate communication and coordination with the Australian Government.

Once large platforms have an Australian presence, the ACMA should work with the Treasury to create
tax policies that incentivise compliance with misinformation codes. For example, the Australian
Government could create preferential tax arrangements for platforms that set up an Australian
presence or offer tax deductions for hiring trust and safety staff focussing on matters relevant to
Australia (Recommendation 14).

Either in this Bill or in future Bills, the ACMA should also consider creating whistleblower protection
schemes so that employees at large digital platforms can alert the regulator, government and civil
society to any malfeasance (Recommendation 15). This would support a higher ethical standard for
large platforms, which have significant influence over public discourse. Whistleblower protections would
also ensure that large technology companies remain accountable for their actions and disincentivise
anti-transparent behaviour by executives.

# 5. Implementation issues

Content moderation is an inherently “wicked problem” without easy, clear-cut answers. As such, it is
more effectively managed through multi-stakeholder collaboration and coordination.[10] Engaging various
stakeholders in the process will help pool resources, best practices, and information. It will also help
ensure that interventions are balanced against the protection of democratic freedoms.

This collaboration could be modelled upon provisions in the European Union’s (EU) “2022 Strengthened
Code of Practice on Disinformation”, which require platforms to establish formal working groups,
advisory bodies and other partnerships with experts and key stakeholders across government, the
private sector and civil society to develop best practice measures and collaborative processes and to
share information and research (Recommendation 16).[11]

9 As recommended in the final report by the Select Committee on Foreign Interference through Social
[Media online.](https://parlinfo.aph.gov.au/parlInfo/download/committees/reportsen/RB000062/toc_pdf/SenateSelectCommitteeonForeignInterferencethroughSocialMedia.pdf)
10 Molly Montgomery, Disinformation as a wicked problem: why we need co-regulatory frameworks,
[Brookings, August 2020, online.](https://www.brookings.edu/wp-content/uploads/2020/08/Montgomery_Disinformation-Regulation_PDF.pdf)
[11 2022 Strengthened Code of Practice on Disinformation, European Commission, January 2023, online.](https://digital-strategy.ec.europa.eu/en/library/2022-strengthened-code-practice-disinformation)


-----

As in the strengthened EU code on disinformation, platforms should be required to proactively share
information about cross-platform influence operations and foreign interference (Recommendation 17).
This is another area in which the inclusion of third-party researchers will be crucial. Different platforms
will have varying levels of appetite for identifying foreign influence operations, and the ACMA is not
currently resourced to undertake the research required to identify this activity.

The Bill should also mandate open data access for public platforms of a certain size for the purpose of
non-commercial research (Recommendation 18). This will enable scrutiny of platform effectiveness and
ensure that the platform’s transparency reports are data-driven and not just PR exercises.

# 6. Future trends

The current prevalence of mis- and disinformation is a significant concern in itself, but the introduction
of generative AI technologies will almost certainly exacerbate the problem to unprecedented levels.
Generative AI is already rendering mis- and disinformation more credible and difficult to identify. The
proposed Bill, with its focus on scrutinising the processes the platforms have in place to deal with misand disinformation, is a flexible approach for dealing with a problem that will inevitably continue to
scale.

Another emerging trend is for social media networks to “federate”, allowing users from different
platforms to interact, as long as they are operating on the same protocol. This trend raises pertinent
concerns, as trust and safety practices have primarily been cultivated within the confines of centralised
platforms. As social media platforms "federate", it will become more complex to solve safety issues at
scale. It is not clear how the ACMA will deal with mis- and disinformation when it is happening at the
protocol level, rather than the platform level. The evolving nature of this trend underscores the need for
adaptable regulatory measures.

# 7. Conclusion

Mis- and disinformation is on the rise but our current regulatory approach is inadequate. The
Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2023
represents a much-needed effort to examine what efforts, if any, platforms are taking to deal with this
problem. As we’ve outlined, there's room to enhance the proposed co-regulation model, transforming it
into a collaborative, multi-stakeholder approach that involves a broader range of stakeholders beyond
just the government and the platforms. To effectively address mis- and disinformation and other online
harms, it's crucial to build trust among all users through appropriate safeguards and transparency
measures. Finally, the measures we've suggested will serve as checks and balances, preventing potential
overreach by both regulators and platforms.


-----

