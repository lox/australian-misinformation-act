# Critique and Amendment Suggestions:
 The Communications Legislation Amendment 
 (Combatting Misinformation and Disinformation) Bill 2023

## Author: C. Martin
 Date: 26/06/2023 [Revised]

 A. Opening Comment

 B. Two Approaches in Information Regulation

 C. The Bill’s Discrimination of Information Sources

 D. 14 Recommendations for Amendment to the Draft Bill

**A. OPENING COMMENT**

The Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill
2023, as it stands, will be a landmark change in legislation that should be of great concern to all peoples
of Australia. Under the auspice of “safety” we the Australian public are to be guarded from “false,”
“misleading,” or “deceptive” content, the meanings of which are to be adjudicated by ‘regulating
codes’ determined by the Australians Communications and Media Authority (ACMA), and subject to a
party-term revolving door of political conflicts of interests.

Aside from the tyranny of its scope, and frightening prospects of the bureaucracy required to police it,
one of the greatest issues with this bill, is the arbitrary canonisation of the dominant or preferred
‘news narrative’ of the moment—which often, and always, evolves and changes, an evolution and
change that begins with what can at first seem to be misinformation.

The draft Bill basically offloads the logistical nightmare of censorship to the private sector, and in turn,
holds such digital platform services to ransom to such an extent they will err on the side of shuttingdown information rather than allowing free-discourse, due to fear of penalties.

No doubt, the safeguard of the common good requires the preservation of truth and the dissemination
of reliable information. Government ought to have some role in this regulation—the scope, limits, and
extent a matter of debate. Disinformation is a pressing issue, especially in the wake of artificial
intelligence technologies (e.g., ‘Deep Fakes’). Yet the methodology of regulation this Bill adopts is
troubling.

**B. TWO APPROACHES IN INFORMATION REGULATION**

In question here is not content already broadly covered by the _Online Safety Act 2021, including_
regulating certain obscene types of content e.g., “intimate image” (Part 1 15), and “material that
depicts abhorrent violent conduct” (Part 1 9), but content understood more broadly as information,
chiefly, text, and audio and visual discourse.

There are two approaches in information regulation:

1) educative regulation, which counters alleged misinformation with warnings or purported true
information (e.g., ad campaigns, digital stamps, notifications), and

2) sanction regulation, which involves liability for breaches of ‘disinformation’ etc.

Page 1 of 7


-----

This Bill is far too stringent in adopting the second methodology of regulation, seeking to ratify
sanctions for digital platforms who allow ‘disinformation.’ If it is not to be rejected entirely, at the very
least, the Bill ought to be greatly mitigated and should adopt the first methodology, educative
regulation. For example, this could entail the requirement for digital platforms to issue or allow
issuance of ‘disinformation’ stamps, notifications, or labels, but should, itself, not prevent the spread
of information that for all intents and purposes is relatively benign, and in fact, may be helpful and
conducive to the public forum of debate, the development of ideas, and truth finding.

The acute problem with this draft Bill, is that it seeks to prevent the actual sharing of certain
information not typically regulated in modern democracies, and for good reason. Since this is an
approach with issues historically well attested to be associated with state enforced censorship of
information, e.g., Nazi Germany, Soviet Russia. This is an authoritarian model, fraught in itself, and
liable to future abuse. It would be healthier to seek to educate and counter certain information within
a model that engages with, and does not, shut down the public forum of debate—a democratic model.

The strengths of the educative model of regulation with respect to information content is made more
apparent when considering the arbitrary nature of sanction regulation, especially “because there is
significant ‘gray area’ regarding what actually constitutes fake news” which “is difficult to define.

[Especially given] news sites fall on a spectrum” (Andorfer, 2018).[1]

**C. THE BILL’S DISCRIMINATION OF INFORMATION SOURCES**

The draft Bill discriminates against private or non-institutional sources of information (Schedule 1 Part
1 Clause 2, pg.5). “Professional” and “accredited” sources of information, are categorised as “excluded
content for misinformation purposes,” rendering these immune from the regulations against
‘disinformation’. The fact such exceptions are made for all these sources of information is
demonstrative of the unreasonable nature of these ‘disinformation’ regulations, so unreasonable that
these sources are made exempt from them.

Yet it is okay to impose them on the private sector’s communication, e.g., the business sector, the
armature journalist or blogger, the non-incorporated association, the average or professional
individual (e.g., even a licensed doctor, or university professor, commenting in their domain of
expertise).

This Bill effectively monopolises the freedom of information to the public or institutional sector, and
greatly infringes the rights of the private sector, which as the Bill stands, is a sector only immune from
these proposed provisions should it be engaged in “entertainment, parody or satire.” Thus, private
sector good faith purposes of a serious nature, such as an educative purpose, or engaging in research,
_are not protected. In this respect, the proposed legislation is remarkably intolerant and condescending._

**D. RECOMMENDATIONS FOR AMENDMENT TO THE DRAFT BILL**

On the assumption this Bill, regrettably, will remain largely as it stands in adopting a sanction regulation
model of information regulation (i.e., censorship), I make the following specific recommendations for
amendment:

**216F Schedule 9—Digital platform services Broadcasting Services Act 1992**

MAIN AMENDMENTS—SCHEDULE 1

1 Alexandra Andorfer, “Spreading like Wildfire: Solutions for Abating the Fake News Problem on Social Media
via Technology Controls and Government Regulation,” Hastings Law Journal 69.5 (2018): 1428, 27.

Page 2 of 7


-----

**1.** **Schedule 1 Part 1 Clause 2 (pg.5)**

**Problem: “excluded content for misinformation purposes” states, “a) content produced in good faith**
for the purposes of ‘entertainment, parody or satire’”. This is the only category of exemption for the
non-institutional communications sector (e.g., neither professional news, government, or accredited
education etc.), that is, the private sector.

**Rec. Amendment: This needs amendment to include greater scope of excluded content produced in**
the private sector. E.g., include i) content produced for private exchange between individuals,
businesses, and organisations, ii) content produced in good faith for the purpose of education,
research, critique of other media content, religious education, for the promotion of goods or services.
Also, iii) content produced with an explicitly stated disclaimer to the effect of opinion etc., e.g., ‘opinion
only, educational purposes only.’ These provisions would be qualified by already extant legislation, for
example, by the Online Safety Act 2021.

**2.** **Schedule 1 Part 1 Clause 6 (pg.11-12)**

**Problem: At present, document sharing platforms (e.g., Google Docs, Microsoft Teams) are apparently**
subject to ‘misinformation,’ apparently also even private video and audio sharing. Since “Excluded
services for misinformation purposes,” 1b “a media sharing service that does not have an interactive
feature” – these do have interactive features albeit can be private or quasi-private (e.g., Google Docs).
This is a substantial deficit and should not be left to the Minister (1c, 2) to elect.

**Rec. Amendment: Clause 2 definition “Private messaging” should be redefined broader as “private**
content” to encompass private audio and video messaging or interfacing, and private document
sharing. And/or Clause 2 “all content produced or shared privately” should be added to the list of
“excluded content for misinformation purposes.” And Clause 6, “Excluded services for misinformation
purposes”: include content sharing digital platforms that are not public i.e., restricted access or private.
These provisions would be qualified by already extant legislation, for example, by the Online Safety Act
_2021._

**3.** **Schedule 1 Part 1 Clause 7 (1c) (pg.12)**

**Problem: 1c: “the content is provided on the digital service to one or more end-users in Australia,” this**
is too broad, it needs further limitation of scope, especially to safeguard undue invasion of privacy,
radical breach of freedom of speech (even into private free speech), and information not intended for
_public dissemination. The scope as it stands, by including content shared between “one or more end-_
users,” thus contradicts and contravenes the principle already protected and recognised when it
exempts “private messages” (Sched. 1 Part 3 Div. 3 clause 34, pg.30) and “an email service” (Sched. 1
Part 1 clause 6, pg. 11). ‘Misinformation’ and ‘disinformation’ should only apply to properly public
content. Hence, the current text goes beyond its stated mandate and presents legislation that could
itself become “a threat to the safety and wellbeing of Australians.”[2]

**Rec. Amendment: Change 1c to read: “the content is provided on a digital service available to the**
public in Australia.” “Public” would require re-defining as it currently stands in Sched. 1 10, “When a
service is provided to the public etc.,” changing to accommodate any content shared publicly.

2 Department of Infrastructure, Transport, Regional Development, Communications and the Arts,
_Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2023—Fact_
_Sheet (June 2023), https://www.infrastructure.gov.au/sites/default/files/documents/communications-_
legislation-amendment-combatting-misinformation-and-disinformation-bill-2023-factsheet-june2023.pdf.

Page 3 of 7


-----

Or worse case, e.g., 1c: “the content is provided on a digital service to more than a hundred end-users
in Australia,” viz., this in view of b) content (clause 2) and services (clause 7) excluded for
misinformation purposes.

**4.** **Schedule 1 (Part 1) Clause 7 (1d) and (2d) (pg.12)**

**Problem: 1d: “the provision of the content on the digital service is reasonably likely to cause or**
contribute to serious harm.” The text, especially given the definition of “harm” (Clause 2) has too broad
a scope and requires limitation. Namely, it operates on assumption of capacity to reasonably predict
caused harm or contribution to serious harm. How can one predict certain information will reasonably
cause or contribute to harm? Too broad a scope for application, open to serious abuse, and relativistic
or arbitrary judgement. “Contribute to serious harm” is too broad a scope since any information can
be said to contribute to serious harm if one simply asserts it. As Andorfer (2018) states, “To punish fake
news via a claim other than fraud or defamation would [or we can argue, should] require a ‘direct
causal link’ between the speech in question and the harm that resulted.”[3]

**Rec. Amendment: Change to demonstrable evident fact: “has demonstrably caused harm.” This would**
also require amending sub-clause 3 (pg.12).

**5.** **Schedule 1 (Part 2 Division 3) Clause 19 (4) (pg.22)**

**Problem: Sub-clause 4: “However, a notice cannot require a person to give information or evidence,**
or produce a document or copy, that would reveal the content of a private message.” This is too narrow
and should be broadened, especially to take into consideration confidential documentation (e.g.,
medical or otherwise), and any private content stored and shared on digital platforms in general—
keeping in mind, harmful content is already subject to the Online Safety Act 2021.

**Rec. Amendments: “…any content (i.e., text, audio, video) stored or shared in private, including but**
not limited to content subject to confidentiality.”

Add a provision that documentation cannot be required according to clause 19 apart from a
notification from the ACMA that includes specifying which code of misinformation or disinformation
is in question (this ought to not be a generic resolution code, but a lower resolution more specific
code), a code that must be limited to those listed and accessible on its public register of misinformation
and disinformation. Hence removing the assessment of misinformation alone to allow information to
be supplied under clause 19 and requiring the ACMA to supply the grounds of misinformation.

**6.** **Schedule 1 (Part 2 Division 3) Clause 21 (pg.24)**

**Problem: Broadened contravention of the common law right against self-incrimination. An individual**
is not protected from self-incrimination in supplying mandated evidence under Clause 19 and 20 of
the same Division. It is akin to the provisions of the Online Safety Act 2021 on self-incrimination (Part
4 Div. 3 Subdiv. C clause 63 sub-clause 2), yet more problematic due to the scope of powers to require
basically any information of private persons except private messages. At present, these provisions
(clause 19 and 20) could be used in bad faith to acquire information from a targeted person simply
believed by the ACMA to have relevant information in relation to the provisions of ‘misinformation’
and ‘disinformation,’ hence allowing information fishing to incriminate in the absence of reasonable
suspicion, equivalent to an unjust ‘stop, search and seizure.’

3 Op.cit., Andorfer, “Spreading like Wildfire: Solutions for Abating the Fake News Problem on Social Media via
Technology Controls and Government Regulation,” 1428.

Page 4 of 7


-----

**Rec. Amendments: A provision that better balances the individual’s interests with the public’s interest**
(Traditional Rights and Freedoms—Encroachments By Commonwealth Laws (IP 46), 11.47 and 11.49).
Thus give greater protection from self-incrimination for information and documents required in accord
with clause 19 and 20.

Also, provision that information is inadmissible in evidence against the individual if found to be
obtained on unjust grounds beyond the scope of its mandate. This in turn would require a provision
that the ACMA log a report prior to issuing notice requiring information under clause 19 and 20 stating
why “the ACMA has reason to believe that the person: i) has information that is relevant” etc., (clause
19 sub-clause 1).

**7.** **Schedule 1 (Part 3 Division 3) Clause 34 (pg.30-31)**

**Problem: Limitation—private messages only, expand this limitation. ‘Misinformation’ and**
‘disinformation’ should not include any private information shared between few users.

**Rec. Amendment: Add subclause or c) any other content privately shared [qualify / specific if needed].**
Any such ‘private content’ that is harmful is already regulated under the Online Safety Act 2021.

**8.** **Schedule 1 (Part 3 Division 3) Clause 35 (2c) (pg.30-31)**

**Problem: Limitation—electoral and referendum matters, (2) authorised content, (c) “matter**
communicated or intended to be communicated for the dominant purpose.” “Dominant” too vague
and arbitrary a criterion, subject to abuse, undermines democracy and free discourse on electoral and
referendum matters.

**Rec. Amendment: Restrict by removing “dominant,” thus reading, “matter communicated or intended**
to be communicated for the purpose…”

**9.** **Schedule 1 (Part 3 Division 7) Clause 55 (4) (pg.48)**

**Problem: Publication of codes registered on website of the ACMA or other government website**
unclear with present wording, “The Register is to be made available for inspection on the internet.”
Clearer accountability and transparency needed, especially for the public.

**Rec. Amendment: Provisions requiring ACMA to provide the register on a publicly available website**
with each purported ‘misinformation’ and ‘disinformation’ category clearly noted.

**10.** **Schedule 1 (Part 4) Clause 61 (pg.50)**

**Problem: Acquisition of property related to the provisions introduced by this Bill noted to not extend**
beyond the just terms already noted in the Constitution (para. 51 xxxi), but lack of safeguard apparent
here, requires restriction.

**Rec. Amendment:** That corporate or private property cannot be acquisitioned by reason of any of
these provisions alone. (Possibly covered by 64.2b ii).

SCHEDULE 2—CONSEQUENTIAL AMENDMENTS AND 1 TRANSITIONAL PROVISIONS

_Broadcasting Services Act 1992_

**11.** **Schedule 2 Clause 13 (pg.55)**

Page 5 of 7


-----

**Problem: See this draft amendment to Broadcasting Services Act 1992 (Sched. 2 Clause 13, pg.55).**
There is extremely limited, near absent, provision for appeals to be made against the ACMA’s decisions
under the digital platform rules. The digital platform rules allow for non-reviewable decisions (see
Sched. 2 Clause 13), this is unacceptable. In connection with this, there is a lack of measures in place
to safeguard the rights of the digital platform provider, and the individual or group user who posted
content subject to a registered code of ‘disinformation’ or ‘misinformation,’ from abuse by the ACMA,
who is not restrained by the draft amendments from supressing information arbitrarily judged as
“disinformation” or “misinformation,” according to its flexible criteria. That this suppression is
‘indirect’ is inconsequential.

**Rec. Amendment: Include clearer provision for appeals to the Administrative Appeals Tribunal after**
decisions of breach by the ACMA, by i) adding a provision that all final decisions of ACMA are subject
to review and appeal, by both the digital platform and content poster; ii) the content is not to be taken
as “misinformation” or “disinformation” until after the appeal is heard, and unless the response is
negative (i.e., in favour of the ACMA decision), and iii) the digital platform provider and person in
question, is not obliged to remove the content until the appeal is responded to and unless the decision
of the ACMA is affirmed.

**12.** **General**

**Problem: Lack of oversight of ACMA. It should be subject to audit by another government body that**
ensures adherence to limitations, in view of the common good, and within its own body a more regular
sector that reviews its judgements e.g., Sched. 1. Clause 37 Registration Codes, where the ACMA is
free to consider if a given code restricts political freedom or does not (see also Sched. 1. Clause 45,
pg.40).

**Rec. Amendment:** Provisions requiring ACMA to give regular reports to maintain accountability,
including instances of complaints and appeals.

**13.** **General**

**Problem: Cutting out the individual from the government regulation of ‘disinformation’ and**
‘misinformation,’ effectively becoming an exclusionary dynamic between the ACMA and digital
platform providers yet affecting the individual without effective representation and recourse.

**Rec. Amendment: Provisions that allow individuals or groups to lodge complaints and appeals against**
ACMA regulated codes, to lobby for a code’s revocation (see Sched. 1 clause 52, pg.45). For example,
should such regulation lead to the suspension of use or posting capacity of an individual or group on
a digital media platform. The individual should have the right to be notified what code their content
has breached, should it be removed from a digital service platform.

**14.** **General**

**Problem: The ACMA regulated misinformation codes and standards can readily discriminate against**
broader types and categories of information, effectively shutting down discussion on entire topics,
even apart from perspectives delivered with balance and disclaimers.

**Rec. Amendment: Provisions that clearly restrict and limit the regulating misinformation codes and**
standards that the ACMA can ratify.

Page 6 of 7


-----

Page 7 of 7


-----

