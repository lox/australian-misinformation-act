# Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2023

**Summary: This bill is not fit for purpose in a democracy. It should not be submitted to the parliament in its current**
form.

## Government misinformation about this bill

### Freedom of Speech

The Guidance Note says:

_The Bill does not seek to curtail freedom of speech_

However curtailing freedom of speech is actually the dominant purpose of this bill.

You may not value the speech, because it is “misinformation”, but the dominant purpose of this bill is to prevent
that speech. So please don’t lie about it.

There is nothing in this bill that specifies what approach to “misinformation” service providers must or must not
take.

### Targeting specific content

The Fact Sheet says:

_The ACMA will not have the power to request specific content or posts be removed from digital platform_

_services._

This is disingenuous at best. There is nothing in this bill that specifies that. The bill is almost completely vague
about what a Code or Standard might say on this topic.

If you expect claims like this one to be taken seriously then you should write them into the bill.

### Truthfulness

The Fact Sheet says:

_The ACMA would have no role in determining truthfulness_

There is nothing in this bill that specifies that. A Code could specify that ACMA is the sole arbiter of truth. ACMA
would have the option, but not the obligation, to decline to register such a Code. If the result is that there is no
Code then ACMA would have to develop a Standard. There is nothing in the bill that prevents ACMA specifying in
the Standard that it has a role in determining truthfulness.

If you expect claims like this one to be taken seriously then you should write them into the bill.

The bill should specify that ACMA must decline to register a Code which gives any government or any government
agency any role in determining truthfulness – and that likewise ACMA is prevented from promulgating a Standard
which provides for such a role.


-----

_won’t._

### Accountability

The Fact Sheet says:

_The proposed powers are designed to encourage digital platform services to be accountable for improving_

_and implementing measures to counter the spread [of ] misinformation_

It appears that the regime has actually been designed to ensure that the government is not _accountable for what_
happens under this regime. Whatever happens (bad), the government can say that it is all industry’s doing: “They
designed the Code. They implemented the Code. It’s nothing to do with us.”

### Summary

If I were a service provider operating under this regime, I would be blocking https://www.infrastructure.gov.au

## Justification is lacking

This is a large and burdensome regime considering that there are absolutely no real world examples given.

Surely the government should be obliged to give actual real world examples of speech that has been allowed to
occur on digital services in the recent past but which would be “prevented” under this regime i.e. speech that
caused “serious harm” that occurred in an Australian context.

## Regime flaws

### Very Vague

There are no requirements and almost no restrictions (anti-requirements). A Code could contain just about
anything. That leaves open the possibility of “mission creep” i.e. the slippery slope. Conversely, there are very
limited restrictions to determine what can’t appear in a Code.

While the bill pretends to protect “freedom of political communication”, the bill only requires ACMA to consider
this. The bill does not obligate ACMA to refuse to register a Code where the burden is unreasonable.

Because there are no requirements, there is also no obligation on ACMA to register a Code that meets the
requirements. That is, ACMA is completely free to withhold registration unreasonably.

Among the many missing requirements is any requirement to give “due process”. “due process” would at least
require that a service provider always explicitly notifies the end user when content is interfered with under the
provisions of this bill. Such notification would have to include the harm letter, (a) to (f). Just secretly removing
content or rendering content visible only to the end user who posted the content is not on.

Another missing requirement relates to partial suppression. A service provider obviously has the option to suppress
a post fully. I believe that a service provider should not have the option to suppress a post partially or to alter a post
in any way. That is, a post should be made public in whole or not at all. This is because of the potential to
misrepresent what was actually posted, to cause immense confusion, and the general creepiness that a service
provider could put words into the mouth of someone who posts.

In general, there are no restrictions that enforce a basic minimum standard of respect for human rights by a service
provider There is no obligation on ACMA to refuse to register a Code that violates even the most basic standards


-----

misinformation purposes ), it is not absolutely the case that in practice the service provider could not violate those
restrictions. That is, the service provider could always do more than what is written in the Code.

The effect of allowing ACMA unfettered power to register and to refuse to register a Code with no requirements
and no restrictions effectively means that the parliament is abdicating its legislative role to ACMA. Effectively, no
_member or senator who votes for this bill actually knows what he or she is voting for, now or in the future._

### One size fits all

The bill allows the $100 billion behemoths of the industry sector to design a Code, and then impose that Code on
their much smaller competitors (and potentially on other companies who are not competitors). This is both anticompetitive and unreasonable.

As a simple example, if the Code specifies that a service provider shall respond within 4 hours to something or
other, that advantages players that can afford to employ staff in all timezones and disadvantages a start-up who
only has staff in one timezone.

There should be an exemption for smaller players i.e. a threshold (as measured in revenue, profit or active
_Australian users – or some combination thereof) below which the player is exempt from the regime. While it is_
understood that that creates safe spaces for “misinformation”, it should be acknowledged that the ability to cause
“harm” is much lower for a platform with 100 active users as compared with a platform with 100 million active
users. (It is also likely that a platform with 100 active users operating as an echo chamber will continue to believe
what they believe regardless of interference from the Australian government – and indeed that interference can be
used to reinforce the mentality.)

If there are no exemptions for smaller players then there ought to be a requirement for a set of Codes, and which
Code is applicable depends on the size of the player.

It is true that ACMA has the flexibility to accommodate this but it is also true that ACMA has the flexibility to fail
to accommodate this.

The size of the fines are obviously completely inappropriate when you take into account the vast disparity in the
size of the entities who might come under this regime. I believe that ACMA does not have the flexibility to
accommodate this, in the sense that ultimately the size of the fine is the court’s decision, not ACMA’s, and the
legislation itself does not accommodate this.

### Deregistration

ACMA has unfettered power to renege on a previous agreement to register a Code i.e. without providing a reason,
much less justifying it.

ACMA has the power to remove an individual provision from a Code without regard to whether that makes a
material difference, a difference to the extent that the industry sector would never have proposed the Code in the
first place without that provision. I am unconvinced that ACMA should have any power to vary a Code unilaterally.

### Extra-territoriality

The bill is somewhat vague on the question of when it is triggered. Clause 3 specifies that the content must be
“accessible to” “one or more users in Australia”. That effectively means the entire internet.

One could have a group discussion between a bunch of Albanians (and no Australians) and the Australian
government thinks that it is appropriate for the Australian government to stick its oar in and interfere in that


-----

and conflicting requirements that are literally _impossible for a service provider to comply with._

As an example, consider the Armenian Genocide. It can be said that denying a genocide is a way to encourage it to
occur again, and genocide is certainly “serious harm”. The government of Armenia has “its truth” and the
government of Turkey has “its truth”. It is not possible for a service provider to resolve that. The government of
Australia might eventually have to take a position as to which government’s truth we get to see, or indeed a
separate version of “the truth” that would be created by the Australian government might be what we get to see.

A more reasonable definition of when this bill applies outside Australia would contain two limitations:

1. To come under this regime, the content must actually be delivered to a user in Australia.

2. Any interference by the service provider must be limited to the view of the content made available to a user

in Australia. So content does not just, for example, disappear. It only disappears from the Australian view.

This ensures that interference is limited to where it “must” occur (doesn’t waste time on content that never comes
anywhere near Australia) and ensures that contradictory and conflicting requirements can never arise.

### Legal certainty

Summarising some of the previous points, this bill completely fails to provide legal certainty for service providers
who operate under this regime.

There is nothing that must appear in a Code, and nothing that must not appear in a Code.

ACMA can arbitrarily approve any Code, or arbitrarily renege on its previous approval, or arbitrarily impose its
own Code (i.e. Standard). ACMA can arbitrarily remove individual provisions from a Code.

ACMA can arbitrarily decide on how service providers are segmented. To elaborate on this point, there is such
uncertainty as to what section or subset of section a service provider has been arbitrarily deemed by ACMA to be in
that there ought to be a legal requirement for ACMA to provide explicit notification to a service provider that it is
covered by a specific, publicly available Code (or Standard).

Because sections are not mutually exclusive, it is unclear how a service provider will comply when the multiple
Codes (or Standards) that apply to it are conflicting. I am not referring here to the situation that a service provider
provides more than one service and the two services are covered by different Codes (or Standards).

I think ACMA will be flexible enough to resolve this in practice but there is no obligation on it to do so, nor to
avoid the situation in the first place. The regulatory regime would be simpler and cleaner if any given affected
_service is covered by exactly one Code (or Standard) i.e. is mutually exclusive._

## Other criticism

### Overcensorship

Invariably in a regime like this, the service provider will play it safe. Faced with the potential of massive fines, and
in general providing a free service, it is logical for the service provider to err on the side of caution. This is without
even considering self-censorship by individual people who are posting content to the internet. Hence whatever the
government thinks will be the level of censorship, the actual level will likely be higher.


-----

While the bill purports to protect the content of private messages, it is not clear that private messages are
completely outside the scope of this regime. That is, the service provider could simply be required to block the
_transmission of private messages._

Also, while the content of private messages may be protected from reporting to ACMA and thereby disclosure, the
_metadata pertaining to those private message is not protected in the same way._

It is not clear that Clause 14(3) actually protects the content of private messages. If the intention is that the content
of private messages is completely outside the scope of this bill then perhaps the language should be strengthened
by adding after “make or retain records of the content of private messages” the text “or to examine the content of
_private messages”. As it stands today, this item could be construed as permitting aggregated or statistical reporting_
of the content of private messages.

The actual definition of “private message” is a bit dodgy because it hinges on the definition of “instant message”
and “instant message” is not defined. There are clear situations where a message would meet the definition of
“private message” (and the sender would have legitimate expectations that it would be, you know, private) except
that it fails to meet the requirement of this definition because it is not called an “instant message” by the correct
(unspecified) party.

### Private messaging

The bill is confusing as to when and whether private messaging is outside the scope of this regime. The distinction
seemingly being made in the Guidance Note at 2.1.3 is unsatisfactory. While in most existing cases the distinction
between “instant messaging” and “restricted post” is evident in practice, there are two problems here.

1. It violates the legislative principle that equivalent acts should get equivalent results. As it stands today, I

could consciously bypass censorship by choosing “instant messaging” even though my content will reach
the exact same set of people. This doesn’t make a lot of sense.

2. Worse still, the legal distinction encourages a greater future technological distinction. Users and hence

service providers will gravitate towards mechanisms that are exempt.

As a related example, if I make a post to social media but I choose to use a “custom” audience (i.e. choose which
specific set of friends will receive the message) then I can choose to use an audience of just one friend. So what is
essentially a person-to-person private message between me and that one friend of mine would still be subject to
censorship and reporting.

I propose that this should all be fixed by limiting this entire regime to content that is genuinely made public.

That is to say, if content is not made available on a restricted basis then it is made public, and is subject to this
regime, but if content is made available on a restricted basis (to a defined set of users) then it is not subject to this
regime – regardless of the technology used or the type of service or what the service provider calls the type of
service.

The practical effect of this would be to hit web forum content harder than social media content. Most web forums
make content available for reading to anyone in the world, without registration i.e. everything is public – and the
person posting the content doesn’t control that anyway. On the other hand, with social media, the person posting
the content decides whether the content has restricted access (e.g. friends only – hence by definition registered
users only) or public access (anyone in the world).

One might legitimately question whether an “influencer” with millions of followers could rely on such an
exemption but I believe that existing social media platforms can distinguish adequately between “friends” and


-----

still relying on such an exemption e.g. 9999 friends.

### Serious harm

The bill explicitly defines “harm” but in fact the bill’s operation depends on the meaning of “serious harm” but it
does not define “serious harm”. It is true that some (fictitious) examples of “serious harm” are given outside of the
bill.

### Accountability

ACMA may publish something to let the public know how this regime is operating but there is no obligation on it
to do so. No accountability there.

ACMA is being given too much power, given that it is unelected and unaccountable.

### Exempting yourself

It is embarrassing that the government exempts itself from this regime. What’s sauce for the goose is sauce for the
gander. You may claim that no government will ever produce misinformation but, as documented above, that is not
true and, more generally, it is absurd to think that no government will ever produce misinformation. There’s a
certain arrogance to the idea that government will never produce misinformation.

You might claim that no government will ever produce misinformation that will cause “serious harm” but then the
present government has made so much political mileage out of the so-called Robodebt thingy where reportedly
some welfare recipients committed suicide. “suicide” might fall within the (missing) definition of “serious harm”.

If you don’t buy that example, how about: “Iraq has WMD”? How many soldiers died in Iraq? “death” and
“serious injury” might fall within the definition of “serious harm”.

The bottom line is that if no government will ever produce misinformation then there is no problem in applying the
regime equally to yourself. If nothing else, it’s not a good look to be exempting yourself.

### Historic “misinformation”

History is replete with examples of “misinformation” that we now know to be true.

“asbestos is a health hazard”

“thalidomide is a health hazard”

“tobacco is a health hazard”

Had your regime been operating at the time, these would have been blocked by the Albanese government as
“health misinformation” and “harmful to the economy”.

The earliest (known) media report about the effects of burning coal to cause global warming appeared in 1912. No
doubt at the time the Albanese government would have blocked that too as “alarmist misinformation” and “harmful
to the economy”.

It is healthy in a democracy to be able to challenge the orthodoxy. It is healthy in a democracy to be able to make
statements that conventional wisdom says are “false”.


-----

It is borderline illegal to get companies to do “voluntarily” what the government itself would be prevented from
doing. This isn’t the first time a government has tried this and won’t be the last – but the government should always
be called out for it.

### Search and aggregation

It is unclear that it is reasonable to include search and aggregation services within this regime. At the end of the
day, they don’t provide any content. The content exists whether or not search and aggregation services engage with
it. If you are successful at getting the content provider to censor the content then it doesn’t matter what happens
with search and aggregation. The content will not be accessible, whether referenced by the search or aggregation
service or not.

### Standard v Code fines

It is a pretty low act from government to make the fines for a Standard 2.5 times the fines for a Code. The
justification given is that this would only apply where a Code has been developed but it proves to be irredeemably
ineffective, in particular due to non-compliance. However it is also a possibility that no Code was ever developed
because there is no agreed industry representative body, or there is an industry representative body but they fail to
reach agreement on a Code.

On balance, given that the goals are the same, whether it’s a Code or a Standard, and that the non-compliance could
be the same, the fines should be the same.

### Defamation

The bill ought to indemnify any service provider against a defamation claim arising out of the service provider’s
compliance with the provisions of this bill.

### Dating Sites?

You don’t think that’s a bit icky? Surely there must be about zero evidence that anyone has ever distributed
“misinformation” using a dating site. In any case, having the government stick its oar in to what is surely a rather
personal and intimate aspect of someone’s life is not a good look.

If a person specifies in a dating profile that he or she only wants to have sex with people of a certain racial group
because of some perceived negative aspect of other racial groups then, despite the “harm”, for the government to
be legislating about whom a person can prefer to have sex with is overreach.

## Other comments

### Parody / Satire

It is unclear how the “parody / satire” exemption will work in practice. These days it is so difficult to distinguish
between satire and reality. If you had told me 3 years ago that the Australian government would be censoring
“misinformation” I would have thought that that is satire.

On the one hand, this seems like a loophole in some cases of “harm” but on the other hand it hinges on “good
faith”, a rather nebulous term.

It is questionable as to whether any automated software solution will be able to detect the difference between


-----

satire” in “good faith” anyway – so the exemption is not worth much.

### Interactive feature

The term seems in some ways poorly chosen (poorly named) and the definition overly complex and expansive.

It could be simplified down to “end users post content” while not overly restricting the government overreach. (In
other words, that by itself would still catch most of the same service providers.)

It also illustrates how it could adversely impact on non-core aspects of a web site.

If all the content of the web site is posted by the web site operator, and none by end users, the web site might still
fall within the scope of this bill because end users can “like” content and the total count of “likes” (or the list of
users who “liked” or both) is visible to end users. A sensible web site operator might choose to make the “like”
mechanism invisible to end users i.e. end users can still “like” but they won’t ever see whether anyone else has
“liked”. (This presumes that the software being used by the web site operator offers the functionality to allow
“like” but make it invisible to end users.)

This does raise a question as to how “observable” is to be interpreted. Does “observable” mean: it is displayed
explicitly to the end user? Or does “observable” mean: it is inferrable by some indirect means?

Going a bit further, suppose all the content of the web site is posted by the web site operator, and none by end
users, but end users do get to make comments on the content of the web site, even though this is non-core
functionality. Again, a sensible web site operator might choose to disable the commenting functionality (and again
assuming that the software offers the choice to disable the functionality).

In either case, one might very reasonably question whether there is any evidence at all that anyone has ever used
such a web site to spread “misinformation” i.e. this is unnecessarily constraining web sites.

Ironically, with the offending “interactive features” disabled, the web site might still be a good way to spread
“misinformation” if the content provided by the web site operator itself is “misinformation”.

The concept of “sharing”, as relevant to this definition, is not well-defined. In particular, the situation could arise
that the web site itself does not offer “sharing” at all but instead “sharing” is done by the client. This would
particularly be relevant where the web site is accessed via an app i.e. the client is an app – since in that case one
end user might use one app and another end user might use a different app and the operator of the web site might
not control which app an end user uses (or even have knowledge of what apps exist). So, to the casual observer,
there is no “sharing” functionality at all – but if you download the right app then suddenly there is “sharing”
functionality. To summarise that point: whether “sharing” is a possibility is not necessarily an attribute of the
service provider or the service but instead could be an attribute of the client software used to access the service of
that service provider.

It is unclear how this definition would relate to Wikipedia. It certainly meets the “end users post content” test in the
literal sense – all the content is from end users. However end users do not post content in the conventional sense,
such as occurs in social media (and there is no mechanism for “liking” or “sharing”). While clearly it is the
antithesis of the mission of Wikipedia to host “misinformation”, some errors definitely do slip through and it is also
possible that “misinformation” slips through – although that is going to depend on who is determining what is
“misinformation”.

One thing is abundantly clear. Wikipedia is a great resource for the internet – and anything that makes their life

diffi lt ld ightl l d t d ti f th A t li g t (Thi ld l t b k t th


-----

service and is a not-for-profit and has no registered users as far as consuming content is concerned.)

### Ineffectiveness

From experience in other authoritarian regimes, people learn how to say things in ways that evade the censors.

### Standard

I believe that ACMA should develop and publish the Standards that would apply if no Codes are developed. The
Standards would be a Schedule to the bill. This should happen before this bill comes before the parliament. That
would then establish the BATNA for industry and also better inform everyone, including members of parliament,
how this regime might work in practice.

## A better way forward

The legislation would be fixed up in order to address many of the issues raised above about the legislation itself.
The legislation would specify the minimum standards of respect for human rights that are expected of service
providers in addressing “misinformation”. Non-public communication would be excluded. Extra-territoriality
would be required to be addressed in a sane way. There would be mandatory reporting to the public as to what is
happening in the operation of the regime. The fines would be proportionate. Missing definitions added. etc. etc. etc.

All power to set rules would be taken away from ACMA. ACMA’s role would be limited to monitoring compliance
and initiating prosecution or other action for non-compliance.

Rules would be set in Regulations that would be altered by the Minister using a disallowable instrument. This
effectively abandons the idea of Codes, and effectively only has Standards.

The rules would clearly specify how services are divided up, both by type of service, and by the size of the service,
into mutually exclusive groups.

The rules would clearly specify who is the arbiter of truth.

The rules would clearly specify what a service provider must achieve in terms of “misinformation” but not how it
must achieve it. Consequently the rules would be mostly focused on targets around false negatives and false
positives, and timeliness of addressing “misinformation”.

This would be more transparent and more accountable and more likely to take into account the human rights of the
people whose speech is actually affected.


-----

