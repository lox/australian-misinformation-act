To Whom It May Concern

Thank you for taking my comments, and the comments of the public, into consideration during the
drafting and implementation of the “Communications Legislation Amendment (Combatting
Misinformation and Disinformation) Bill 2023” (Henceforth referred to as the “Bill”) which concerns
them and their interaction with the information environment including their ability to access, engage
and communicate information.

It is evident that the purpose of the Bill is to solve a current and emerging set of problems that are a
threat to the stability of our institutions and our economic, cultural, and democratic systems. These
threats are very real however on reading the bill it is evident that this legislation will act more like a
hammer when instead carefully applied tweezers are needed. Furthermore, the Bill does not include
mentions of key points of concern in this space, namely the use of bots or fake accounts for the
purpose of astroturfing. This is a concern as an individual sharing content behind their name and
reputation should not be treated with the same extreme prejudice that one might with fake
accounts/bots disseminating information perceived to be misinformation/disinformation. The lack of
distinction and failure to address specific points of concern in the Bill in favour of being broad and
general means that the Bill will likely require future adjustments, the creation of a new Bill, or in the
near future for ACMA to create its own misinformation codes (as defined in Div 5: 50).

On the webpage https://www.infrastructure.gov.au/ where public feedback is elicited, it is asked
whether the Bill strikes a balance with freedom of expression. In my opinion, this balance can be
struck by ensuring that misinformation codes include a provision that allows freedom of speech for
individuals on platforms who are posting behind their name and reputation (even if that must be
verified). This would not include calls to commit crime or illegal activity but will provide a layer of
protection for freedom of expression so that misinformation codes can be developed bearing this
provision in mind rather than allowing the censoring of individuals. Furthermore, free expression can
be protected by ensuring that citizen journalists, small independent groups and emerging new
organisations can be labelled “professional news content”.

**Why this is a critical issue to get correct and why nitpicking is necessary.**

Lawmakers and public institutions have the difficult problem of protecting the Australian public, in
this highly complex environment, both now and in the future. The restriction of expression,
information sharing, and ability to access different points of view through law must be approached
carefully as these are rights that are fundamental to a society that aims to ensure the freedom of its
citizens. Regardless of intent and initial use of legal instruments, there are always unintended
consequences of intervening in a complex system (such as our collective information environment)
and always the potential for misuse. This is why specificity must be employed in this situation
because for the first time in human history, due to technology, a complete and immovable
totalitarian state is possible and lawmakers must defend against this rather than pave the way for
systemic control (even if it is paved with good intentions). The horrors of 20[th] Century (or modern
day China) are a warning sign of what can happen once a society/culture loses its way. We are not
immune now and won’t be in the future. Free speech is both a defence against this possibility and
will be the first victim if such a state should arise. Hence, use metaphorical tweezers, proceed
carefully, make laws with care, don't make it easy for the would-be controllers of humanity by
centralising the power to censor, because they do exist, have existed, and will exist.

**Specific issues:**

1) Stifle innovation: (Div 3: 32, Div 4: 37 – 39)


-----

The Bill requires that representatives of a section of the digital platform industry create a
misinformation code that will then apply to all other platforms in this section of the industry.
This eliminates the chance for a variety of solutions to be devised and tested by various
companies (or within the same company). Misinformation and disinformation are complex
problems, and the best solution is unlikely to be the first or most obvious. By restricting the
number of approaches that can be implemented there is a reduction in the chance that best
practices will be uncovered. An example of differing approaches is playing out now, Twitter
(now “X”) utilises “community notes” allowing users to act as an aggregated fact checker
compared to Facebook utilising third-party fact-checkers. It is not clear which of these or if
another method is best practice for managing misinformation/disinformation, however the
only way to determine that is to allow these companies the freedom to compete, compare
and contrast the approaches.

Further, those in the industry establishing codes will be able to use their disproportionate
influence to establish codes that may not be implementable for smaller companies, those
with fewer resources, or those with different governance structures (such as a decentralised
digital media platform). Whether intentionally (which it might be, to protect market share) or
accidentally, Misinformation Codes established by representatives of a section may stifle
experimental approaches to solving the unique problem and also stifle the innovation of new
business models that may ultimately benefit the Australian people or “solve” the
disinformation problem more effectively.

2) Incentive/Disincentive structure encourages over-censoring regardless of actual content:

Businesses/systems act within their best interests given the environment and the
incentives/disincentives of that space. This Bill creates an environment that incentivises overregulating content as the penalties for making a mistake in regulating that content are
financial in nature as well as the possibility that ACMA would create their own
misinformation code that will then apply to the digital platform. Without ACMA explicitly
taking the role of dictating what should and shouldn’t be allowed on platforms the platforms
themselves are likely to take a hardline approach and regulate any content that might be
considered misinformation/disinformation regardless of if that content matches the criteria.
Thereby it is expected that the platforms will take a “better-safe-than-sorry” approach, given
the rapid rise of A.I and automated systems it is likely that management of content will be
done by an algorithm, harsher than necessary and will disregard actual legitimacy or content.

3) Definitions of misinformation and disinformation do not consider the difficulty in

determining “true” from “false”. (Subclause 7(2))

Definitions of misinformation and disinformation in the Bill specify for content to meet the
criteria it must be “considered false, misleading, or deceptive”. How to correctly identify
what information is false or true is unlikely to be immediately evident in various domains.
Public opinion, understanding, and acceptance of information are determined by the ability
to access information and have it contrasted with conflicting ideas or competing information.
In some cases, this is a simple and clear process and fact is easily discernible. However, in
other cases what is “true” and “false” is not evident and for Australians to make personal
decisions on these topics access to competing information is necessary. As digital platforms
have become the equivalent of a town square it is here that debates on cultural topics,
emerging science, new conflicting ideas/viewpoints, and more emerge and are debated
before they are accepted/proven as “true” or “false”. By ensuring that this process cannot


-----

take place the opportunity to determine truth is taken away from broader society and given
to whichever third-party/process the digital platform has specified in their Misinformation
Code. The danger in this is that true information will be censored and thereby hidden from
the public, examples of this have arisen post-pandemic where certain discussion points were
censored by tech platforms despite “true” and “false” not being clear at the time, another
key example would be the replication crisis in scientific literature. Regardless of where
individuals fall on these issues the determination of “true” or “false” is not straightforward
and scientific evidence/viewpoints presented in good faith must be made available to the
public to make their own minds up. This is especially important as policy is often
downstream from public opinion and public opinion is determined by access to information,
often in the digital town square.

(Side note: as with determining fact so too is the creation of culture, it is a process of
socialisation that requires pushback, discussion, and competition. Changes in cultural
perspectives are inevitable and the processes by which they happen need to be healthy and
unbiased to ensure that the result is closer to actual public consensus rather than a
manufactured consensus).

4) Definitions of “harm”, misinformation, and disinformation are broad. (Subclause 7(2))

A further requirement for content to meet the criteria is that it is “reasonably likely” (a vague
and bendable provision) to cause “harm”. “Harm” is defined broadly and when combined
with “reasonably likely” a wide plethora of content may fall under this definition. Of concern
is the potential for genuine discussion and criticism of belief systems (cultural or religious) to
be labelled incorrectly as “hate” (point (a) under definitions of harm page 6) and the
potential for any content related to organising protest (necessary for a functioning
democracy) being censored (point (b)). Further examples could be provided, however they
could be endless and are not necessary, one can make an argument for almost anything to
be “reasonably likely” to cause “harm”. In fact, in a backward logic, as this bill aims to protect
Australians from “harm”, by protesting the implementation of this Bill on a digital platform
and disseminating that viewpoint it could be argued that the content protesting the Bill is
harmful and should be removed, as it would be with any online discussion and information
sharing on future government Bills that aim to “protect” or “reduce harm”. This is troubling
as citizens in a democracy must be able to publicly debate and disagree with government
policies (for it to be called a democracy).

5) The Bill provides provisions for protecting democracy but misunderstands underlying

requirements for democracy to function. (Div 3: 35)

The Bill provides protections for political speech and information on digital platforms in
relation to elections, referendums, and related authorised content that has the purpose of
influencing how citizens vote. The intent of this seems to be a means of protecting
democratic processes and to not impede on political campaigns on matters related to the
primary engagement most Australians have with the political process (voting in elections and
referendums). By only enshrining this part of the democratic process the Bill misunderstands
a core part of democracy and instead ensures the possibility that all information immune to
misinformation codes are only political content that is coming from Top-down. Democracy at


-----

its core is a bottom-up process, democracy is the allowance of a population to utilise their
aggregated wisdom to elect representatives that will then debate and compete on their
behalf. For this process to work the way it is intended the population needs exposure to all
available information about their world, country, and circumstance, so that a bottom-up
decision can be made. Furthermore, democracy is not just involvement in elections. Policy
and political decisions (even the issues that political campaigns campaign on) are
downstream from public opinion, public opinion is formed based on a variety of information
and sources. To codify in the law the ability to reduce potential sources of information is to
also codify the ability to constrict public opinion and thereby policy and political decisions.
While it is evident this is not the intention of the Bill, this must be considered as both an
unintended consequence and the potential for misuse in the future.

These are some of the core issues I have identified with this Bill, it may seem like I am grasping at
straws, being overly concerned about censorship, or nitpicking but I believe this to be absolutely
necessary as the consequences of establishing broad laws around censorship are widespread. The
freedom to engage with a variety of different information is tantamount to the freedom to think.
Focus on bots, fake profiles, A.I generated content, and intentionally manipulative dissemination of
content via proxy rather than individuals engaging in discussion, debate and information seeking
behind their real-life persona.

Kind Regards

Jared Knott


-----

