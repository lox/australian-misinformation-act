# COMMUNICATIONS ALLIANCE LTD

 Communications Alliance Submission 

## to the Department of Infrastructure, Transport, Regional Development, Communications and the Arts

# Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2023

## Exposure Draft and supplementary information
 18 August 2023


-----

## CONTENTS 

### COMMUNICATIONS ALLIANCE 2

 INTRODUCTION 3

 1. DEFINITIONS 4

**MISINFORMATION** **4**

**DISINFORMATION** **4**

**RELEVANT MATTERS TO CONSIDER** **5**

**INTERACTIVE FEATURE** **5**

### 2. SERVICES CAPTURED IN THE SCOPE OF THE DRAFT BILL 6

**CONTENT AGGREGATION SERVICE** **6**

**CONNECTIVE MEDIA SERVICE** **6**

**MEDIA SHARING SERVICE** **7**

### 3. SERVICES OUT OF SCOPE OF THE DRAFT BILL 7

 4. INDUSTRY CODES AND STANDARDS 8

 5. RECORD KEEPING, REPORTING AND INFORMATION GATHERING POWERS 9

**POWER TO COMPEL DIGITAL PLATFORM SERVICES TO PROVIDE INFORMATION** **9**

**PUBLICATION OF INFORMATION** **9**

### 6. CONCLUSION 10

Communications Alliance Submission to
DITRDCA _Communications Legislation Amendment (Combating Misinformation and Disinformation) Bill 2023_


-----

## Communications Alliance 

Communications Alliance is the primary communications industry body in Australia. Its
membership is drawn from a wide cross-section of the communications industry, including
carriers, carriage and internet service providers, content providers, platform providers,
equipment vendors, IT companies, consultants and business groups.

Its vision is to be the most influential association in Australian communications, co-operatively
initiating programs that promote sustainable industry development, innovation and growth,
while generating positive outcomes for customers and society.

The prime mission of Communications Alliance is to create a co-operative stakeholder
environment that allows the industry to take the lead on initiatives which grow the Australian
communications industry, enhance the connectivity of all Australians and foster the highest
standards of business behaviour.

For more details about Communications Alliance, see http://www.commsalliance.com.au.

Communications Alliance Submission to
DITRDCA _Communications Legislation Amendment (Combating Misinformation and Disinformation) Bill 2023_


-----

## Introduction

We welcome the opportunity comment on the exposure draft of the Communications
_Legislation Amendment (Combating Misinformation and Disinformation) Bill 2023 (Draft Bill),_
and the constructive engagement we and many of our members have had with the
Department of Infrastructure, Transport, Regional Development, Communications and the
Arts (DITRDCA).

Communications Alliance and its members share Government’s desire to combat the harms
and risks associated with mis- and disinformation. We acknowledge that it is inherently
difficult to appropriately balance the regulation of lawful content that may be harmful,
against the need to protect freedom of expression. Our digital platform members already
devote substantial resources to these aims and will work to continue to improve their systems,
processes and technologies in this area.

In making this submission, we also expressly endorse the submission provided by the Digital
Industry Group Inc. (DIGI) in response to the Draft Bill and the supplementary material.

In addition to lending our support to DIGI’s feedback, we also take the opportunity to review
and comment on the Draft Bill, ahead of its introduction into Parliament. The comments we
have set out below are designed to help ensure that any new legislation does not have
unintended consequences and is practical and effective in preventing and addressing
misinformation and disinformation.

Free TV did not participate in the development of this submission (including the endorsement
of DIGI’s submission) and does not endorse its contents.

Communications Alliance Submission to
DITRDCA _Communications Legislation Amendment (Combating Misinformation and Disinformation) Bill 2023_


-----

## 1. Definitions

**Misinformation**

1.1. Under the Draft Bill, content that would satisfy the definition of “misinformation” is
content that (a) is false, misleading or deceptive; and (b) is not excluded for
misinformation purposes; and (c) is provided on the digital service to one or more end
users in Australia; and (d) is reasonably likely to cause or contribute to serious harm
(subcl 7(1)).

1.2. In relation to the first limb of the definition, our observation is that requiring digital
services to determine whether any piece of content is false, misleading or deceptive is
challenging, and can require detailed investigation, as well as subjective and highlevel judgement that is difficult to scale. We recommend adopting the terminology
from the Australian Code of Practice on Disinformation and Misinformation (ACPDM),
which requires that the content is “verifiably” false, misleading or deceptive (ACPDM
subcls 3.2 and 3.6), as there must be some objective means to determine whether
content is false, misleading and deceptive, and that the assessment of the content
(and subsequent action in relation to the assessed content) does not unduly infringe on
freedom of expression. The Guidance Note to the Draft Bill states that the ACMA will
not have a role in determining what is truthful. In the absence of any objective
parameters, however, it is unclear how digital platform services could make this
determination and how the regulator could enforce an industry code or standard
without making such determinations; for example in relation to complaint handling with
respect to content.

1.3. We also raise concerns in relation to the “contribute to [serious harm]” element of the
definition. This sets a low threshold, and there should be greater emphasis on a causal,
reasonably foreseeable link between the content and serious harm. Adopting this
approach would assist regulated digital service platforms to identify and prioritise the
type of misinformation or disinformation that pose the most severe risks based on their
link to potential real-world serious harm.

1.4. Under the Draft Bill, the definition of “harm” in cl 2 includes hatred against a group in
Australian society on the basis of ethnicity, nationality, race, gender, sexual orientation,
age, religion and harm to the health of Australians. The concept of “hatred” can be
subjective, and in and of itself is not a real-world harm. The term “harm” also includes
harm to the health of Australians, which is a vague and broad concept that requires
greater refinement.

1.5. Recommendations:

    - Amend cl 7 to include “verifiably” or a similar term before “false, misleading or
deceptive”. The ACMA should provide guidance on content that it is aware of
that is verifiably false, misleading or deceptive to enable digital platform services
to monitor and respond to this content on their services.

    - Amend the language in cl 7 with the following: “the provision of the content on
the digital service is reasonably likely to cause or contribute to serious harm”.

    - Clarify that hatred against a of group of Australians is a harm if it poses a credible
and serious threat to the protection or safety of a group, particularly if that group
is marginalised or vulnerable.

    - Clarify that harm to health requires the endangerment of the health of
Australians.

**Disinformation**

1.6. Disinformation has the same definition as misinformation, with the additional
requirement that the person disseminating, or causing the dissemination of, the
content intends to deceive another person (subcl 7(2)). In addition to the comments
above, our concern with the concept of “disinformation” is the onus placed on digital

Communications Alliance Submission to
DITRDCA _Communications Legislation Amendment (Combating Misinformation and Disinformation) Bill 2023_


-----

platform services to determine whether or not content could be classified as
disinformation. A service provider is unlikely to have insight into the intention of the
person disseminating the content, and therefore will often not be in a position to assess
whether, pursuant to subcl 7(2), a person is disseminating, or causing the dissemination
of, the content intends that the content deceive another person.

1.7. Recommendation:

    - Adopt the definitions of “disinformation” and “inauthentic behaviours” from the
ACPDM. This would require an additional limb to the definition of “disinformation”
in subcl 7(2) which is that disinformation is propagated amongst users of digital
platforms via inauthentic behaviours, being spam and other forms of deceptive,
manipulative or bulk, aggressive behaviours (which may be perpetrated via
automated systems) and includes behaviours which are intended to artificially
influence users’ online conversations and/or to encourage users of digital
platforms to propagate digital content (ACPDM subcls 3.2 and 3.5). This provides
a scalable way for service providers to identify and address disinformation using
objective, not subjective, criteria.

**Relevant matters to consider**

1.8. Subclause 7(3) outlines the matters that must be regarded in determining whether
content on a digital service is reasonably likely to cause or contribute to serious harm.
We support the requirement to have a serious harm threshold, however these factors
should not individually, or in combination, be the sole determinants. Further, additional
illustrative factors such as warnings or notices that may be presented in conjunction
with an item of content to counteract any potential misleading or deceptive effect
should be considered. This could come in the form of a banner or some other
indication that the content has not been fact checked. The existence of warnings puts
customers on notice, and is a scalable and effective way for digital services to
balance freedom of expression against the need to protect consumers from
misinformation or disinformation.

1.9. Recommendation:

    - Clarify subcl 7(3) to specify that these factors are not the only factors used to
determine serious harm when assessing the material, and expand the list of
factors to include warnings or notices presented in conjunction with an item of
content, and the identity of the person responsible for posting the content (in
addition to the original author). This puts customers on notice and would reduce
the likelihood of the content from causing serious harm in the context of
misinformation and disinformation.

**Interactive feature**

1.10. Under subcl 5(c), a digital service has an interactive feature if the digital service makes

interactions between end-users or by end-users with content provided on the service
“observable” to other end-users. This would capture interactive features that enable
“likes” or similar reactions. This is problematic, as the criterion would also apply where
those interactions are “observable” but where they do not affect the reach of the
content in question, i.e., even if the level of prominence given to the content on the
service is not affected by the number of “likes” or other reactions it receives. From a
misinformation perspective, this type of feature should only be problematic if it affects
the potential virality of the content and, therefore, the likelihood that this content will
cause serious harm.

1.11. Recommendation:

    - Amend subcl 5(c) so that it only applies where the observable interactions also
directly affect the level of prominence given to the content on the service.

Communications Alliance Submission to
DITRDCA _Communications Legislation Amendment (Combating Misinformation and Disinformation) Bill 2023_


-----

## 2. Services captured in the scope of the Draft Bill

2.1. The ACMA’s proposed new powers would apply to “digital platform services”, except
for any services excluded by cl 6. The Draft Bill separates digital platform services into
the following three categories: (1) content aggregation services; (2) connective media
services; and (3) media sharing services. The definitions of these categories as currently
drafted are overly broad, and our comments below make recommendations on
refining these definitions to ensure that the services captured are those that have a
propensity for the dissemination of misinformation and disinformation.

2.2. The use of “a primary function” is, in our view, problematic for all three digital platform
service categories. This terminology is ambiguous, contemplates a significant degree of
overlap between these categories, does not provide digital services with clarity, and
could potentially capture a broader swathe of services than intended.

2.3. We also recommend introducing a risk assessment as a factor when determining
whether a digital service should be regulated by the ACMA. The definition of the
categories is broad and could capture digital services that present a low risk of
misinformation or disinformation that is reasonably likely to cause serious harm, for
example, because the service does not facilitate virality and amplification of the
content.

2.4. Recommendations:

    - Replace “a primary function” with “the primary function” (emphasis added) in
subcls 4(2)(a), 4(3)(a) and 4(4)(a).

    - Introduce a second limb to each of the three categories of digital platform
services to include a risk assessment of the service to ensure low-risk services are
not unintentionally captured. This provides for a flexible approach which would
allow for digital services to be included at a future point, should it be established
that they have new features that could facilitate misinformation or disinformation
that have a potential to cause serious harm.

**Content aggregation service**

2.5. A digital service is a content aggregation service if “a primary function of the digital
service is to collate and present to end-users content from a range of online sources,
including sources other than the digital service”. This definition ostensibly captures all
websites, software and apps that provide content to end-users. We recommend this
definition is specifically targeted at digital services that index or collate information
from the world wide web, being search engines and news aggregation services,
consistent with the intent expressed in the Guidance Note rather than broadly framed
with vague terms such as “content from a range of online sources” (subcl 4(2)).

2.6. Recommendation:

    - Amend the definition of “content aggregation service” in subcl 4(2) to: “(a) the a
primary function of the digital service is to collate and present to end-users
content from the world wide web a range of online sources, including sources
other than the digital service; (b) such other conditions (if any) as are set out in
the digital platform rules.”

**Connective media service**

2.7. A digital service is a connective media service if “(a) a primary function of the digital
service is to enable online interaction between two or more end-users; (b) the digital
service allows end-users to link to, or interact with, some or all of the other end-users; (c)
the digital service has an interactive feature” (subcl (4)(3)).

Communications Alliance Submission to
DITRDCA _Communications Legislation Amendment (Combating Misinformation and Disinformation) Bill 2023_


-----

2.8. This is a broad definition and may capture digital services that pose a low risk of
widespread and viral dissemination of misinformation and disinformation. For example,
this definition captures all websites and apps with a comment or forum feature even if
the website or app is for an enterprise, public service, or specialised purpose (e.g.,
academic research, troubleshooting etc.). Further, the definition does not give
consideration to other factors, such as total number of end users which will impact the
risk of widespread dissemination of misinformation on that digital service. There is also
potential duplication and lack of clarity between the three limbs of this definition
(subcls (a), (b) and (c)).

2.9. Recommendations:

    - Provide greater clarity between some of the limbs within the definition. Amend
limb (a) of the definition so that it only captures services where the primary
function is to enable online interaction between end-users via an interactive
feature.

    - Subsequently, delete limbs (b) and (c) as they would be incorporated into the
amended limb (a).

**Media sharing service**

2.10. A digital service satisfies the definition of a media sharing service if “a primary function

of the digital service is to provide audio, audio-visual or moving visual content to endusers” (subcl 4(4)). The Draft Bill further provides that a media sharing service that does
not have an interactive feature is excluded (subcl 6(1)(b)).

2.11. This is another broad definition and may unintentionally capture services that have a

low risk of widespread and viral dissemination of misinformation and disinformation. For
example, a media sharing service’s primary function may not be interactive, but there
could be a limited function that would satisfy this definition (such as a subscription
video on demand streaming services with a chat functionality, or the ability to leave
reviews). Notwithstanding this, if the service poses a low risk for misinformation and
disinformation, we submit that the inclusion of such a service may be unintended.

2.12. Recommendation:

    - Amend the definition of media sharing service so that it only covers services that
have an interactive feature. In other words, move the carve out from section
6(1)(b) into the definition itself, in order to align with the approach taken for
connective media services.

## 3. Services out of scope of the Draft Bill

3.1. The Draft Bill expressly identifies a number of services that would be excluded from the
ACMA’s proposed new powers. These include, internet carriage services, SMS, MMS,
email services for misinformation purposes, and media sharing services that do not
have an interactive feature. The exclusions are limited, drafted in a confusing manner
(i.e., to be found in various places) and, in our view, fail to appropriately exclude some
carriage services.

3.2. Subcls 4(1)(e), (f) and (g) appear to be intended to exclude carriage services as these
services are prohibited by statute (Telecommunications Act 1997 and
_Telecommunications (Access and Interception) Act 1979) from interfering with the_
privacy of communications. However, these subclauses fail to exclude all carriage
services, for example, email services provided as a carriage services (e.g. Bigpond,
Optusnet etc.). This ought to be corrected. (We note that the exclusion of email
services in cl 6 only applies to misinformation.)

3.3. To ensure digital services that are not intended to be subject to ACMA’s enhanced
powers, we recommend the following.

Communications Alliance Submission to
DITRDCA _Communications Legislation Amendment (Combating Misinformation and Disinformation) Bill 2023_


-----

3.4. Recommendations:

    - Aggregate subcls 4(1)(e), (f) and (g) into one single class of exempted services,
i.e., 4(1)(e) “a carriage service”, and include a definition under cl 2 which
references the definition of carriage service in the Telecommunications Act 1997.

    - Expressly list additional digital services that present a low risk for misinformation
and disinformation. This could include e-commerce websites and marketplaces;
messaging forums intended for customers of a specific organisation in relation to
products and services supplied by that organisation, and services intended for
the use within a specific organisation.

## 4. Industry codes and standards

4.1. The Draft Bill gives the ACMA the power to determine a binding standard, such as
where a request to develop a misinformation code is not complied with (Division 5).
There is no requirement for the ACMA to undertake public consultation prior to making
an industry standard. This approach is at odds with the approach taken under the
_Online Safety Act 2021 (s 148) and the Telecommunications Act 1997 (s 132), which_
require the eSafety Commission and the ACMA, respectively, to conduct public
consultation of at least 30 days prior to making an industry standard.

4.2. It is also not clear why industry associations developing an industry code should be
required to publicly consult on a draft code (subcl 37(1)(f)) while the ACMA is only
required to consult with respective industry associations. Given the substantial penalties
for non-compliance, it is imperative all interested parties have an opportunity to
comment on a proposed standard. The very large number of submissions in response to
the public consultation on the Draft Bill also highlights the importance of and interest by
the public in relation to these issues.

4.3. The Draft Bill sets the minimum period for the development of an industry code to 120
days (subcl 38(2). This period is unrealistic as many code development processes that
Communications Alliance has undertaken (individually or jointly) take substantially
more time. For example, the development of the Online Safety Codes has taken more
than 20 months. Similarly, the revision (let alone new development) of the major
consumer protections code in the telecommunications sector (the
_Telecommunications Consumer Protections Code) regularly requires more than twelve_
months.

4.4. While the stated period sets a lower limit and longer periods for code development are
possible, we believe the legislation ought not to state unrealistic timeframes which put
the developing industry associations at the ‘mercy’ of the regulator (or Minister) to
grant extensions to the timeframe.

4.5. Recommendations:

    - Consistent with the Online Safety Act 2021 (s 148) and the Telecommunications
_Act 1997 (s 132), require the ACMA to undertake a public consultation prior to_
making any industry standard on misinformation. This should include a
requirement that the ACMA publish on its website the draft industry standard or
variation and a notice inviting interested parties to comment, which should run
for at least 30 days after publication.

    - Amend subcl 38(2) to extend the timeframe to 365 days or other reasonable
period commensurate with the complexity of the code requested. This would
allow for shorter timeframes (than 365 days) to be set, provided those timeframes
remain reasonable for the task at hand.

    - Given the far-reaching potential for industry standards to curtail freedom of
expression, make industry standards disallowable by Parliament.

Communications Alliance Submission to
DITRDCA _Communications Legislation Amendment (Combating Misinformation and Disinformation) Bill 2023_


-----

## 5. Record keeping, reporting and information gathering powers

5.1. The Draft Bill gives the ACMA broad powers to, for example, obtain information and
documents and make rules about the prevalence of content containing false,
misleading or deceptive information provided on the service other than excluded
content for misinformation purposes (see for example, subcls 14(1) and 18(2)). The
definition of “excluded content for misinformation purposes” includes a narrow list of
content such as content that is produced in good faith for the purposes of
entertainment, parody or satire and professional news content. These provisions
present challenges regarding the determination of content that is false, misleading or
deceptive, as these concepts are ambiguous and service providers lack the means
and expertise to fact-check content. The reference to “false, misleading or deceptive
information” also introduces uncertainty regarding the difference between
misinformation, disinformation and content that is false, misleading or deceptive.

5.2. The Draft Bill also sets out ACMA’s proposed new rule-making powers. One is that
reports must be prepared “as and when” required by ACMA (subcl 14(8)). We note
that there is no minimum time frame to produce reports, which creates uncertainty for
industry and potentially inconsistent approaches by the regulator.

5.3. Recommendations:

    - Replace the reference to “false, misleading or deceptive information” in the
provisions dealing with ACMA’s powers with “misinformation and disinformation”
(which includes a harm threshold), as these concepts are clearly defined in the
Draft Bill.

    - Specify a minimum timeframe for the production of a report such as 28 days,
which is the timeframe provided under the Online Safety Act 2021 for an
analogous requirement.

**Power to compel digital platform services to provide information**

5.4. Under cls 18 and 19, the ACMA would have the power to compel digital platform
providers and other persons to provide information and documents relevant to:
misinformation and disinformation on the service; measures taken to prevent or
respond to misinformation or disinformation; or the prevalence of false, misleading or
deceptive information provided on the digital platform service.

5.5. These proposed information-gathering powers should be subject to limitations,
including the protection of commercially sensitive information, and the requirement
that the ACMA prepare guidelines on these powers as other regulators such as the
Office of the eSafety Commissioner and the Australian Competition and Consumer
Commission are required to do.

5.6. Recommendations:

    - Include safeguards to ensure that the ACMA exercises the proposed informationgathering powers in a proportionate manner. Factors could include whether the
information requested is in the public interest, whether there is a history of
complaints about misinformation regarding a particular service provider, and the
burden on the service provider to provide the information. Note that these
safeguards should also be applied to ACMA’s record-keeping powers.

    - ACMA should be required to prepare guidelines on its information gathering
powers.

**Publication of information**

5.7. Cle 25 sets out the ACMA’s ability to publish information collected under the
information-gathering and record-keeping powers on the ACMA’s website. Clause 26
provides some balance, stipulating that the ACMA must consult with the relevant
digital platform provider and invite the provider to identify any information that could

Communications Alliance Submission to
DITRDCA _Communications Legislation Amendment (Combating Misinformation and Disinformation) Bill 2023_


-----

be expected to materially prejudice the commercial interests of a person and provide
reasons. Notwithstanding this, the ACMA is only required to consider this response.

5.8. The publication of the information that the ACMA gathers about a digital platform
service may not be proportionate to the prevalence of misinformation or
disinformation, or reflect the steps that service has taken to address such content.
Consequently, publications of such information could have a disproportionate impact
on a digital platform service and, therefore, these powers ought to be exercised with
caution.

5.9. Recommendations:

    - Include requirements that the ACMA may only publish information that is in the
public interest; that this information is a summary of the information gathered,
unless the information is already available in the public domain; and
commercially sensitive information is not published.

    - Require the ACMA to also consider a range of factors when determining whether
to publish information, such as whether publication may compromise safeguards
that the digital platform service has in place to combat misinformation or
disinformation.

## 6. Conclusion

Communications Alliance looks forward to continued engagement with DITRDCA and other
relevant stakeholders to help ensure that the potential risks and harms associated with the
dissemination and propagation of misinformation and disinformation are appropriately
limited, while safeguarding freedom of expression and other fundamental democratic rights.

It is important that all stakeholders cooperate in the development of any legislation and
regulation that has the potential to inadvertently infringe on Australia’s democratic rights,
and we welcome the consultative approach so far taken. We would welcome further
debate in relation to the issues raised in this submission and stand ready to answer any
questions that DITRDCA or other stakeholders may have.

For any questions relating to this submission please contact Christiane Gillespie-Jones on

or at .

Communications Alliance Submission to
DITRDCA _Communications Legislation Amendment (Combating Misinformation and Disinformation) Bill 2023_


-----

**Published by:**
**COMMUNICATIONS**
**ALLIANCE LTD**

**Level 12**
**75 Miller Street**
**North Sydney**
**NSW 2060 Australia**

**Correspondence**
**PO Box 444**
**Milsons Point**
**NSW 1565**

**T 61 2 9959 9111**
**F 61 2 9954 6136**
**E**
**info@commsalliance.com.au**
**www.commsalliance.com.au**
**ABN 56 078 026 507**


-----

