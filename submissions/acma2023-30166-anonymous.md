**The following details some serious concerns with the Combatting**
**Misinformation Bill**

**Freedom of speech/expression**

The broad definition of "misinformation" could potentially capture some speech that,
while false or misleading, does not cause serious harm. This may unnecessarily
restrict freedom of expression.

The exclusion for "professional news" content could lead to platforms making
judgment calls on what constitutes professional news. This could impact press
freedom if platforms end up restricting news content.

The exclusion for "authorised" government content could enable governments to
spread misinformation without accountability.

The legislation lacks clear guardrails to prevent over-blocking or over-removal of
lawful content by platforms seeking to avoid penalties.

**Privacy**

The information gathering powers are broad and may reveal private user data,
despite the carve out for private messages. It is unclear what safeguards exist
around ACMA's ability to disclose obtained information to other agencies.

**Here are some specific privacy concerns with the proposed legislation:**

The information gathering powers allow ACMA to compel platforms to provide
potentially sensitive user data, even if not related to private messages. Things like
user profiles, posts, networks, demographics, and platform interactions could reveal
private information about individuals.

It's unclear what limits exist around ACMA sharing user data with other government
agencies. There is potential for user data obtained under these powers to end up in
the hands of agencies like law enforcement without sufficient justification.

The legislation lacks clarity around what safeguards will exist for stored user data,
such as data minimization, access controls, and deletion requirements. The lack of
defined protections increases privacy risks.

User consent is not required for platforms to disclose private information to ACMA
under these powers. This bypasses users' ability to control use of their data.

Even though private messages are excluded, requiring platforms to turn over
moderation data could reveal information about private conversations that were
reported/flagged. This indirectly exposes private content.

Collecting data on the "prevalence of false, misleading or deceptive information"
could involve monitoring private groups, chats, or accounts for "misinformation" even
if not publicly accessible.

In summary, the broad information gathering and data sharing powers create
significant risks of mass collection and misuse of users' private data without their


-----

consent. Stronger privacy safeguards around data collection, storage, access,
sharing, and oversight are needed to protect Australians' digital rights.

**Government overreach**

The broad definitions of digital services could capture many sites and apps well
beyond major platforms. Small sites may struggle with compliance. The code and
standard making powers give the government significant influence over platform
policies and content moderation. The scope of enforcement powers, large fines, and
criminal penalties may incentivize over-compliance and censorship by platforms.
There are no defined limits on when ACMA can request new industry codes, replace
voluntary codes, or create binding standards. In summary, while the goal of reducing
online harms has merit, parts of the proposed legislation raise concerns about
impacts to free speech, privacy, and fair process. Input from legal and civil society
groups could help refine the legislation to better safeguard rights and liberties. I
would recommend further review of the potential unintended consequences.

**Here are some ways the proposed legislation potentially allows for government**
**overreach:**

The broad definitions of digital services and vague harms could allow regulations to
encroach on lawful speech and small websites beyond major platforms.

The code/standard making process gives the government and ACMA significant
influence over platforms' internal policies, moderation practices, and algorithms
without judicial oversight.

The lack of defined limits on when ACMA can require new codes or standards leaves
discretion to determine what constitutes "harm" that requires intervention.

The large fines and criminal penalties could lead to censorship as platforms overcomply to avoid punishment. But non-compliance also risks severe penalties.

The legislation lacks due process guarantees around content takedowns or account
suspensions imposed under codes/standards. Users may lack recourse for unfair
moderation.

Information gathering powers allow compelled access to potentially sensitive platform
data with minimal justification required. Few protections against data misuse.

Power to obtain user identities and private information risks breaching
journalist/source confidentiality and enabling surveillance.

Ability to disclose user data to other agencies facilitates access for authorities without
usual legal checks and balances.

Limited judicial oversight over enforcement decisions could see punitive actions
taken without impartial review.

In summary, the legislation lacks safeguards against regulatory overreach, gives
significant discretion to authorities, imposes severe penalties that incentivize overcompliance, and enables access to user data with minimal due process. Stronger
checks and balances are needed to prevent potential government overreach.


-----

**Digital Services**

**Here are some potential negative impacts the proposed legislation could have**
**on digital services:**

Compliance burden - The new regulatory requirements like record-keeping, reporting,
and modifying platforms to comply with codes/standards may impose significant
costs, especially on smaller platforms with limited resources. This could create
barriers to market entry.

Over-removal of content - To avoid large fines, platforms may end up being overly
cautious and remove lawful content like satire, parody, etc that may seem
questionable. This could restrict free expression.

Withdrawal from Australia - Major platforms like Facebook or Twitter may choose to
geo-block Australian users rather than deal with the burdens of complying with the
new rules. This would cut off Australians' access to those services.

Reduced innovation - The increased regulatory scrutiny and compliance costs may
discourage digital platforms from launching new or experimental products in Australia,
limiting innovation.

User impacts - Changes to platforms' algorithms, moderation policies, and interfaces
to comply with codes could degrade the user experience, limiting utility of these
services for Australians.

Competitive impacts - Larger platforms will be better equipped to handle compliance
than startups, entrenching the market power of incumbents. Could inhibit competition
and consumer choice.

To summarize, the legislation imposes significant new burdens on digital platforms
that could stifle innovation and competition, force withdrawal of services, increase
costs, and ultimately degrade the experience for Australian consumers and
businesses that rely on these services. A more cooperative regulatory approach may
help avoid unintended harms.

**Serious Harm**

**The examples of "serious harm" provided in the legislation could be seen as**
**having some limitations:**

- Many of the examples like hate speech, foreign interference, and environmental
harm are already illegal under existing Australian law. So it's unclear if additional
regulation of legal speech is justified to prevent such harms.

- Terms like "public order", "integrity of democratic processes", and "economic or
financial harm" are vague and open to wide interpretation. Clearer definitions may be
needed to prevent overreach.

- Just because speech is offensive, causes discomfort, or spreads false information
does not necessarily constitute "serious harm" that warrants limiting expression. The
threshold for "serious" is subjective.


-----

- The requirement that harm be "serious and wide-reaching" suggests only largescale viral misinformation would be affected. But lower-level false claims can also
cumulatively cause real harm.

- Assessing "harm" requires making editorial judgements of truth/falsity and public
impact. But platforms lack expertise and authority to make such journalistic
judgments.

- "Serious harm" often manifests from complex factors beyond just online
misinformation. Restricting speech may not adequately address underlying socioeconomic drivers.

- Certain examples like undermining democratic institutions or causing harm to public
health rely on speculative chains of causation that are difficult to prove.

In summary, while the goal of preventing serious societal harm is valid, the subjective
definitions and focus only on widespread viral misinformation could limit the
effectiveness of the regulations. More nuanced approaches may be needed to
address different levels and types of false information.

**Freedom of Speech**

Freedom of speech is a fundamental human right that refers to the ability of
individuals to express their thoughts, ideas, opinions, and beliefs without fear of
censorship, interference, or punishment from the government or other sources of
authority. It is considered a cornerstone of democratic societies and is typically
protected by laws or constitutional provisions in many countries.

At its core, freedom of speech encompasses the right to seek, receive, and impart
information and ideas through various means of communication, including spoken
words, written texts, visual representations, artistic expression, and digital platforms.
It not only protects popular or widely accepted viewpoints but also extends to
unpopular, controversial, or dissenting opinions.

The concept of freedom of speech is rooted in the belief that a free and open
exchange of ideas leads to a more informed and vibrant society. It allows individuals
to engage in public debate, challenge prevailing norms, criticize the government or
other powerful entities, advocate for social change, and participate in the democratic
process.


-----

